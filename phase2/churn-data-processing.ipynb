{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r5xlr7Gerkp"
   },
   "source": [
    "## Churn data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25491,
     "status": "ok",
     "timestamp": 1761830563077,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "JuleXCuGQwiZ",
    "outputId": "2750e781-96af-4cf2-8a6f-a5cd7eab6334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3117,
     "status": "ok",
     "timestamp": 1761822073255,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "PQ2pnW6ZdFqZ",
    "outputId": "e902ca16-2067-4dfd-8179-0e51c63ae392"
   },
   "outputs": [],
   "source": [
    "# === EDIT THESE ===\n",
    "PROJECT_DIR   = \"path\"  # where outputs/scripts will go\n",
    "DATASET_PATH  = \"path\"\n",
    "REPORTS_JSONL = \"path\"\n",
    "\n",
    "# Make project directories\n",
    "import os\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "os.makedirs(f\"{PROJECT_DIR}/artifacts\", exist_ok=True)\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "print(\"Working in:\", os.getcwd())\n",
    "print(\"Dataset:\", DATASET_PATH)\n",
    "print(\"Reports:\", REPORTS_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ebIo-Gsk-7W"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip -q install -U pip\n",
    "!pip -q install pandas numpy scikit-learn joblib lightgbm catboost shap tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1761822077582,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "CZkaHWE2lEVx",
    "outputId": "c212dfd9-6c7f-4e35-d69f-83112f4d454a"
   },
   "outputs": [],
   "source": [
    "%%writefile train_churn.py\n",
    "import argparse, json, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, classification_report\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except:\n",
    "    CatBoostClassifier = None\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except:\n",
    "    LGBMClassifier = None\n",
    "\n",
    "LABEL_COL = \"Churn_Status\"\n",
    "DATE_COLS = [\"First_Policy_Date_Gre\",\"Last_Renewal_Date_Gre\",\"Last_Expiration_Date\"]\n",
    "NUM_SAFE_NONNEG = [\"Years_With_Company\",\"Total_Premium_Paid\",\"Renewal_Count\",\"Avg_Claim_Amount\",\"Claim_Count\"]\n",
    "\n",
    "def _parse_dates(df):\n",
    "    for c in DATE_COLS:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
    "    return df\n",
    "\n",
    "def _winsorize(s, p=0.995):\n",
    "    if s.isna().all(): return s\n",
    "    hi = s.quantile(p); lo = s.quantile(1-p)\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "def _clean_and_engineer(df):\n",
    "    key_cols = [c for c in [\"Customer_ID\",\"Last_Expiration_Date\"] if c in df.columns]\n",
    "    df = df.drop_duplicates(subset=key_cols if key_cols else None, keep=\"last\").copy()\n",
    "    df = _parse_dates(df)\n",
    "    for c in NUM_SAFE_NONNEG:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            df.loc[df[c] < 0, c] = np.nan\n",
    "    for c in [\"Total_Premium_Paid\",\"Avg_Claim_Amount\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = _winsorize(df[c].astype(float), p=0.995)\n",
    "\n",
    "    ref_date = df[\"Last_Expiration_Date\"].max()\n",
    "    ref_date = pd.Timestamp(ref_date) if pd.notnull(ref_date) else pd.Timestamp.today().normalize()\n",
    "\n",
    "    feat = pd.DataFrame(index=df.index)\n",
    "    m = {\n",
    "        \"years_with_company\":\"Years_With_Company\",\n",
    "        \"total_premium_paid\":\"Total_Premium_Paid\",\n",
    "        \"renewal_count\":\"Renewal_Count\",\n",
    "        \"avg_claim_amount\":\"Avg_Claim_Amount\",\n",
    "        \"claim_count\":\"Claim_Count\",\n",
    "    }\n",
    "    for new, old in m.items():\n",
    "        feat[new] = df[old].astype(float) if old in df.columns else np.nan\n",
    "\n",
    "    feat[\"days_since_first_policy\"] = (ref_date - df[\"First_Policy_Date_Gre\"]).dt.days\n",
    "    feat[\"days_since_last_renewal\"] = (ref_date - df[\"Last_Renewal_Date_Gre\"]).dt.days\n",
    "    feat[\"days_to_expiration\"]      = (df[\"Last_Expiration_Date\"] - ref_date).dt.days\n",
    "\n",
    "    feat = feat.replace([np.inf,-np.inf], np.nan)\n",
    "    medians = feat.median(numeric_only=True).to_dict()\n",
    "    feat = feat.fillna(medians)\n",
    "\n",
    "    feat[\"days_since_first_policy\"] = feat[\"days_since_first_policy\"].clip(-365*5, 365*50)\n",
    "    feat[\"days_since_last_renewal\"] = feat[\"days_since_last_renewal\"].clip(-365*5, 365*5)\n",
    "    feat[\"days_to_expiration\"]      = feat[\"days_to_expiration\"].clip(-365*5, 365*5)\n",
    "\n",
    "    assert LABEL_COL in df.columns, f\"Label '{LABEL_COL}' not found\"\n",
    "    y = df[LABEL_COL].astype(int).values\n",
    "    return feat, y, medians\n",
    "\n",
    "def _fit_model(algo, X, y):\n",
    "    if algo==\"catboost\":\n",
    "        if CatBoostClassifier is None: raise RuntimeError(\"catboost not installed\")\n",
    "        m = CatBoostClassifier(loss_function=\"Logloss\", eval_metric=\"AUC\",\n",
    "                               depth=6, learning_rate=0.08, iterations=800,\n",
    "                               l2_leaf_reg=3.0, random_seed=42, verbose=False)\n",
    "        m.fit(X, y)\n",
    "        return m\n",
    "    if algo==\"lightgbm\":\n",
    "        if LGBMClassifier is None: raise RuntimeError(\"lightgbm not installed\")\n",
    "        m = LGBMClassifier(objective=\"binary\", boosting_type=\"gbdt\",\n",
    "                           num_leaves=63, learning_rate=0.05, n_estimators=1200,\n",
    "                           reg_lambda=1.0, subsample=0.9, colsample_bytree=0.9,\n",
    "                           random_state=42, n_jobs=-1)\n",
    "        m.fit(X, y)\n",
    "        return m\n",
    "    raise ValueError(\"algo must be catboost or lightgbm\")\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--csv\", required=True)\n",
    "    ap.add_argument(\"--algo\", choices=[\"catboost\",\"lightgbm\"], required=True)\n",
    "    ap.add_argument(\"--test_size\", type=float, default=0.2)\n",
    "    ap.add_argument(\"--out_dir\", default=\"artifacts\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    os.makedirs(args.out_dir, exist_ok=True)\n",
    "    df = pd.read_csv(args.csv)\n",
    "    X, y, med = _clean_and_engineer(df)\n",
    "\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=args.test_size, stratify=y, random_state=42)\n",
    "\n",
    "    base = _fit_model(args.algo, Xtr, ytr)\n",
    "    cal = CalibratedClassifierCV(base, method=\"isotonic\", cv=3).fit(Xtr, ytr)\n",
    "\n",
    "    p = cal.predict_proba(Xte)[:,1]\n",
    "    auc = roc_auc_score(yte, p); ap_ = average_precision_score(yte, p); brier = brier_score_loss(yte, p)\n",
    "    print(f\"[{args.algo}] AUC={auc:.4f} | AP={ap_:.4f} | Brier={brier:.5f}\")\n",
    "    print(classification_report(yte, (p>=0.5).astype(int), digits=3))\n",
    "\n",
    "    joblib.dump(cal, f\"{args.out_dir}/churn_{args.algo}_calibrated.joblib\")\n",
    "    with open(f\"{args.out_dir}/churn_preprocess_artifacts.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump({\"feature_order\": list(X.columns), \"medians\": med}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    if hasattr(base,\"feature_importances_\"):\n",
    "        imp = pd.Series(base.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        imp.to_csv(f\"{args.out_dir}/feature_importance_{args.algo}.csv\")\n",
    "        print(\"Top features:\\n\", imp.head(10))\n",
    "\n",
    "    print(\"DONE\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 78427,
     "status": "ok",
     "timestamp": 1761822156034,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "3_a1d41WlGlh",
    "outputId": "5bff98ef-2b07-485a-dbd1-d1b77608f405"
   },
   "outputs": [],
   "source": [
    "# Train both models (choose the better one by AUC/AP/Brier)\n",
    "!python train_churn.py --csv \"{DATASET_PATH}\" --algo catboost  --out_dir artifacts\n",
    "!python train_churn.py --csv \"{DATASET_PATH}\" --algo lightgbm --out_dir artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1761822156241,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "vpGabsVklJPn",
    "outputId": "86163fae-3bbe-4f8f-a180-8ed249e23d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score_bulk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score_bulk.py\n",
    "import json, joblib, argparse\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def load_artifacts(model_path, art_path):\n",
    "    model = joblib.load(model_path)\n",
    "    art = json.load(open(art_path, \"r\", encoding=\"utf-8\"))\n",
    "    return model, art[\"feature_order\"], art[\"medians\"]\n",
    "\n",
    "def engineer(df):\n",
    "    df = df.copy()\n",
    "    for c in [\"First_Policy_Date_Gre\",\"Last_Renewal_Date_Gre\",\"Last_Expiration_Date\"]:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True)\n",
    "    ref = df[\"Last_Expiration_Date\"].max()\n",
    "    ref = pd.Timestamp(ref) if pd.notnull(ref) else pd.Timestamp.today().normalize()\n",
    "\n",
    "    X = pd.DataFrame(index=df.index)\n",
    "    X[\"years_with_company\"] = df[\"Years_With_Company\"]\n",
    "    X[\"total_premium_paid\"] = df[\"Total_Premium_Paid\"]\n",
    "    X[\"renewal_count\"]      = df[\"Renewal_Count\"]\n",
    "    X[\"avg_claim_amount\"]   = df[\"Avg_Claim_Amount\"]\n",
    "    X[\"claim_count\"]        = df[\"Claim_Count\"]\n",
    "    X[\"days_since_first_policy\"] = (ref - df[\"First_Policy_Date_Gre\"]).dt.days\n",
    "    X[\"days_since_last_renewal\"] = (ref - df[\"Last_Renewal_Date_Gre\"]).dt.days\n",
    "    X[\"days_to_expiration\"]      = (df[\"Last_Expiration_Date\"] - ref).dt.days\n",
    "\n",
    "    X = X.replace([np.inf,-np.inf], np.nan)\n",
    "    return X, ref\n",
    "\n",
    "def unwrap_tree_model(model):\n",
    "    # Unwrap scikit's CalibratedClassifierCV to the underlying tree model (LightGBM/CatBoost)\n",
    "    # Supported patterns across sklearn versions:\n",
    "    # 1) model.base_estimator\n",
    "    # 2) model.calibrated_classifiers_[0].estimator\n",
    "    if model.__class__.__name__ == \"CalibratedClassifierCV\":\n",
    "        if hasattr(model, \"base_estimator\") and model.base_estimator is not None:\n",
    "            return model.base_estimator\n",
    "        if hasattr(model, \"calibrated_classifiers_\") and len(model.calibrated_classifiers_) > 0:\n",
    "            cc = model.calibrated_classifiers_[0]\n",
    "            if hasattr(cc, \"estimator\"):\n",
    "                return cc.estimator\n",
    "    return model  # already a native tree model\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--csv_in\", required=True)\n",
    "    ap.add_argument(\"--csv_out\", default=\"artifacts/churn_scored.csv\")\n",
    "    ap.add_argument(\"--model_path\", required=True)\n",
    "    ap.add_argument(\"--art_path\", default=\"artifacts/churn_preprocess_artifacts.json\")\n",
    "    ap.add_argument(\"--topk\", type=int, default=3)\n",
    "    ap.add_argument(\"--with_shap\", action=\"store_true\")\n",
    "    ap.add_argument(\"--shap_sample\", type=int, default=0, help=\"0=all rows, otherwise sample N rows for SHAP\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    model, feats, med = load_artifacts(args.model_path, args.art_path)\n",
    "    df = pd.read_csv(args.csv_in)\n",
    "    X, ref = engineer(df)\n",
    "    X = X[feats].fillna(med)\n",
    "\n",
    "    probs = model.predict_proba(X)[:,1]\n",
    "    seg = np.where(probs>=0.7,\"High\", np.where(probs>=0.4,\"Mid\",\"Low\"))\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"p_churn\"] = probs\n",
    "    out[\"segment\"] = seg\n",
    "\n",
    "    if args.with_shap:\n",
    "        import shap\n",
    "\n",
    "        base = unwrap_tree_model(model)\n",
    "        # Safety: if SHAP still doesn't support the unwrapped model, skip gracefully\n",
    "        try:\n",
    "            explainer = shap.TreeExplainer(base)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] SHAP TreeExplainer unsupported for model={type(base)}; skipping SHAP. Error: {e}\")\n",
    "            out[\"top_reasons\"] = \"\"\n",
    "            out.to_csv(args.csv_out, index=False)\n",
    "            print(\"Saved:\", args.csv_out)\n",
    "            return\n",
    "\n",
    "        if args.shap_sample and args.shap_sample < len(X):\n",
    "            samp_idx = np.random.RandomState(42).choice(len(X), size=args.shap_sample, replace=False)\n",
    "            X_shap = X.iloc[samp_idx]\n",
    "            index_target = X.index[samp_idx]\n",
    "        else:\n",
    "            X_shap = X\n",
    "            index_target = X.index\n",
    "\n",
    "        sv = explainer.shap_values(X_shap)\n",
    "        if isinstance(sv, list):  # e.g., [neg_class, pos_class]\n",
    "            sv = sv[1]\n",
    "        vals = np.abs(sv)\n",
    "\n",
    "        top_names = []\n",
    "        for i in range(X_shap.shape[0]):\n",
    "            idx = np.argsort(-vals[i])[:args.topk]\n",
    "            top_names.append(\",\".join([X_shap.columns[j] for j in idx]))\n",
    "\n",
    "        out[\"top_reasons\"] = \"\"\n",
    "        out.loc[index_target, \"top_reasons\"] = top_names\n",
    "    else:\n",
    "        out[\"top_reasons\"] = \"\"\n",
    "\n",
    "    out.to_csv(args.csv_out, index=False)\n",
    "    print(\"Saved:\", args.csv_out)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346237,
     "status": "ok",
     "timestamp": 1761822502484,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "SngnHIEIlLfX",
    "outputId": "4bb0a294-97a9-40a9-849f-634c931459cd"
   },
   "outputs": [],
   "source": [
    "# Score full dataset (set model to the winner: lightgbm or catboost)\n",
    "MODEL_PATH = f\"{PROJECT_DIR}/path\"  # change if catboost won\n",
    "ART_PATH   = f\"{PROJECT_DIR}/path\"\n",
    "\n",
    "!python score_bulk.py --csv_in \"{DATASET_PATH}\" --model_path \"{MODEL_PATH}\" --art_path \"{ART_PATH}\" --with_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1761823493195,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "3ImiwERUlOe_",
    "outputId": "73e1b420-fdbc-462e-c066-1cb853977f74"
   },
   "outputs": [],
   "source": [
    "%%writefile build_sft_dataset.py\n",
    "import json, argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "SYS_PROMPT = (\n",
    "\"sys prompt\"\n",
    ")\n",
    "\n",
    "def safe_read_reports(path, strict=False, max_warn=20):\n",
    "    reports, warns = [], 0\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    first_non_ws = next((ch for ch in data if not ch.isspace()), \"\")\n",
    "    if first_non_ws == \"[\":  # JSON array file\n",
    "        try:\n",
    "            parsed = json.loads(data)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            else:\n",
    "                raise ValueError(\"Top-level JSON must be a list.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to parse JSON array file: {e}\")\n",
    "\n",
    "    # JSONL fallback\n",
    "    with open(path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            s = line.strip()\n",
    "            if not s or s.startswith(\"#\") or s.startswith(\"//\"):\n",
    "                continue\n",
    "            try:\n",
    "                reports.append(json.loads(s))\n",
    "            except Exception as e:\n",
    "                warns += 1\n",
    "                msg = f\"[WARN] bad JSON at line {i}: {e}\\nLINE: {s[:200]}\"\n",
    "                if strict:\n",
    "                    raise ValueError(msg)\n",
    "                if warns <= max_warn:\n",
    "                    print(msg)\n",
    "                continue\n",
    "    return reports\n",
    "\n",
    "def get_nested(d, *keys, default=None):\n",
    "    cur = d\n",
    "    try:\n",
    "        for k in keys:\n",
    "            if cur is None: return default\n",
    "            cur = cur.get(k)\n",
    "        return cur if cur is not None else default\n",
    "    except AttributeError:\n",
    "        return default\n",
    "\n",
    "def to_example(report, churn_row):\n",
    "    inp = {\n",
    "      \"customer_profile\": {\n",
    "        \"years_with_company\": get_nested(report,\"input\",\"years_with_company\", default=get_nested(report,\"input\",\"Years_With_Company\")),\n",
    "        \"total_premium_paid\": get_nested(report,\"input\",\"total_premium_paid\"),\n",
    "        \"renewal_count\":      get_nested(report,\"input\",\"renewal_count\"),\n",
    "        \"avg_claim_amount\":   get_nested(report,\"input\",\"avg_claim_amount\"),\n",
    "        \"claim_count\":        get_nested(report,\"input\",\"claim_count\"),\n",
    "        \"days_since_first_policy\": get_nested(report,\"input\",\"days_since_first_policy\"),\n",
    "        \"days_since_last_renewal\": get_nested(report,\"input\",\"days_since_last_renewal\"),\n",
    "        \"days_to_expiration\":      get_nested(report,\"input\",\"days_to_expiration\")\n",
    "      },\n",
    "      \"underwriting\": {\n",
    "        \"risk_score\": report.get(\"risk_score\"),\n",
    "        \"risk_level\": report.get(\"risk_level\"),\n",
    "        \"coverage\": get_nested(report,\"input\",\"coverage\")\n",
    "      },\n",
    "      \"churn\": {\n",
    "        \"p_churn\": float(churn_row.get(\"p_churn\", 0.0)) if isinstance(churn_row.get(\"p_churn\", 0.0),(int,float,str)) else 0.0,\n",
    "        \"segment\": churn_row.get(\"segment\",\"\"),\n",
    "        \"top_reasons\": (str(churn_row.get(\"top_reasons\",\"\")).split(\",\") if pd.notna(churn_row.get(\"top_reasons\",\"\")) else [])\n",
    "      },\n",
    "      \"constraints\": {\n",
    "        \"max_discount_pct\": number,\n",
    "        \"allowed_actions\": [\"actions\"]\n",
    "      }\n",
    "    }\n",
    "    rec_conditions = report.get(\"recommended_conditions\", [])\n",
    "    if not isinstance(rec_conditions, list):\n",
    "        rec_conditions = []\n",
    "    retention_plan = [{\"step\": i+1, \"action\": a} for i,a in enumerate(rec_conditions)]\n",
    "    if not retention_plan:\n",
    "        rpa = report.get(\"recommended_premium_action\", \"\")\n",
    "        if rpa:\n",
    "            retention_plan = [{\"step\":1,\"action\": rpa}]\n",
    "\n",
    "    out = {\n",
    "      \"summary\": report.get(\"short_summary\",\"\"),\n",
    "      \"retention_plan\": retention_plan,\n",
    "      \"underwriting_notes\": [report.get(\"decision_rationale\",\"\")] if report.get(\"decision_rationale\") else [],\n",
    "      \"cx_message_short\": report.get(\"short_summary\",\"\")\n",
    "    }\n",
    "\n",
    "    return {\n",
    "      \"messages\":[\n",
    "        {\"role\":\"system\",\"content\":SYS_PROMPT},\n",
    "        {\"role\":\"user\",\"content\":json.dumps(inp, ensure_ascii=False)},\n",
    "        {\"role\":\"assistant\",\"content\":json.dumps(out, ensure_ascii=False)}\n",
    "      ]\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--reports_jsonl\", required=True)\n",
    "    ap.add_argument(\"--churn_scored_csv\", required=True)\n",
    "    ap.add_argument(\"--out_jsonl\", default=\"artifacts/llm_sft_dataset.jsonl\")\n",
    "    ap.add_argument(\"--join_key\", default=None)\n",
    "    ap.add_argument(\"--strict_reports\", action=\"store_true\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    churn = pd.read_csv(args.churn_scored_csv).reset_index(drop=True)\n",
    "    reports = safe_read_reports(args.reports_jsonl, strict=args.strict_reports)\n",
    "\n",
    "    data = []\n",
    "    if args.join_key and args.join_key in churn.columns:\n",
    "        idx_map = {row[args.join_key]: i for i,row in churn.iterrows()}\n",
    "        matched = 0\n",
    "        for rep in reports:\n",
    "            key = get_nested(rep,\"input\",args.join_key, default=rep.get(args.join_key))\n",
    "            if key in idx_map:\n",
    "                data.append(to_example(rep, churn.loc[idx_map[key]]))\n",
    "                matched += 1\n",
    "        print(f\"[INFO] Keyed join matched {matched}/{len(reports)}\")\n",
    "    else:\n",
    "        n = min(len(reports), len(churn))\n",
    "        for i in range(n):\n",
    "            data.append(to_example(reports[i], churn.iloc[i]))\n",
    "        if len(reports) != len(churn):\n",
    "            print(f\"[INFO] Index-join used: reports={len(reports)} churn={len(churn)} -> n={n}\")\n",
    "\n",
    "    with open(args.out_jsonl,\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in data:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    print(\"Saved:\", args.out_jsonl, \"count:\", len(data))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1243,
     "status": "ok",
     "timestamp": 1761823497672,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "s2aV0rbFlSkV",
    "outputId": "26b1536c-32cc-41e4-f0ca-eef9c9e2cb2d"
   },
   "outputs": [],
   "source": [
    "# Build SFT dataset (pre fine-tuning artifact)\n",
    "!python build_sft_dataset.py --reports_jsonl \"{REPORTS_JSONL}\" --churn_scored_csv \"{PROJECT_DIR}/artifacts/churn_scored.csv\" --out_jsonl \"{PROJECT_DIR}/artifacts/llm_sft_dataset.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1871,
     "status": "ok",
     "timestamp": 1761833745031,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "AoAfXQolQfk3",
    "outputId": "57e9cc3c-5744-49a0-f21e-11dc8573636d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROJECT_DIR   = Path(\"path\")\n",
    "DATASET_PATH  = Path(\"path\")\n",
    "REPORTS_JSONL = Path(\"path\")\n",
    "\n",
    "# create folders\n",
    "PROJECT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(PROJECT_DIR / \"artifacts\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_DIR exists:\", PROJECT_DIR.exists())\n",
    "print(\"Dataset exists:\", DATASET_PATH.exists())\n",
    "print(\"Reports exists:\", REPORTS_JSONL.exists())\n",
    "\n",
    "# cd using pure Python (avoids comment/space issues)\n",
    "os.chdir(str(PROJECT_DIR))\n",
    "print(\"Working in:\", os.getcwd())\n",
    "\n",
    "# quick sanity checks (raise if paths are wrong)\n",
    "assert (PROJECT_DIR / \"score_bulk.py\").exists(), \"score_bulk.py not found in PROJECT_DIR\"\n",
    "assert DATASET_PATH.exists(), \"Dataset path not found\"\n",
    "assert REPORTS_JSONL.exists(), \"reports.json not found\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19174,
     "status": "ok",
     "timestamp": 1761834059981,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "xf1bWXMgffLs",
    "outputId": "572496bd-05cf-46dc-8c0e-05b682c15380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hcatboost: 1.2.8\n",
      "python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "# Install required libs for loading & SHAP (optional)\n",
    "!pip -q install catboost shap pandas numpy scikit-learn joblib\n",
    "\n",
    "# Quick sanity check\n",
    "import catboost, sklearn, shap, pandas as pd, numpy as np, joblib, sys\n",
    "print(\"catboost:\", catboost.__version__)\n",
    "print(\"python:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29876,
     "status": "ok",
     "timestamp": 1761834098042,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "PfZX6E7bQttp",
    "outputId": "76408399-2bdb-4109-e104-c6b56fc676b9"
   },
   "outputs": [],
   "source": [
    "# Uses the calibrated CatBoost you trained: churn_catboost_calibrated.joblib\n",
    "!python \"score_bulk.py\" \\\n",
    "  --csv_in \"{DATASET_PATH}\" \\\n",
    "  --model_path \"{PROJECT_DIR}/artifacts/churn_catboost_calibrated.joblib\" \\\n",
    "  --art_path   \"{PROJECT_DIR}/artifacts/churn_preprocess_artifacts.json\" \\\n",
    "  --with_shap --shap_sample 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1761834130822,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "308ryFbVebFB",
    "outputId": "297f3b25-d69d-44b0-e934-61f179eedb6e"
   },
   "outputs": [],
   "source": [
    "%%writefile \"policy_engine.py\"\n",
    "def retention_policy(p_churn, risk_score, max_discount=10):\n",
    "    seg = \"High\" if p_churn>=0.7 else \"Mid\" if p_churn>=0.4 else \"Low\"\n",
    "    ladder = {\n",
    "        \"Low\": [\"values\"],\n",
    "        \"Mid\": [\"values\"],\n",
    "        \"High\":[\"values\"]\n",
    "    }[seg]\n",
    "    # risk gate: reduce discount cap for high-risk underwriting\n",
    "    cap = max_discount if risk_score<=60 else max(0, max_discount-4)\n",
    "    return {\"segment\": seg, \"allowed_actions\": ladder, \"max_discount_pct\": cap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4147,
     "status": "ok",
     "timestamp": 1761834153610,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "mGmy8wncf1vY",
    "outputId": "4bb7a0be-924a-46ec-b7aa-9f6616c26273"
   },
   "outputs": [],
   "source": [
    "!python \"build_sft_dataset.py\" \\\n",
    "  --reports_jsonl \"{REPORTS_JSONL}\" \\\n",
    "  --churn_scored_csv \"{PROJECT_DIR}/artifacts/churn_scored.csv\" \\\n",
    "  --out_jsonl \"{PROJECT_DIR}/artifacts/llm_sft_dataset.jsonl\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNYBVq+yPOSufmHZCtnr/R",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
