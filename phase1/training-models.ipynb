{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeNT_J7aKClB"
   },
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FC2Hbr5K7Frs"
   },
   "source": [
    "### Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18803,
     "status": "ok",
     "timestamp": 1762026526469,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "RQqnNJ4rKEJk",
    "outputId": "62117cf3-dcb1-4c5c-dd26-6614fc254bb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "executionInfo": {
     "elapsed": 35380,
     "status": "ok",
     "timestamp": 1761925130178,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "J1y7t3FXLRL3",
    "outputId": "0261e6cc-3ebd-49df-8a29-a39fecf22fdd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = 'path'\n",
    "df = pd.read_csv(dataset_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tz2O2qak4ZA"
   },
   "source": [
    "### Gradient Boosting Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4456,
     "status": "ok",
     "timestamp": 1761641711565,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "5alnZ90aKyOb",
    "outputId": "265a022e-2559-4cf0-bc8f-c05c4377a87d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.0)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
      "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/400.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.10.1 optuna-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4556,
     "status": "ok",
     "timestamp": 1761641716151,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "6RjkW0J2SkWC",
    "outputId": "4b84eec7-3c76-4598-a2cf-264118719617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna-integration[xgboost]\n",
      "  Downloading optuna_integration-4.5.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (from optuna-integration[xgboost]) (4.5.0)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (from optuna-integration[xgboost]) (3.1.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[xgboost]) (1.17.0)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[xgboost]) (6.10.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[xgboost]) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[xgboost]) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[xgboost]) (2.0.44)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[xgboost]) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna->optuna-integration[xgboost]) (6.0.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost->optuna-integration[xgboost]) (2.27.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost->optuna-integration[xgboost]) (1.16.2)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->optuna-integration[xgboost]) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->optuna-integration[xgboost]) (4.15.0)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[xgboost]) (3.2.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[xgboost]) (3.0.3)\n",
      "Downloading optuna_integration-4.5.0-py3-none-any.whl (99 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/99.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: optuna-integration\n",
      "Successfully installed optuna-integration-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna-integration[xgboost]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYgHQIGxMxhJ"
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gooMJe-RMvf4",
    "outputId": "52753b94-2041-4682-df13-1ef12a0c8719"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# XGBoost (GPU / A100) Multiclass — rock-solid (no category errors)\n",
    "# - Colab-ready; saves under RESULTS/XGBoost_GPU/<timestamp>\n",
    "# - Hybrid categorical encoding:\n",
    "#     * ≤ MAX_ONEHOT_CARD uniques -> one-hot\n",
    "#     * > MAX_ONEHOT_CARD -> hashing (K bins)  ✅ no unseen-category errors\n",
    "# - No pandas StringDtype; pure numeric matrices\n",
    "# - Datetime expansion, numeric downcast, stratified 70/15/15\n",
    "# - Optuna (TPE) + EarlyStopping; device='cuda' (XGBoost ≥ 3.1)\n",
    "# - Full metrics & artifacts; inference helper (labels 1..10)\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, warnings, datetime, hashlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    log_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import callback as xgb_cb\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.integration import XGBoostPruningCallback\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ COLAB DRIVE SETUP ------------------------\n",
    "drive_folder = 'path'\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive\n",
    "    if not Path(\"/content/drive\").exists() or not any(Path(\"/content/drive\").iterdir()):\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Subfolder for this run (timestamped)\n",
    "RUN_NAME = f\"XGBoost_GPU_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUTPUT_DIR = Path(drive_folder) / \"XGBoost_GPU\" / RUN_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_DIR  = OUTPUT_DIR / \"splits\"; SPLIT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving all outputs to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# >>>>>>>>>>>> SET THIS TO YOUR FILE <<<<<<<<<<<<\n",
    "DATA_CSV = \"path\"\n",
    "\n",
    "TARGET_ASCII   = \"target_risk_class\"\n",
    "TARGET_PERSIAN = \"risk_score\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "\n",
    "# GPU training controls\n",
    "N_TRIALS            = 30\n",
    "N_ESTIMATORS_LIMIT  = 3000\n",
    "EARLY_STOP_ROUNDS   = 300\n",
    "NTHREAD             = os.cpu_count()\n",
    "TIMEOUT_SEC         = None\n",
    "\n",
    "# Encoding thresholds\n",
    "MAX_ONEHOT_CARD     = 128   # ≤ this -> one-hot\n",
    "HASH_BINS           = 32    # > MAX_ONEHOT_CARD -> hashing to K bins\n",
    "\n",
    "REFIT_ON_TRAINVAL   = True\n",
    "\n",
    "# Tokens (for normalizing text before encoding)\n",
    "MISSING_TOKEN = \"Missing\"\n",
    "\n",
    "# ------------------------ UTILITIES ------------------------\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)   # Persian ZWNJ/RTL marks\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"int64\",\"int32\",\"float32\"]).columns:\n",
    "        if pdt.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].astype(object)\n",
    "            if s.astype(str).str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try:\n",
    "                    df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pdt.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]   = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"]  = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]    = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]    = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]   = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"] = s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]   = s.dt.is_month_end.astype(\"Int8\")\n",
    "    if dt_cols:\n",
    "        df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def resolve_target_name(df):\n",
    "    for k in [TARGET_ASCII, TARGET_PERSIAN, \"risk_class\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "        if k in df.columns: return k\n",
    "    norm = {re.sub(r\"[_\\-\\s]+\",\" \", str(c)).strip().lower(): c for c in df.columns}\n",
    "    for k in [\"target_risk_class\", \"risk_score\", \"risk class\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "        kk = re.sub(r\"[_\\-\\s]+\",\" \", k).strip().lower()\n",
    "        if kk in norm: return norm[kk]\n",
    "    return None\n",
    "\n",
    "# Stable 32-bit hash for strings\n",
    "def stable_hash32(val):\n",
    "    if pd.isna(val):\n",
    "        val = MISSING_TOKEN\n",
    "    if not isinstance(val, str):\n",
    "        val = str(val)\n",
    "    h = hashlib.sha1(val.encode(\"utf-8\")).hexdigest()[:8]\n",
    "    return int(h, 16)\n",
    "\n",
    "# ------------------------ PREPROCESSOR (one-hot + hashing) ------------------------\n",
    "class TabularPreprocessor:\n",
    "    \"\"\"\n",
    "    Learns from TRAIN only:\n",
    "      - numeric columns + medians\n",
    "      - low-card categorical columns and their categories (for fixed one-hot columns)\n",
    "      - high-card categorical columns and number of hash bins\n",
    "      - final feature_names_ (fixed order)\n",
    "    Transform returns a purely numeric float32 DataFrame aligned to feature_names_.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_onehot=MAX_ONEHOT_CARD, hash_bins=HASH_BINS):\n",
    "        self.max_onehot = int(max_onehot)\n",
    "        self.hash_bins  = int(hash_bins)\n",
    "        self.num_cols_  = []\n",
    "        self.cat_low_   = []            # columns one-hot encoded\n",
    "        self.cat_low_categories_ = {}   # col -> sorted categories list (incl MISSING)\n",
    "        self.cat_high_  = []            # columns hashed\n",
    "        self.num_median_ = {}           # col -> median\n",
    "        self.feature_names_ = []\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def _prep_base(self, df):\n",
    "        d = df.copy()\n",
    "        # normalize text-ish columns\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == object:\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        # booleans -> int8\n",
    "        for c in d.columns:\n",
    "            if pdt.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X):\n",
    "        d = self._prep_base(X)\n",
    "\n",
    "        # Decide numeric vs categorical by dtype after base prep\n",
    "        # Anything not numeric -> treat as categorical (object or category)\n",
    "        num_cols = [c for c in d.columns if pdt.is_numeric_dtype(d[c])]\n",
    "        cat_cols = [c for c in d.columns if not pdt.is_numeric_dtype(d[c])]\n",
    "\n",
    "        # Low vs high cardinality split (TRAIN only)\n",
    "        self.cat_low_  = []\n",
    "        self.cat_high_ = []\n",
    "        for c in cat_cols:\n",
    "            s = d[c].astype(object).where(pd.notna(d[c]), MISSING_TOKEN)\n",
    "            k = int(pd.Series(s).nunique(dropna=False))\n",
    "            if k <= self.max_onehot:\n",
    "                self.cat_low_.append(c)\n",
    "                # freeze categories for one-hot (sorted for stability)\n",
    "                cats = sorted(pd.Series(s).unique().tolist())\n",
    "                if MISSING_TOKEN not in cats:\n",
    "                    cats.append(MISSING_TOKEN)\n",
    "                self.cat_low_categories_[c] = cats\n",
    "            else:\n",
    "                self.cat_high_.append(c)\n",
    "\n",
    "        self.num_cols_ = num_cols\n",
    "\n",
    "        # Numeric medians\n",
    "        for c in self.num_cols_:\n",
    "            self.num_median_[c] = float(pd.to_numeric(d[c], errors=\"coerce\").median())\n",
    "\n",
    "        # Build feature_names_ by simulating transform on TRAIN\n",
    "        feats = []\n",
    "\n",
    "        # 1) numeric (kept as-is)\n",
    "        feats.extend(self.num_cols_)\n",
    "\n",
    "        # 2) one-hot columns -> fixed dummy names\n",
    "        for c in self.cat_low_:\n",
    "            cats = self.cat_low_categories_[c]\n",
    "            feats.extend([f\"{c}__oh__{v}\" for v in cats])\n",
    "\n",
    "        # 3) hashed columns -> fixed bin names\n",
    "        for c in self.cat_high_:\n",
    "            feats.extend([f\"{c}__h{b}\" for b in range(self.hash_bins)])\n",
    "\n",
    "        self.feature_names_ = feats\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def _encode_onehot(self, s, col):\n",
    "        # Ensure values are strings (no StringDtype) and map unknown -> MISSING_TOKEN\n",
    "        s = pd.Series(s, copy=False).astype(object).where(pd.notna(s), MISSING_TOKEN)\n",
    "        cats = set(self.cat_low_categories_[col])\n",
    "        s = s.where(s.isin(cats), MISSING_TOKEN)\n",
    "        # get dummies, then reindex to fixed columns\n",
    "        dummies = pd.get_dummies(s, prefix=f\"{col}__oh\", prefix_sep=\"__\", dummy_na=False)\n",
    "        # Column names are like f\"{col}__oh__value\"\n",
    "        target_cols = [f\"{col}__oh__{v}\" for v in self.cat_low_categories_[col]]\n",
    "        dummies = dummies.reindex(columns=target_cols, fill_value=0)\n",
    "        return dummies\n",
    "\n",
    "    def _encode_hash(self, s, col):\n",
    "        # Stable hash into HASH_BINS; returns a DataFrame with K indicator columns\n",
    "        s = pd.Series(s, copy=False).astype(object).where(pd.notna(s), MISSING_TOKEN)\n",
    "        idx = s.map(lambda v: stable_hash32(v) % self.hash_bins)\n",
    "        mat = np.zeros((len(s), self.hash_bins), dtype=np.float32)\n",
    "        rows = np.arange(len(s))\n",
    "        mat[rows, idx.values] = 1.0\n",
    "        cols = [f\"{col}__h{b}\" for b in range(self.hash_bins)]\n",
    "        return pd.DataFrame(mat, columns=cols, index=s.index)\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert self.fitted_, \"Call fit() first.\"\n",
    "        d = self._prep_base(X)\n",
    "\n",
    "        # Start empty matrix and fill blocks to avoid reindex per step\n",
    "        out = pd.DataFrame(index=d.index)\n",
    "\n",
    "        # 1) numeric\n",
    "        for c in self.num_cols_:\n",
    "            if c not in d.columns:\n",
    "                out[c] = float(self.num_median_[c])\n",
    "            else:\n",
    "                col = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "                col = col.fillna(self.num_median_[c]).astype(np.float32)\n",
    "                out[c] = col\n",
    "\n",
    "        # 2) one-hot categoricals\n",
    "        for c in self.cat_low_:\n",
    "            s = d[c] if c in d.columns else pd.Series([MISSING_TOKEN]*len(d), index=d.index)\n",
    "            block = self._encode_onehot(s, c)\n",
    "            out = pd.concat([out, block], axis=1)\n",
    "\n",
    "        # 3) hashed categoricals\n",
    "        for c in self.cat_high_:\n",
    "            s = d[c] if c in d.columns else pd.Series([MISSING_TOKEN]*len(d), index=d.index)\n",
    "            block = self._encode_hash(s, c)\n",
    "            out = pd.concat([out, block], axis=1)\n",
    "\n",
    "        # Final align & type\n",
    "        out = out.reindex(columns=self.feature_names_, fill_value=0).astype(np.float32, copy=False)\n",
    "        return out\n",
    "\n",
    "# ------------------------ LOAD & TARGET ------------------------\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "tgt = resolve_target_name(df)\n",
    "if tgt is None:\n",
    "    raise KeyError(\"Could not find target column. Expected 'target_risk_class' or 'risk_score'.\")\n",
    "\n",
    "y_1_10 = pd.to_numeric(df[tgt], errors=\"coerce\").astype(\"Int64\")\n",
    "y_1_10 = y_1_10.where((y_1_10>=1) & (y_1_10<=10))\n",
    "mask = y_1_10.notna()\n",
    "df = df.loc[mask].copy()\n",
    "y_1_10 = y_1_10.loc[mask].astype(\"int16\")\n",
    "y = (y_1_10 - 1).astype(\"int16\")  # 0..9\n",
    "\n",
    "X = df.drop(columns=[tgt])\n",
    "\n",
    "# ------------------------ SPLIT (70/15/15) ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "# Save indices\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(SPLIT_DIR/\"train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(SPLIT_DIR/\"val_indices.csv\",   index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(SPLIT_DIR/\"test_indices.csv\",  index=False)\n",
    "\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ PREPROCESS ------------------------\n",
    "pp = TabularPreprocessor(max_onehot=MAX_ONEHOT_CARD, hash_bins=HASH_BINS).fit(X_train)\n",
    "Xtr = pp.transform(X_train)\n",
    "Xva = pp.transform(X_val)\n",
    "Xte = pp.transform(X_test)\n",
    "\n",
    "# class-balanced weights\n",
    "classes_present = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_train)\n",
    "cw_map = {int(c): float(w) for c, w in zip(classes_present, class_weights)}\n",
    "w_train = y_train.map(cw_map).astype(\"float32\")\n",
    "w_val   = y_val.map(cw_map).astype(\"float32\")\n",
    "\n",
    "# ------------------------ DMATRIX (pure numeric) ------------------------\n",
    "def to_dmatrix_num(X, y=None, w=None):\n",
    "    Xc = X.astype(np.float32, copy=False)\n",
    "    return xgb.DMatrix(\n",
    "        Xc,\n",
    "        label=(y if y is not None else None),\n",
    "        weight=(w if w is not None else None),\n",
    "        nthread=NTHREAD\n",
    "    )\n",
    "\n",
    "# ------------------------ OPTUNA (xgb.train) ------------------------\n",
    "def suggest_xgb_params(trial):\n",
    "    params = {\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"num_class\": NUM_CLASSES,\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"eta\": trial.suggest_float(\"eta\", 0.02, 0.2, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 14),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 20.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.6, 1.0),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 0.0, 5.0),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 50.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 10.0, log=True),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 64, 512),\n",
    "        # XGBoost >= 3.1 GPU\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"nthread\": NTHREAD,\n",
    "        \"random_state\": SEED,\n",
    "    }\n",
    "    if trial.suggest_categorical(\"use_dart\", [False, True]):\n",
    "        params.update({\n",
    "            \"booster\": \"dart\",\n",
    "            \"rate_drop\": trial.suggest_float(\"rate_drop\", 0.0, 0.3),\n",
    "            \"skip_drop\": trial.suggest_float(\"skip_drop\", 0.0, 0.3),\n",
    "            \"sample_type\": trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"]),\n",
    "            \"normalize_type\": trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"]),\n",
    "        })\n",
    "    else:\n",
    "        params[\"booster\"] = \"gbtree\"\n",
    "    return params\n",
    "\n",
    "def train_booster(params, X_train, y_train, w_train, X_val, y_val, w_val, pruning_cb=None):\n",
    "    dtrain = to_dmatrix_num(X_train, y_train, w_train)\n",
    "    dval   = to_dmatrix_num(X_val,   y_val,   w_val)\n",
    "    evals = [(dval, \"validation_0\")]\n",
    "    callbacks = [xgb_cb.EarlyStopping(rounds=EARLY_STOP_ROUNDS, save_best=True)]\n",
    "    if pruning_cb is not None:\n",
    "        callbacks.append(pruning_cb)\n",
    "    booster = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=N_ESTIMATORS_LIMIT,\n",
    "        evals=evals,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    return booster\n",
    "\n",
    "def objective(trial):\n",
    "    params = suggest_xgb_params(trial)\n",
    "    pruning_cb = XGBoostPruningCallback(trial, \"validation_0-mlogloss\")\n",
    "    booster = train_booster(params, Xtr, y_train, w_train, Xva, y_val, w_val, pruning_cb=pruning_cb)\n",
    "    dval = to_dmatrix_num(Xva, y_val)\n",
    "    try:\n",
    "        br = getattr(booster, \"best_iteration\", None)\n",
    "        proba = booster.predict(dval, iteration_range=(0, br)) if br is not None else booster.predict(dval)\n",
    "    except TypeError:\n",
    "        bntl = getattr(booster, \"best_ntree_limit\", 0)\n",
    "        proba = booster.predict(dval, ntree_limit=bntl if bntl > 0 else 0)\n",
    "    if proba.ndim == 1:\n",
    "        proba = np.column_stack([1 - proba, proba])\n",
    "    return log_loss(y_val, proba, labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\",\n",
    "                            sampler=TPESampler(seed=SEED),\n",
    "                            pruner=MedianPruner(n_warmup_steps=10))\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_SEC, show_progress_bar=False)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": NUM_CLASSES,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\",\n",
    "    \"nthread\": NTHREAD, \"random_state\": SEED,\n",
    "})\n",
    "with open(OUTPUT_DIR / \"best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, ensure_ascii=False, indent=2)\n",
    "print(\"Best params:\", json.dumps(best_params, indent=2, ensure_ascii=False))\n",
    "\n",
    "# ------------------------ FINAL TRAIN ------------------------\n",
    "if REFIT_ON_TRAINVAL:\n",
    "    X_refit = pd.concat([X_train, X_val], axis=0)\n",
    "    y_refit = pd.concat([y_train,  y_val], axis=0)\n",
    "    w_refit = pd.concat([w_train,  w_val], axis=0)\n",
    "    X_refit_enc = pp.transform(X_refit)\n",
    "    X_eval_enc  = pp.transform(X_val)\n",
    "    X_train_use, y_train_use, w_train_use = X_refit_enc, y_refit, w_refit\n",
    "    X_eval_use,  y_eval_use,  w_eval_use  = X_eval_enc,  y_val,   w_val\n",
    "else:\n",
    "    X_train_use, y_train_use, w_train_use = Xtr, y_train, w_train\n",
    "    X_eval_use,  y_eval_use,  w_eval_use  = Xva, y_val,   w_val\n",
    "\n",
    "final_booster = train_booster(best_params, X_train_use, y_train_use, w_train_use, X_eval_use, y_eval_use, w_eval_use, pruning_cb=None)\n",
    "best_iter = getattr(final_booster, \"best_iteration\", None)\n",
    "print(\"Best iteration:\", best_iter)\n",
    "\n",
    "# ------------------------ EVALUATION ------------------------\n",
    "train_feature_names = list(X_train_use.columns)\n",
    "\n",
    "def predict_proba_booster(booster, Xs):\n",
    "    dm = to_dmatrix_num(Xs)\n",
    "    try:\n",
    "        br = getattr(booster, \"best_iteration\", None)\n",
    "        return booster.predict(dm, iteration_range=(0, br)) if br is not None else booster.predict(dm)\n",
    "    except TypeError:\n",
    "        bntl = getattr(booster, \"best_ntree_limit\", 0)\n",
    "        return booster.predict(dm, ntree_limit=bntl if bntl > 0 else 0)\n",
    "\n",
    "def eval_split(name, Xs_raw, ys_zero):\n",
    "    Xs = pp.transform(Xs_raw)\n",
    "    proba = predict_proba_booster(final_booster, Xs)\n",
    "    if proba.ndim == 1:\n",
    "        proba = np.column_stack([1 - proba, proba])\n",
    "    pred0 = np.argmax(proba, axis=1)\n",
    "    metrics = {\n",
    "        \"split\": name,\n",
    "        \"n_samples\": int(len(ys_zero)),\n",
    "        \"accuracy\": float(accuracy_score(ys_zero, pred0)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(ys_zero, pred0, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(ys_zero, proba, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = np.nan\n",
    "    try:\n",
    "        y_bin = pd.get_dummies(pd.Categorical(ys_zero, categories=list(range(NUM_CLASSES))))\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(roc_auc_score(y_bin.values, proba, average=\"macro\", multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = np.nan\n",
    "\n",
    "    ys_one  = ys_zero + 1\n",
    "    pred_one = pred0 + 1\n",
    "    report = classification_report(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0)\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics, pred_one, proba\n",
    "\n",
    "metrics_train, yhat_train_1_10, proba_train = eval_split(\"train\", X_train, y_train)\n",
    "metrics_val,   yhat_val_1_10,   proba_val   = eval_split(\"val\",   X_val,   y_val)\n",
    "metrics_test,  yhat_test_1_10,  proba_test  = eval_split(\"test\",  X_test,  y_test)\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_train, metrics_val, metrics_test])\n",
    "metrics_df.to_csv(OUTPUT_DIR / \"metrics_xgb_gpu.csv\", index=False)\n",
    "print(metrics_df)\n",
    "\n",
    "# ------------------------ FEATURE IMPORTANCE ------------------------\n",
    "score_gain = final_booster.get_score(importance_type='gain')\n",
    "imp = pd.DataFrame({\"feature\": list(score_gain.keys()), \"gain\": list(score_gain.values())}).sort_values(\"gain\", ascending=False)\n",
    "imp.to_csv(OUTPUT_DIR / \"feature_importance_gain.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10, max(4, min(16, len(imp.head(40)) * 0.25))))\n",
    "topk = imp.head(40).iloc[::-1]\n",
    "plt.barh(topk[\"feature\"], topk[\"gain\"])\n",
    "plt.title(\"XGBoost (GPU) Feature Importance (gain) - Top 40\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"feature_importance_gain_top40.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ------------------------ SAVE ARTIFACTS ------------------------\n",
    "final_booster.save_model(str(OUTPUT_DIR / \"xgb_gpu_multiclass.json\"))\n",
    "joblib.dump(pp, OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "with open(OUTPUT_DIR / \"training_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"target_name\": resolve_target_name(pd.read_csv(DATA_CSV, nrows=1)),\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"label_order_zero_indexed\": list(range(NUM_CLASSES)),\n",
    "        \"seed\": SEED,\n",
    "        \"best_iteration\": best_iter,\n",
    "        \"n_trials\": N_TRIALS,\n",
    "        \"refit_on_trainval\": REFIT_ON_TRAINVAL,\n",
    "        \"splits\": {\"train\": \"splits/train_indices.csv\", \"val\": \"splits/val_indices.csv\", \"test\": \"splits/test_indices.csv\"},\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"train_feature_names\": train_feature_names,\n",
    "        \"max_onehot_card\": MAX_ONEHOT_CARD,\n",
    "        \"hash_bins\": HASH_BINS\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ All artifacts saved to Google Drive at: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ INFERENCE HELPER ------------------------\n",
    "def predict_target_risk_class(df_new: pd.DataFrame,\n",
    "                              model_path=OUTPUT_DIR / \"xgb_gpu_multiclass.json\",\n",
    "                              preproc_path=OUTPUT_DIR / \"preprocessor.pkl\",\n",
    "                              meta_path=OUTPUT_DIR / \"training_meta.json\") -> pd.Series:\n",
    "    \"\"\"Predict on new risks (returns labels in 1..10). df_new is raw frame BEFORE preprocessing.\"\"\"\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(str(model_path))\n",
    "    preproc = joblib.load(preproc_path)\n",
    "    Xn = preproc.transform(df_new)\n",
    "    dm = xgb.DMatrix(Xn.astype(np.float32, copy=False), nthread=NTHREAD)\n",
    "    proba = booster.predict(dm)\n",
    "    if proba.ndim == 1:\n",
    "        proba = np.column_stack([1 - proba, proba])\n",
    "    pred1 = np.argmax(proba, axis=1) + 1\n",
    "    return pd.Series(pred1, index=df_new.index, name=\"pred_target_risk_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ob4iRedyKClF"
   },
   "source": [
    "#### LightGBM (it was trained with CPU and used in VSCODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7gcL6NdKClG",
    "outputId": "32417631-b687-4b7b-fb45-c5748949d36b"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Full, edited, single-cell pipeline (final):\n",
    "# - Labels 1..10 are shifted to 0..9 for LightGBM, then +1 on outputs\n",
    "# - Robust preprocessing (string/object -> categorical, datetime expansion, downcast)\n",
    "# - Stratified 70/15/15 split + saved indices\n",
    "# - Subset-based Optuna tuning (memory-light) with pruning (fixed valid name)\n",
    "# - Final LightGBM multiclass training + metrics & artifacts\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    log_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ configuration ------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# >>>>>>>>>>>> SET YOUR FILE HERE (prefer model_ready_ascii.csv) <<<<<<<<<<<<\n",
    "DATA_CSV = \"path\"\n",
    "\n",
    "OUTPUT_DIR = Path(\"path\"); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_DIR  = OUTPUT_DIR / \"splits\"; SPLIT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "TARGET_ASCII = \"target_risk_class\"\n",
    "TARGET_PERSIAN = \"risk score\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15  # of the whole dataset (applied after test split)\n",
    "\n",
    "# ---- Resource-friendly tuning/training settings ----\n",
    "SUBSET_FRAC = 0.20        # fraction of train+val used for Optuna tuning\n",
    "N_TRIALS    = 15          # tuning trials (increase on stronger machine)\n",
    "N_ESTIMATORS = 2500       # trees upper bound\n",
    "EARLY_STOP_ROUNDS = 300\n",
    "NUM_THREADS = 4           # reduce to 2 on laptop\n",
    "TIMEOUT_SEC = None        # e.g. 1800 for 30-min guard\n",
    "\n",
    "# ---- Eval wiring (fixes your valid-name mismatch) ----\n",
    "VALID_NAME  = \"val\"\n",
    "METRIC_NAME = \"multi_logloss\"\n",
    "\n",
    "# ------------------------ helpers ------------------------\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)  # remove ZWNJ/RTL/LTR marks\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def resolve_target_name(df):\n",
    "    candidates = [TARGET_ASCII, TARGET_PERSIAN, \"risk_class\", \"label\", \"target\", \"class\", \"y\"]\n",
    "    norm = {re.sub(r\"[_\\-\\s]+\",\" \", str(c)).strip().lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        k = re.sub(r\"[_\\-\\s]+\",\" \", cand).strip().lower()\n",
    "        if k in norm: return norm[k]\n",
    "    return None\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n",
    "        if pd.api.types.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    \"\"\"Lightweight parse for obvious date strings; skip heavy conversions.\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == \"object\":\n",
    "            s = df[c].astype(\"string\")\n",
    "            if s.str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try:\n",
    "                    df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]  = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"] = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]   = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]   = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]  = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"]= s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]  = s.dt.is_month_end.astype(\"Int8\")\n",
    "    df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def pd_cat_fix(series, allowed):\n",
    "    \"\"\"Coerce series to categorical with fixed categories; unseen -> 'Missing'.\"\"\"\n",
    "    s = series.astype(\"string\").fillna(\"Missing\")\n",
    "    s = s.where(s.isin(allowed), \"Missing\")\n",
    "    return pd.Categorical(s, categories=allowed, ordered=False)\n",
    "\n",
    "# ------------------------ Preprocessor ------------------------\n",
    "class TabularPreprocessor:\n",
    "    \"\"\"\n",
    "    Fits on train, then applies consistent transforms to val/test/inference:\n",
    "    - normalize whitespace\n",
    "    - downcast numeric\n",
    "    - parse + expand datetimes, drop originals\n",
    "    - convert non-numeric to categorical with a 'Missing' bucket\n",
    "    - store feature order, cat columns & categories, numeric columns\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.feature_names_ = None\n",
    "        self.cat_cols_ = []\n",
    "        self.cat_categories_ = {}\n",
    "        self.num_cols_ = []\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def _prep_base(self, df):\n",
    "        d = df.copy()\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == \"object\":\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        for c in d.columns:\n",
    "            if pd.api.types.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        d = self._prep_base(X)\n",
    "        self.num_cols_  = [c for c in d.columns if pd.api.types.is_numeric_dtype(d[c])]\n",
    "        self.cat_cols_  = [c for c in d.columns if c not in self.num_cols_]\n",
    "\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(\"string\").fillna(\"Missing\")\n",
    "            categories = pd.Index(pd.unique(s.dropna()))\n",
    "            if \"Missing\" not in categories:\n",
    "                categories = categories.insert(len(categories), \"Missing\")\n",
    "            self.cat_categories_[c] = categories.tolist()\n",
    "            d[c] = pd_cat_fix(s, self.cat_categories_[c])\n",
    "\n",
    "        self.feature_names_ = self.num_cols_ + self.cat_cols_\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame):\n",
    "        assert self.fitted_, \"Preprocessor not fitted.\"\n",
    "        d = self._prep_base(X)\n",
    "        for c in self.feature_names_:\n",
    "            if c not in d.columns:\n",
    "                d[c] = np.nan\n",
    "        d = d[self.feature_names_].copy()\n",
    "        for c in self.num_cols_:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\")\n",
    "            if d[c].isna().any():\n",
    "                d[c] = d[c].fillna(d[c].median())\n",
    "        for c in self.cat_cols_:\n",
    "            allowed = self.cat_categories_[c]\n",
    "            d[c] = pd_cat_fix(d[c], allowed)\n",
    "        return d\n",
    "\n",
    "# ------------------------ data load & target ------------------------\n",
    "df_raw = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "tgt = resolve_target_name(df_raw)\n",
    "if tgt is None:\n",
    "    raise KeyError(\"Could not find target column. Expected 'target_risk_class' or 'risk score'.\")\n",
    "\n",
    "# sanitize target: keep only labels in 1..10, drop NaN labels\n",
    "y_all_1_10 = pd.to_numeric(df_raw[tgt], errors=\"coerce\").astype(\"Int64\")\n",
    "y_all_1_10 = y_all_1_10.where((y_all_1_10 >= 1) & (y_all_1_10 <= 10))\n",
    "mask_labeled = y_all_1_10.notna()\n",
    "df_raw = df_raw.loc[mask_labeled].copy()\n",
    "y_all_1_10 = y_all_1_10.loc[mask_labeled].astype(\"int16\")\n",
    "\n",
    "# ---- shift to 0..9 for LightGBM ----\n",
    "y_all = (y_all_1_10 - 1).astype(\"int16\")\n",
    "\n",
    "X_all = df_raw.drop(columns=[tgt])\n",
    "\n",
    "# ------------------------ split (stratified 70/15/15) ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X_all, y_all))\n",
    "X_trainval, X_test = X_all.iloc[trainval_idx], X_all.iloc[test_idx]\n",
    "y_trainval, y_test = y_all.iloc[trainval_idx], y_all.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "# save split indices (relative to labeled subset)\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(SPLIT_DIR/\"train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(SPLIT_DIR/\"val_indices.csv\",   index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(SPLIT_DIR/\"test_indices.csv\",  index=False)\n",
    "\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ SUBSET-BASED tuning set ------------------------\n",
    "# pick a stratified subset from TRAIN+VAL pool to reduce memory during Optuna\n",
    "sss_sub = StratifiedShuffleSplit(n_splits=1, train_size=SUBSET_FRAC, random_state=SEED)\n",
    "subset_idx, _ = next(sss_sub.split(X_trainval, y_trainval))\n",
    "X_train_sub = X_trainval.iloc[subset_idx]\n",
    "y_train_sub = y_trainval.iloc[subset_idx]\n",
    "print(f\"Tuning subset: {X_train_sub.shape} from train+val {X_trainval.shape}\")\n",
    "\n",
    "# ------------------------ preprocessing ------------------------\n",
    "# Preprocessor for tuning subset\n",
    "pp_tune = TabularPreprocessor().fit(X_train_sub)\n",
    "Xtr_sub = pp_tune.transform(X_train_sub)   # tuning train\n",
    "Xva_sub = pp_tune.transform(X_val)         # use full VAL as holdout during tuning\n",
    "\n",
    "# Preprocessor for FINAL model (fit on full TRAIN)\n",
    "pp = TabularPreprocessor().fit(X_train)\n",
    "Xtr = pp.transform(X_train)\n",
    "Xva = pp.transform(X_val)\n",
    "Xte = pp.transform(X_test)\n",
    "\n",
    "# categorical indices\n",
    "cat_idx_sub = [Xtr_sub.columns.get_loc(c) for c in Xtr_sub.columns\n",
    "               if pd.api.types.is_categorical_dtype(Xtr_sub[c])]\n",
    "cat_idx = [Xtr.columns.get_loc(c) for c in Xtr.columns\n",
    "           if pd.api.types.is_categorical_dtype(Xtr[c])]\n",
    "\n",
    "# class weights -> sample weights (zero-index labels)\n",
    "classes_present = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_train)\n",
    "class_weight_map = {int(c): float(w) for c, w in zip(classes_present, class_weights)}\n",
    "w_train_sub = y_train_sub.map(class_weight_map).astype(\"float32\")\n",
    "w_train = y_train.map(class_weight_map).astype(\"float32\")\n",
    "\n",
    "# LightGBM Dataset objects for TUNING\n",
    "dtrain_sub = lgb.Dataset(\n",
    "    Xtr_sub, label=y_train_sub.values, weight=w_train_sub.values,\n",
    "    categorical_feature=cat_idx_sub, free_raw_data=False\n",
    ")\n",
    "dval_sub = lgb.Dataset(\n",
    "    Xva_sub, label=y_val.values,\n",
    "    categorical_feature=cat_idx_sub, reference=dtrain_sub, free_raw_data=False\n",
    ")\n",
    "dtrain_sub.save_binary = True\n",
    "dval_sub.save_binary   = True\n",
    "\n",
    "# ------------------------ Optuna tuning (on subset) ------------------------\n",
    "def objective(trial: optuna.Trial):\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"num_class\": NUM_CLASSES,\n",
    "        \"metric\": METRIC_NAME,\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting\": trial.suggest_categorical(\"boosting\", [\"gbdt\", \"dart\"]),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.15, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 127, step=8),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 300, 1000),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 0.9),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 0.9),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 5),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 5.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0.0, 2.0),\n",
    "        \"extra_trees\": trial.suggest_categorical(\"extra_trees\", [False, True]),\n",
    "        \"max_bin\": trial.suggest_int(\"max_bin\", 63, 127),\n",
    "        \"seed\": SEED,\n",
    "        \"force_row_wise\": True,\n",
    "        \"deterministic\": True,\n",
    "        \"num_threads\": NUM_THREADS,\n",
    "    }\n",
    "    if params[\"boosting\"] == \"dart\":\n",
    "        params[\"drop_rate\"] = trial.suggest_float(\"drop_rate\", 0.05, 0.2)\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=EARLY_STOP_ROUNDS, verbose=False),\n",
    "        lgb.log_evaluation(period=0),\n",
    "        # IMPORTANT: valid_name matches valid_names passed to lgb.train\n",
    "        optuna.integration.LightGBMPruningCallback(trial, METRIC_NAME, valid_name=VALID_NAME),\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        dtrain_sub,\n",
    "        num_boost_round=N_ESTIMATORS,\n",
    "        valid_sets=[dval_sub],\n",
    "        valid_names=[VALID_NAME],\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "\n",
    "    # Return best score from the same VALID_NAME/METRIC_NAME\n",
    "    try:\n",
    "        return float(model.best_score[VALID_NAME][METRIC_NAME])\n",
    "    except KeyError:\n",
    "        ev = model.evals_result_\n",
    "        return float(ev[VALID_NAME][METRIC_NAME][-1])\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=TPESampler(seed=SEED),\n",
    "    pruner=MedianPruner(n_warmup_steps=10),\n",
    ")\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_SEC, show_progress_bar=False)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params.update({\"num_threads\": NUM_THREADS, \"force_row_wise\": True, \"deterministic\": True})\n",
    "with open(OUTPUT_DIR / \"best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, ensure_ascii=False, indent=2)\n",
    "print(\"Best params:\", json.dumps(best_params, indent=2, ensure_ascii=False))\n",
    "print(\"Best val multi_logloss:\", study.best_value)\n",
    "\n",
    "# ------------------------ Final training on FULL TRAIN (validate on VAL) ------------------------\n",
    "# build final datasets from FULL train/val (preprocessed with pp)\n",
    "dtrain = lgb.Dataset(\n",
    "    Xtr, label=y_train.values, weight=w_train.values,\n",
    "    categorical_feature=cat_idx, free_raw_data=False\n",
    ")\n",
    "dval = lgb.Dataset(\n",
    "    Xva, label=y_val.values,\n",
    "    categorical_feature=cat_idx, reference=dtrain, free_raw_data=False\n",
    ")\n",
    "dtrain.save_binary = True\n",
    "dval.save_binary   = True\n",
    "\n",
    "final_params = {\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"num_class\": NUM_CLASSES,\n",
    "    \"metric\": METRIC_NAME,\n",
    "    \"verbosity\": -1,\n",
    "    \"seed\": SEED,\n",
    "    **best_params,\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=EARLY_STOP_ROUNDS, verbose=True),\n",
    "    lgb.log_evaluation(period=100),\n",
    "]\n",
    "\n",
    "final_model = lgb.train(\n",
    "    final_params,\n",
    "    dtrain,\n",
    "    num_boost_round=N_ESTIMATORS,\n",
    "    valid_sets=[dval],\n",
    "    valid_names=[VALID_NAME],\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "best_iter = final_model.best_iteration\n",
    "print(\"Best iteration:\", best_iter)\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "def eval_split(name, Xs: pd.DataFrame, ys_zero: pd.Series):\n",
    "    \"\"\"ys_zero expected in 0..9; we also save 1..10 artifacts for readability.\"\"\"\n",
    "    d = pp.transform(Xs)\n",
    "    proba = final_model.predict(d, num_iteration=best_iter)            # (n, 10) for classes 0..9\n",
    "    pred_zero  = np.argmax(proba, axis=1)                              # 0..9\n",
    "    # Metrics computed in zero-index space (matches proba columns)\n",
    "    metrics = {\n",
    "        \"split\": name,\n",
    "        \"n_samples\": int(len(ys_zero)),\n",
    "        \"accuracy\": float(accuracy_score(ys_zero, pred_zero)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(ys_zero, pred_zero, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(ys_zero, proba, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = np.nan\n",
    "    try:\n",
    "        y_bin = pd.get_dummies(pd.Categorical(ys_zero, categories=list(range(NUM_CLASSES))))\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(roc_auc_score(y_bin.values, proba, average=\"macro\", multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = np.nan\n",
    "\n",
    "    # Save human-readable reports in 1..10 label space\n",
    "    ys_one  = ys_zero + 1\n",
    "    pred_one = pred_zero + 1\n",
    "    report = classification_report(\n",
    "        ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0, output_dict=False\n",
    "    )\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics, pred_one, proba\n",
    "\n",
    "metrics_train, yhat_train_1_10, proba_train = eval_split(\"train\", X_train, y_train)\n",
    "metrics_val,   yhat_val_1_10,   proba_val   = eval_split(\"val\",   X_val,   y_val)\n",
    "metrics_test,  yhat_test_1_10,  proba_test  = eval_split(\"test\",  X_test,  y_test)\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_train, metrics_val, metrics_test])\n",
    "metrics_df.to_csv(OUTPUT_DIR / \"metrics_lightgbm_optuna.csv\", index=False)\n",
    "print(metrics_df)\n",
    "\n",
    "# ------------------------ Feature importance ------------------------\n",
    "imp = pd.DataFrame({\n",
    "    \"feature\": final_model.feature_name(),\n",
    "    \"gain\": final_model.feature_importance(importance_type=\"gain\")\n",
    "}).sort_values(\"gain\", ascending=False)\n",
    "imp.to_csv(OUTPUT_DIR / \"feature_importance_gain.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10, max(4, min(16, len(imp.head(40)) * 0.25))))\n",
    "topk = imp.head(40).iloc[::-1]\n",
    "plt.barh(topk[\"feature\"], topk[\"gain\"])\n",
    "plt.title(\"LightGBM Feature Importance (gain) - Top 40\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"feature_importance_gain_top40.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ------------------------ Save artifacts ------------------------\n",
    "joblib.dump(final_model, OUTPUT_DIR / \"lightgbm_booster.pkl\")\n",
    "joblib.dump(pp,           OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "with open(OUTPUT_DIR / \"training_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"target_name\": tgt,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"label_order_zero_indexed\": list(range(NUM_CLASSES)),  # trained on 0..9\n",
    "        \"best_iteration\": int(best_iter),\n",
    "        \"seed\": SEED,\n",
    "        \"num_threads\": NUM_THREADS,\n",
    "        \"splits\": {\"train\": \"splits/train_indices.csv\", \"val\": \"splits/val_indices.csv\", \"test\": \"splits/test_indices.csv\"},\n",
    "        \"data_csv\": DATA_CSV\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nArtifacts saved in: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ Inference helper ------------------------\n",
    "def predict_target_risk_class(df_new: pd.DataFrame,\n",
    "                              model_path=OUTPUT_DIR / \"lightgbm_booster.pkl\",\n",
    "                              preproc_path=OUTPUT_DIR / \"preprocessor.pkl\") -> pd.Series:\n",
    "    \"\"\"Predict on new risks. df_new should have the same raw columns as training BEFORE preprocessing.\n",
    "       Returns labels in 1..10.\"\"\"\n",
    "    booster = joblib.load(model_path)\n",
    "    preproc = joblib.load(preproc_path)\n",
    "    d = preproc.transform(df_new)\n",
    "    proba = booster.predict(d, num_iteration=getattr(booster, \"best_iteration\", None))\n",
    "    pred = np.argmax(proba, axis=1) + 1  # map back to 1..10\n",
    "    return pd.Series(pred, index=df_new.index, name=\"pred_target_risk_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv6k9GYESOYT"
   },
   "source": [
    "#### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9958,
     "status": "ok",
     "timestamp": 1761641584880,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "40cFo0c9P4Cx",
    "outputId": "14b497ef-822e-4538-f8d6-489aa7d7190d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
      "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
      "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: catboost\n",
      "Successfully installed catboost-1.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5894662,
     "status": "ok",
     "timestamp": 1761665532662,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "CFnWd1tOSQAr",
    "outputId": "ed142817-c69e-473a-940a-8aebb4e948f6"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# CatBoost (GPU / A100) Multiclass Pipeline (1..10 -> 0..9 labels)\n",
    "# - Colab-ready: saves under RESULTS/CatBoost_GPU/<timestamp>\n",
    "# - Robust preprocessing (datetime expansion, numeric downcast, strings->object)\n",
    "# - CatBoost native categorical hashing (handles unseen categories)\n",
    "# - Stratified 70/15/15 split + saved indices\n",
    "# - Optuna tuning (GPU-safe search space)\n",
    "# - Early stopping via od_type='Iter'\n",
    "# - Metrics: accuracy, macro/weighted P/R/F1, log loss, ROC-AUC OvR macro\n",
    "# - Artifacts: model.cbm, preprocessor.pkl, best_params.json, metrics.csv, reports, confusions, importances\n",
    "# - Inference helper returns labels in 1..10\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, warnings, datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    log_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ COLAB DRIVE SETUP ------------------------\n",
    "drive_folder = 'path'\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive\n",
    "    if not Path(\"/content/drive\").exists() or not any(Path(\"/content/drive\").iterdir()):\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Subfolder for this run (timestamped)\n",
    "RUN_NAME = f\"CatBoost_GPU_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUTPUT_DIR = Path(drive_folder) / \"CatBoost_GPU\" / RUN_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_DIR  = OUTPUT_DIR / \"splits\"; SPLIT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Saving all outputs to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"   # A100 in Colab\n",
    "\n",
    "# >>>>>>>>>>>> SET THIS TO YOUR FILE <<<<<<<<<<<<\n",
    "DATA_CSV = \"path\"\n",
    "\n",
    "TARGET_ASCII   = \"target_risk_class\"\n",
    "TARGET_PERSIAN = \"path\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "\n",
    "# Tuning / training controls\n",
    "N_TRIALS            = 50          # increase if you want deeper search\n",
    "N_ESTIMATORS_LIMIT  = 5000        # upper cap; early stopping stops earlier\n",
    "EARLY_STOP_ROUNDS   = 400\n",
    "NTHREAD             = os.cpu_count()\n",
    "TIMEOUT_SEC         = None\n",
    "REFIT_ON_TRAINVAL   = True\n",
    "\n",
    "MISSING_TOKEN = \"Missing\"\n",
    "\n",
    "# ------------------------ UTILITIES ------------------------\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)   # Persian ZWNJ/RTL/LTR marks\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"int64\",\"int32\",\"float32\"]).columns:\n",
    "        if pdt.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].astype(object)\n",
    "            if s.astype(str).str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try:\n",
    "                    df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pdt.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]   = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"]  = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]    = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]    = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]   = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"] = s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]   = s.dt.is_month_end.astype(\"Int8\")\n",
    "    if dt_cols:\n",
    "        df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def resolve_target_name(df):\n",
    "    for k in [TARGET_ASCII, TARGET_PERSIAN, \"risk_class\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "        if k in df.columns: return k\n",
    "    norm = {re.sub(r\"[_\\-\\s]+\",\" \", str(c)).strip().lower(): c for c in df.columns}\n",
    "    for k in [\"target_risk_class\", \"طبقه خطر\", \"risk class\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "        kk = re.sub(r\"[_\\-\\s]+\",\" \", k).strip().lower()\n",
    "        if kk in norm: return norm[kk]\n",
    "    return None\n",
    "\n",
    "# ------------------------ PREPROCESSOR (CatBoost-friendly) ------------------------\n",
    "class TabularPreprocessor:\n",
    "    \"\"\"\n",
    "    - Expands datetimes, normalizes whitespace\n",
    "    - Numerics -> float32 with median impute\n",
    "    - Categoricals -> object strings with Missing filled (CatBoost hashes them)\n",
    "    - Keeps fixed feature order; exposes cat feature indices for CatBoost Pool\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_cols_  = []\n",
    "        self.cat_cols_  = []\n",
    "        self.num_median_ = {}\n",
    "        self.feature_names_ = []\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def _prep_base(self, df):\n",
    "        d = df.copy()\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == object:\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        for c in d.columns:\n",
    "            if pdt.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X):\n",
    "        d = self._prep_base(X)\n",
    "        self.num_cols_ = [c for c in d.columns if pdt.is_numeric_dtype(d[c])]\n",
    "        self.cat_cols_ = [c for c in d.columns if not pdt.is_numeric_dtype(d[c])]\n",
    "        for c in self.num_cols_:\n",
    "            self.num_median_[c] = float(pd.to_numeric(d[c], errors=\"coerce\").median())\n",
    "        self.feature_names_ = self.num_cols_ + self.cat_cols_\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert self.fitted_\n",
    "        d = self._prep_base(X)\n",
    "        # ensure all training features exist\n",
    "        for c in self.feature_names_:\n",
    "            if c not in d.columns:\n",
    "                d[c] = np.nan\n",
    "        d = d[self.feature_names_].copy()\n",
    "        # numerics -> float32 + median\n",
    "        for c in self.num_cols_:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").astype(\"float32\")\n",
    "            if d[c].isna().any():\n",
    "                d[c] = d[c].fillna(self.num_median_[c])\n",
    "        # categoricals -> object strings; fill Missing\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(object)\n",
    "            d[c] = s.where(pd.notna(s), MISSING_TOKEN).astype(object)\n",
    "        return d\n",
    "\n",
    "    def cat_feature_indices(self):\n",
    "        return [self.feature_names_.index(c) for c in self.cat_cols_]\n",
    "\n",
    "# ------------------------ LOAD & TARGET ------------------------\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "tgt = resolve_target_name(df)\n",
    "if tgt is None:\n",
    "    raise KeyError(\"Could not find target column. Expected 'target_risk_class' or 'risk sore'.\")\n",
    "\n",
    "y_1_10 = pd.to_numeric(df[tgt], errors=\"coerce\").astype(\"Int64\")\n",
    "y_1_10 = y_1_10.where((y_1_10>=1) & (y_1_10<=10))\n",
    "mask = y_1_10.notna()\n",
    "df = df.loc[mask].copy()\n",
    "y_1_10 = y_1_10.loc[mask].astype(\"int16\")\n",
    "y = (y_1_10 - 1).astype(\"int16\")  # 0..9\n",
    "\n",
    "X = df.drop(columns=[tgt])\n",
    "\n",
    "# ------------------------ SPLIT (70/15/15) ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "# Save indices\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(SPLIT_DIR/\"train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(SPLIT_DIR/\"val_indices.csv\",   index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(SPLIT_DIR/\"test_indices.csv\",  index=False)\n",
    "\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ PREPROCESS ------------------------\n",
    "pp = TabularPreprocessor().fit(X_train)\n",
    "Xtr = pp.transform(X_train)\n",
    "Xva = pp.transform(X_val)\n",
    "Xte = pp.transform(X_test)\n",
    "cat_idx = pp.cat_feature_indices()\n",
    "\n",
    "# class-balanced weights (length = NUM_CLASSES for 0..9)\n",
    "classes_present = np.unique(y_train)\n",
    "class_weights_arr = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_train)\n",
    "cw_map = {int(c): float(w) for c, w in zip(classes_present, class_weights_arr)}\n",
    "class_weights = [cw_map.get(k, 1.0) for k in range(NUM_CLASSES)]\n",
    "\n",
    "# Pools (CatBoost requires Pool with cat_features indices)\n",
    "train_pool = Pool(Xtr, label=y_train, cat_features=cat_idx)\n",
    "val_pool   = Pool(Xva, label=y_val,   cat_features=cat_idx)\n",
    "test_pool  = Pool(Xte, label=y_test,  cat_features=cat_idx)\n",
    "\n",
    "# ------------------------ OPTUNA TUNING ------------------------\n",
    "def suggest_cat_params(trial):\n",
    "    # GPU-safe bootstrap options for multiclass\n",
    "    bootstrap_type = trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"])\n",
    "\n",
    "    if bootstrap_type == \"Bayesian\":\n",
    "        bagging_temperature = trial.suggest_float(\"bagging_temperature\", 0.0, 2.0)\n",
    "        subsample = 1.0\n",
    "    else:  # Bernoulli\n",
    "        bagging_temperature = None\n",
    "        subsample = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "\n",
    "    params = dict(\n",
    "        loss_function=\"MultiClass\",\n",
    "        classes_count=NUM_CLASSES,\n",
    "        eval_metric=\"MultiClass\",\n",
    "\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.02, 0.2, log=True),\n",
    "        depth=trial.suggest_int(\"depth\", 4, 10),\n",
    "        l2_leaf_reg=trial.suggest_float(\"l2_leaf_reg\", 1e-3, 100.0, log=True),\n",
    "        random_strength=trial.suggest_float(\"random_strength\", 0.0, 2.0),\n",
    "\n",
    "        # GPU\n",
    "        task_type=\"GPU\",\n",
    "        devices=\"0\",\n",
    "        gpu_ram_part=0.95,\n",
    "\n",
    "        # Sampling\n",
    "        bootstrap_type=bootstrap_type,\n",
    "        one_hot_max_size=128,\n",
    "\n",
    "        # Optional speedup for large data\n",
    "        border_count=128,\n",
    "\n",
    "        random_seed=SEED,\n",
    "        thread_count=NTHREAD,\n",
    "        verbose=False,\n",
    "        allow_writing_files=False,\n",
    "\n",
    "        iterations=N_ESTIMATORS_LIMIT,\n",
    "        od_type=\"Iter\",\n",
    "        od_wait=EARLY_STOP_ROUNDS,\n",
    "        use_best_model=True,\n",
    "        class_weights=class_weights,\n",
    "    )\n",
    "\n",
    "    if bootstrap_type == \"Bernoulli\":\n",
    "        params[\"subsample\"] = subsample\n",
    "    else:  # Bayesian\n",
    "        params[\"bagging_temperature\"] = bagging_temperature\n",
    "\n",
    "    # NOTE: Do NOT set `rsm` on GPU multiclass (unsupported).\n",
    "    # NOTE: Do NOT use `MVS` on GPU multiclass (unsupported).\n",
    "    return params\n",
    "\n",
    "def objective(trial):\n",
    "    params = suggest_cat_params(trial)\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "    proba = model.predict_proba(val_pool)\n",
    "    return log_loss(y_val, proba, labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\",\n",
    "                            sampler=TPESampler(seed=SEED),\n",
    "                            pruner=MedianPruner(n_warmup_steps=10))\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_SEC, show_progress_bar=False)\n",
    "\n",
    "best = study.best_params.copy()\n",
    "\n",
    "# Safety guard in case an old artifact had MVS or other incompatible options\n",
    "if best.get(\"bootstrap_type\") not in (\"Bayesian\", \"Bernoulli\"):\n",
    "    best[\"bootstrap_type\"] = \"Bernoulli\"\n",
    "    if \"subsample\" not in best:\n",
    "        best[\"subsample\"] = 0.8\n",
    "\n",
    "# Normalize final params\n",
    "best_params = dict(\n",
    "    loss_function=\"MultiClass\",\n",
    "    classes_count=NUM_CLASSES,\n",
    "    eval_metric=\"MultiClass\",\n",
    "\n",
    "    task_type=\"GPU\",\n",
    "    devices=\"0\",\n",
    "    gpu_ram_part=0.95,\n",
    "    one_hot_max_size=128,\n",
    "    border_count=128,\n",
    "\n",
    "    random_seed=SEED,\n",
    "    thread_count=NTHREAD,\n",
    "    verbose=100,\n",
    "    allow_writing_files=False,\n",
    "\n",
    "    iterations=N_ESTIMATORS_LIMIT,\n",
    "    od_type=\"Iter\",\n",
    "    od_wait=EARLY_STOP_ROUNDS,\n",
    "    use_best_model=True,\n",
    "    class_weights=class_weights,\n",
    "\n",
    "    # tuned core params\n",
    "    learning_rate=best.get(\"learning_rate\"),\n",
    "    depth=best.get(\"depth\"),\n",
    "    l2_leaf_reg=best.get(\"l2_leaf_reg\"),\n",
    "    random_strength=best.get(\"random_strength\"),\n",
    "    bootstrap_type=best.get(\"bootstrap_type\"),\n",
    ")\n",
    "\n",
    "if best_params[\"bootstrap_type\"] == \"Bernoulli\":\n",
    "    if \"subsample\" in best: best_params[\"subsample\"] = best[\"subsample\"]\n",
    "else:  # Bayesian\n",
    "    if \"bagging_temperature\" in best:\n",
    "        best_params[\"bagging_temperature\"] = best[\"bagging_temperature\"]\n",
    "\n",
    "with open(OUTPUT_DIR / \"best_params.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(best_params, f, ensure_ascii=False, indent=2)\n",
    "print(\"Best params:\", json.dumps(best_params, indent=2, ensure_ascii=False))\n",
    "\n",
    "# ------------------------ FINAL TRAIN ------------------------\n",
    "if REFIT_ON_TRAINVAL:\n",
    "    X_refit = pd.concat([X_train, X_val], axis=0)\n",
    "    y_refit = pd.concat([y_train,  y_val], axis=0)\n",
    "    X_refit_enc = pp.transform(X_refit)\n",
    "    train_pool_final = Pool(X_refit_enc, label=y_refit, cat_features=cat_idx)\n",
    "    eval_pool_final  = Pool(pp.transform(X_val), label=y_val, cat_features=cat_idx)\n",
    "else:\n",
    "    train_pool_final = train_pool\n",
    "    eval_pool_final  = val_pool\n",
    "\n",
    "final_model = CatBoostClassifier(**best_params)\n",
    "final_model.fit(train_pool_final, eval_set=eval_pool_final)  # verbose handled by params\n",
    "best_iter = getattr(final_model, \"best_iteration_\", None)\n",
    "print(\"Best iteration:\", best_iter)\n",
    "\n",
    "# ------------------------ EVALUATION ------------------------\n",
    "def eval_split(name, Xs_raw, ys_zero):\n",
    "    Xs = pp.transform(Xs_raw)\n",
    "    pool = Pool(Xs, label=ys_zero, cat_features=cat_idx)\n",
    "    proba = final_model.predict_proba(pool)\n",
    "    pred0 = np.argmax(proba, axis=1)\n",
    "\n",
    "    metrics = {\n",
    "        \"split\": name,\n",
    "        \"n_samples\": int(len(ys_zero)),\n",
    "        \"accuracy\": float(accuracy_score(ys_zero, pred0)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(ys_zero, pred0, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(ys_zero, proba, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = float(\"nan\")\n",
    "\n",
    "    try:\n",
    "        y_bin = pd.get_dummies(pd.Categorical(ys_zero, categories=list(range(NUM_CLASSES))))\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(roc_auc_score(y_bin.values, proba, average=\"macro\", multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(\"nan\")\n",
    "\n",
    "    # Human-friendly reports in 1..10 space\n",
    "    ys_one  = ys_zero + 1\n",
    "    pred_one = pred0 + 1\n",
    "    report = classification_report(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0)\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics, pred_one, proba\n",
    "\n",
    "metrics_train, _, _ = eval_split(\"train\", X_train, y_train)\n",
    "metrics_val,   _, _ = eval_split(\"val\",   X_val,   y_val)\n",
    "metrics_test,  _, _ = eval_split(\"test\",  X_test,  y_test)\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics_train, metrics_val, metrics_test])\n",
    "metrics_df.to_csv(OUTPUT_DIR / \"metrics_catboost_gpu.csv\", index=False)\n",
    "print(metrics_df)\n",
    "\n",
    "# ------------------------ FEATURE IMPORTANCE ------------------------\n",
    "imp_vals = final_model.get_feature_importance(train_pool_final, type=\"PredictionValuesChange\")\n",
    "imp = pd.DataFrame({\"feature\": pp.feature_names_, \"importance\": imp_vals}).sort_values(\"importance\", ascending=False)\n",
    "imp.to_csv(OUTPUT_DIR / \"feature_importance_prediction_values_change.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10, max(4, min(16, len(imp.head(40)) * 0.25))))\n",
    "topk = imp.head(40).iloc[::-1]\n",
    "plt.barh(topk[\"feature\"], topk[\"importance\"])\n",
    "plt.title(\"CatBoost (GPU) Feature Importance (PredictionValuesChange) - Top 40\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"feature_importance_top40.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ------------------------ SAVE ARTIFACTS ------------------------\n",
    "final_model.save_model(str(OUTPUT_DIR / \"catboost_multiclass.cbm\"))\n",
    "joblib.dump(pp, OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "with open(OUTPUT_DIR / \"training_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"target_name\": tgt,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"label_order_zero_indexed\": list(range(NUM_CLASSES)),\n",
    "        \"seed\": SEED,\n",
    "        \"best_iteration\": int(best_iter) if best_iter is not None else None,\n",
    "        \"n_trials\": N_TRIALS,\n",
    "        \"refit_on_trainval\": REFIT_ON_TRAINVAL,\n",
    "        \"splits\": {\"train\": \"splits/train_indices.csv\", \"val\": \"splits/val_indices.csv\", \"test\": \"splits/test_indices.csv\"},\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"feature_names\": pp.feature_names_,\n",
    "        \"cat_features\": pp.cat_cols_,\n",
    "        \"num_features\": pp.num_cols_\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ All artifacts saved to Google Drive at: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ INFERENCE HELPER ------------------------\n",
    "def predict_target_risk_class(df_new: pd.DataFrame,\n",
    "                              model_path=OUTPUT_DIR / \"catboost_multiclass.cbm\",\n",
    "                              preproc_path=OUTPUT_DIR / \"preprocessor.pkl\",\n",
    "                              meta_path=OUTPUT_DIR / \"training_meta.json\") -> pd.Series:\n",
    "    \"\"\"Predict on new risks (returns labels in 1..10). df_new is raw frame BEFORE preprocessing.\"\"\"\n",
    "    model = CatBoostClassifier()\n",
    "    model.load_model(str(model_path))\n",
    "    preproc = joblib.load(preproc_path)\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    d = preproc.transform(df_new)\n",
    "    cat_idx = [preproc.feature_names_.index(c) for c in preproc.cat_cols_]\n",
    "    pool = Pool(d, cat_features=cat_idx)\n",
    "    proba = model.predict_proba(pool)\n",
    "    pred1 = np.argmax(proba, axis=1) + 1\n",
    "    return pd.Series(pred1, index=df_new.index, name=\"pred_target_risk_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlupx7C0Mq3T"
   },
   "source": [
    "### Transformers Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3BjoXBRND4y"
   },
   "source": [
    "#### tabM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIOPqlZAEMzo"
   },
   "source": [
    "##### CPU/TPU ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13097123,
     "status": "ok",
     "timestamp": 1761954674400,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "7s8vC9OuMuMB",
    "outputId": "5774d0bd-413d-45d3-cd7f-b69aeb64204b"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# TabM (official) – Low-RAM, FIXED for hyphenated columns & unique categories\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, warnings, datetime, math, time, random, subprocess, sys, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    log_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ COLAB DRIVE SETUP ------------------------\n",
    "drive_folder = 'path'\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive\n",
    "    if not Path(\"/content/drive\").exists() or not any(Path(\"/content/drive\").iterdir()):\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ------------------------ Install official packages ------------------------\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "                \"tabm>=0.0.3\", \"rtdl_num_embeddings>=0.0.12\"], check=True)\n",
    "from tabm import TabM\n",
    "from rtdl_num_embeddings import LinearReLUEmbeddings\n",
    "\n",
    "# ------------------------ RUN/OUTPUT DIR ------------------------\n",
    "RUN_NAME = f\"TabM_LowRAM_Fixed_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUTPUT_DIR = Path(drive_folder) / \"TabM_Package\" / RUN_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_DIR  = OUTPUT_DIR / \"splits\"; SPLIT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Saving all outputs to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.set_num_threads(max(1, (os.cpu_count() or 4)//2))\n",
    "\n",
    "# >>>>>>>>>>>> SET THIS TO YOUR FILE <<<<<<<<<<<<\n",
    "DATA_CSV = \"path\"\n",
    "\n",
    "TARGET_NAME = \"target_risk_class\"   # you said this is the label\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "\n",
    "# ------------------------ LOW-RAM PROFILE ------------------------\n",
    "TABM_D_BLOCK   = 256\n",
    "TABM_K         = 8\n",
    "TABM_N_BLOCKS  = 1\n",
    "TABM_DROPOUT   = 0.10\n",
    "\n",
    "BATCH_SIZE       = 256\n",
    "GRAD_ACC_STEPS   = 2\n",
    "MAX_EPOCHS       = 60\n",
    "BASE_LR          = 2e-3\n",
    "WEIGHT_DECAY     = 3e-4\n",
    "PATIENCE         = 10\n",
    "MIN_DELTA        = 1e-4\n",
    "WARMUP_EPOCHS    = 3\n",
    "\n",
    "MISSING_TOKEN    = \"Missing\"\n",
    "MAX_CAT_CARD     = 500\n",
    "USE_NUM_EMBEDDINGS = False\n",
    "STORE_NUM_AS_FP16  = True\n",
    "STORE_CAT_AS_INT32 = True\n",
    "NUM_WORKERS      = 0\n",
    "\n",
    "# ------------------------ helpers ------------------------\n",
    "def canon_col(name: str) -> str:\n",
    "    s = re.sub(r\"[^0-9A-Za-z_]+\", \"_\", str(name))\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def canon_cols_inplace(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [canon_col(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique_in_order(seq):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        x = str(x)\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            out.append(x)\n",
    "    return out\n",
    "\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"int64\",\"int32\",\"float32\"]).columns:\n",
    "        if pdt.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].astype(object)\n",
    "            if s.astype(str).str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try:\n",
    "                    df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pdt.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]   = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"]  = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]    = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]    = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]   = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"] = s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]   = s.dt.is_month_end.astype(\"Int8\")\n",
    "    if dt_cols:\n",
    "        df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def pd_cat_fix(series, allowed):\n",
    "    # ensure unique, ordered categories and include MISSING_TOKEN once\n",
    "    allowed = _unique_in_order(list(allowed) + [MISSING_TOKEN])\n",
    "    s = series.astype(\"string\").fillna(MISSING_TOKEN)\n",
    "    s = s.where(s.isin(allowed), MISSING_TOKEN)\n",
    "    return pd.Categorical(s, categories=pd.Index(allowed), ordered=False)\n",
    "\n",
    "# ------------------------ Preprocessor (canon names + cap cats) ------------------------\n",
    "class TabularPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.num_cols_ = []\n",
    "        self.cat_cols_ = []\n",
    "        self.num_median_ = {}\n",
    "        self.cat_categories_ = {}\n",
    "        self.feature_names_ = []\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def _prep_base(self, X):\n",
    "        d = X.copy()\n",
    "        canon_cols_inplace(d)  # <<< canonicalize EVERY time (train/val/test/inference)\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == object:\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        for c in d.columns:\n",
    "            if pdt.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X):\n",
    "        d = self._prep_base(X)\n",
    "        self.num_cols_ = [c for c in d.columns if pdt.is_numeric_dtype(d[c])]\n",
    "        self.cat_cols_ = [c for c in d.columns if not pdt.is_numeric_dtype(d[c])]\n",
    "\n",
    "        for c in self.num_cols_:\n",
    "            self.num_median_[c] = float(pd.to_numeric(d[c], errors=\"coerce\").median())\n",
    "\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(\"string\").fillna(MISSING_TOKEN)\n",
    "            vc = s.value_counts(dropna=False)\n",
    "            if MAX_CAT_CARD and len(vc) > (MAX_CAT_CARD - 1):\n",
    "                top = vc.index.astype(\"string\").tolist()[:MAX_CAT_CARD - 1]\n",
    "                cats = _unique_in_order(top + [MISSING_TOKEN])\n",
    "            else:\n",
    "                cats = _unique_in_order(pd.unique(s).astype(\"string\").tolist() + [MISSING_TOKEN])\n",
    "            # final safety: ensure uniqueness\n",
    "            if len(cats) != len(set(cats)):\n",
    "                cats = _unique_in_order(cats)\n",
    "            self.cat_categories_[c] = cats\n",
    "\n",
    "        self.feature_names_ = self.num_cols_ + self.cat_cols_\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert self.fitted_\n",
    "        d = self._prep_base(X)\n",
    "        for c in self.feature_names_:\n",
    "            if c not in d.columns:\n",
    "                d[c] = np.nan\n",
    "        d = d[self.feature_names_].copy()\n",
    "\n",
    "        for c in self.num_cols_:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").astype(\"float32\")\n",
    "            if d[c].isna().any():\n",
    "                d[c] = d[c].fillna(self.num_median_[c])\n",
    "        for c in self.cat_cols_:\n",
    "            d[c] = pd_cat_fix(d[c], self.cat_categories_[c])\n",
    "        return d\n",
    "\n",
    "# ------------------------ Torch encoder (compact arrays) ------------------------\n",
    "class TorchTabEncoder:\n",
    "    def __init__(self, num_cols, cat_cols, cat_categories):\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.cat_categories = {c: list(cats) for c, cats in cat_categories.items()}\n",
    "        self.num_mean_ = None\n",
    "        self.num_std_  = None\n",
    "        self.cat_cardinalities_ = {c: len(self.cat_categories[c]) for c in self.cat_cols}\n",
    "\n",
    "    def fit(self, df_proc):\n",
    "        if self.num_cols:\n",
    "            arr = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            self.num_mean_ = arr.mean(axis=0).astype(\"float32\")\n",
    "            std = arr.std(axis=0).astype(\"float32\")\n",
    "            self.num_std_  = np.where(std < 1e-6, 1.0, std).astype(\"float32\")\n",
    "        else:\n",
    "            self.num_mean_ = np.array([], dtype=\"float32\")\n",
    "            self.num_std_  = np.array([], dtype=\"float32\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_proc):\n",
    "        if self.num_cols:\n",
    "            Xn = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            Xn = (Xn - self.num_mean_) / self.num_std_\n",
    "            if STORE_NUM_AS_FP16:\n",
    "                Xn = Xn.astype(\"float16\")\n",
    "        else:\n",
    "            Xn = np.zeros((len(df_proc), 0), dtype=\"float16\" if STORE_NUM_AS_FP16 else \"float32\")\n",
    "\n",
    "        Xc_list = []\n",
    "        for c in self.cat_cols:\n",
    "            codes = df_proc[c].cat.codes.to_numpy(copy=False)\n",
    "            fix = self.cat_categories[c].index(MISSING_TOKEN)\n",
    "            codes = np.where(codes < 0, fix, codes)\n",
    "            codes = codes.astype(\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "            Xc_list.append(codes)\n",
    "        Xc = np.stack(Xc_list, axis=1) if Xc_list else np.zeros((len(df_proc), 0), dtype=\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "        return Xn, Xc\n",
    "\n",
    "    def save_meta(self, path_json):\n",
    "        meta = {\n",
    "            \"num_cols\": self.num_cols,\n",
    "            \"cat_cols\": self.cat_cols,\n",
    "            \"cat_categories\": self.cat_categories,\n",
    "            \"num_mean\": self.num_mean_.tolist(),\n",
    "            \"num_std\": self.num_std_.tolist(),\n",
    "            \"cat_cardinalities\": self.cat_cardinalities_,\n",
    "        }\n",
    "        with open(path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_meta(path_json):\n",
    "        with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        enc = TorchTabEncoder(meta[\"num_cols\"], meta[\"cat_cols\"], meta[\"cat_categories\"])\n",
    "        enc.num_mean_ = np.array(meta[\"num_mean\"], dtype=\"float32\")\n",
    "        enc.num_std_  = np.array(meta[\"num_std\"], dtype=\"float32\")\n",
    "        enc.cat_cardinalities_ = {k:int(v) for k,v in meta[\"cat_cardinalities\"].items()}\n",
    "        return enc\n",
    "\n",
    "# ------------------------ Dataset ------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, Xn, Xc, y=None):\n",
    "        self.Xn = Xn\n",
    "        self.Xc = Xc\n",
    "        self.y  = None if y is None else y.astype(\"int64\")\n",
    "    def __len__(self): return len(self.Xn)\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is None:\n",
    "            return self.Xn[i], self.Xc[i]\n",
    "        return self.Xn[i], self.Xc[i], self.y[i]\n",
    "\n",
    "# ------------------------ LOAD & TARGET ------------------------\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "canon_cols_inplace(df)                  # <<< canonicalize once at read time too\n",
    "if TARGET_NAME not in df.columns:\n",
    "    raise KeyError(f\"Expected label column '{TARGET_NAME}' after canonicalization. \"\n",
    "                   f\"Got columns like: {list(df.columns)[:20]}\")\n",
    "\n",
    "y1 = pd.to_numeric(df[TARGET_NAME], errors=\"coerce\").astype(\"Int64\")\n",
    "y1 = y1.where((y1>=1) & (y1<=10))\n",
    "mask = y1.notna()\n",
    "df = df.loc[mask].copy()\n",
    "y1 = y1.loc[mask].astype(\"int16\")\n",
    "y  = (y1 - 1).astype(\"int16\")\n",
    "X  = df.drop(columns=[TARGET_NAME])\n",
    "del df; gc.collect()\n",
    "\n",
    "# ------------------------ SPLIT ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(SPLIT_DIR/\"train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(SPLIT_DIR/\"val_indices.csv\",   index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(SPLIT_DIR/\"test_indices.csv\",  index=False)\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ PREPROCESS & ENCODE ------------------------\n",
    "pp = TabularPreprocessor().fit(X_train)\n",
    "Xtr_df = pp.transform(X_train); Xva_df = pp.transform(X_val); Xte_df = pp.transform(X_test)\n",
    "\n",
    "enc = TorchTabEncoder(pp.num_cols_, pp.cat_cols_, pp.cat_categories_).fit(Xtr_df)\n",
    "Xtr_num, Xtr_cat = enc.transform(Xtr_df)\n",
    "Xva_num, Xva_cat = enc.transform(Xva_df)\n",
    "Xte_num, Xte_cat = enc.transform(Xte_df)\n",
    "\n",
    "# free raw frames early\n",
    "del X_train, X_val, X_test, X_trainval, Xtr_df, Xva_df, Xte_df, X, y, y1; gc.collect()\n",
    "\n",
    "y_tr = y_train.values.astype(\"int64\")\n",
    "y_va = y_val.values.astype(\"int64\")\n",
    "y_te = y_test.values.astype(\"int64\")\n",
    "\n",
    "ds_tr = TabDataset(Xtr_num, Xtr_cat, y_tr)\n",
    "ds_va = TabDataset(Xva_num, Xva_cat, y_va)\n",
    "ds_te = TabDataset(Xte_num, Xte_cat, y_te)\n",
    "\n",
    "dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "# ------------------------ CLASS WEIGHTS ------------------------\n",
    "classes_present = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_tr)\n",
    "cw_map = {int(c): float(w) for c, w in zip(classes_present, cw)}\n",
    "class_weights = np.ones(NUM_CLASSES, dtype=\"float32\")\n",
    "for c, w in cw_map.items(): class_weights[c] = w\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# ------------------------ MODEL ------------------------\n",
    "device = torch.device(\"cpu\")\n",
    "n_num = Xtr_num.shape[1]\n",
    "cat_cards = [enc.cat_cardinalities_[c] for c in enc.cat_cols] if enc.cat_cols else None\n",
    "num_emb = LinearReLUEmbeddings(n_num) if (USE_NUM_EMBEDDINGS and n_num > 0) else None\n",
    "\n",
    "model = TabM.make(\n",
    "    n_num_features=n_num,\n",
    "    cat_cardinalities=cat_cards,\n",
    "    num_embeddings=num_emb,\n",
    "    d_out=NUM_CLASSES,\n",
    "    k=TABM_K,\n",
    "    n_blocks=TABM_N_BLOCKS,\n",
    "    d_block=TABM_D_BLOCK,\n",
    "    dropout=TABM_DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "def cosine_factor(epoch, max_epochs=MAX_EPOCHS, warmup=WARMUP_EPOCHS):\n",
    "    if epoch < warmup: return (epoch + 1) / max(1, warmup)\n",
    "    t = (epoch - warmup) / max(1, max_epochs - warmup)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda e: cosine_factor(e))\n",
    "\n",
    "best_val = float(\"inf\"); best_epoch = -1; pat = 0\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"lr\": []}\n",
    "t0 = time.time()\n",
    "\n",
    "# ------------------------ TRAIN ------------------------\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, (xnum_np, xcat_np, yb) in enumerate(dl_tr, 1):\n",
    "        xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "        xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "        yb   = yb.to(device)\n",
    "\n",
    "        y_pred = model(xnum, xcat) if (xcat.shape[1] > 0 or n_num > 0) else model(xnum)\n",
    "        B, K, C = y_pred.shape\n",
    "\n",
    "        loss = 0.0\n",
    "        for k in range(K):\n",
    "            loss = loss + F.cross_entropy(y_pred[:, k, :], yb, weight=class_weights_t.to(device))\n",
    "        loss = loss / K\n",
    "\n",
    "        (loss / GRAD_ACC_STEPS).backward()\n",
    "        if step % GRAD_ACC_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total += loss.item() * B\n",
    "        n += B\n",
    "        del xnum, xcat\n",
    "\n",
    "    train_loss = total / max(1, n)\n",
    "\n",
    "    model.eval()\n",
    "    vtotal, vn = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xnum_np, xcat_np, yb in dl_va:\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            yb   = yb.to(device)\n",
    "            y_pred = model(xnum, xcat) if (xcat.shape[1] > 0 or n_num > 0) else model(xnum)\n",
    "            B, K, C = y_pred.shape\n",
    "            vloss = 0.0\n",
    "            for k in range(K):\n",
    "                vloss = vloss + F.cross_entropy(y_pred[:, k, :], yb, weight=class_weights_t.to(device))\n",
    "            vloss = vloss / K\n",
    "            vtotal += vloss.item() * B\n",
    "            vn += B\n",
    "            del xnum, xcat\n",
    "    val_loss = vtotal / max(1, vn)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_loss + MIN_DELTA < best_val:\n",
    "        best_val = val_loss; best_epoch = epoch; pat = 0\n",
    "        torch.save({\"state_dict\": model.state_dict(),\n",
    "                    \"tabm_config\": {\n",
    "                        \"n_num\": n_num, \"cat_cardinalities\": cat_cards,\n",
    "                        \"d_out\": NUM_CLASSES, \"k\": TABM_K,\n",
    "                        \"n_blocks\": TABM_N_BLOCKS, \"d_block\": TABM_D_BLOCK, \"dropout\": TABM_DROPOUT,\n",
    "                        \"use_num_embeddings\": bool(USE_NUM_EMBEDDINGS and n_num > 0),\n",
    "                        \"num_embedding_type\": \"LinearReLUEmbeddings\" if (USE_NUM_EMBEDDINGS and n_num > 0) else None\n",
    "                    }},\n",
    "                   OUTPUT_DIR / \"tabm_model.pt\")\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (best @ {best_epoch} | val {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Training time: {elapsed/60:.1f} min; best epoch: {best_epoch}\")\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "def predict_proba_dl(model, dl):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            if len(batch) == 3:\n",
    "                xnum_np, xcat_np, _ = batch\n",
    "            else:\n",
    "                xnum_np, xcat_np = batch\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            y_pred = model(xnum, xcat) if (xcat.shape[1] > 0 or n_num > 0) else model(xnum)\n",
    "            p = F.softmax(y_pred, dim=-1).mean(dim=1).cpu().numpy()\n",
    "            probs.append(p)\n",
    "            del xnum, xcat\n",
    "    return np.vstack(probs)\n",
    "\n",
    "ckpt = torch.load(OUTPUT_DIR / \"tabm_model.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "dl_tr_eval = DataLoader(ds_tr, batch_size=1024, shuffle=False, num_workers=0)\n",
    "dl_va_eval = DataLoader(ds_va, batch_size=1024, shuffle=False, num_workers=0)\n",
    "dl_te_eval = DataLoader(ds_te, batch_size=1024, shuffle=False, num_workers=0)\n",
    "\n",
    "def eval_split(name, dl, y_zero):\n",
    "    proba = predict_proba_dl(model, dl)\n",
    "    pred0 = np.argmax(proba, axis=1)\n",
    "    metrics = {\n",
    "        \"split\": name,\n",
    "        \"n_samples\": int(len(y_zero)),\n",
    "        \"accuracy\": float(accuracy_score(y_zero, pred0)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_zero, pred0, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(y_zero, proba, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = float(\"nan\")\n",
    "    try:\n",
    "        y_bin = pd.get_dummies(pd.Categorical(y_zero, categories=list(range(NUM_CLASSES))))\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(roc_auc_score(y_bin.values, proba, average=\"macro\", multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(\"nan\")\n",
    "\n",
    "    ys_one  = y_zero + 1\n",
    "    pred_one = pred0 + 1\n",
    "    report = classification_report(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0)\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics, pred_one, proba\n",
    "\n",
    "m_train, _, _ = eval_split(\"train\", dl_tr_eval, y_tr)\n",
    "m_val,   _, _ = eval_split(\"val\",   dl_va_eval, y_va)\n",
    "m_test,  _, _ = eval_split(\"test\",  dl_te_eval, y_te)\n",
    "\n",
    "pd.DataFrame([m_train, m_val, m_test]).to_csv(OUTPUT_DIR / \"metrics_tabm_lowram_fixed.csv\", index=False)\n",
    "\n",
    "# ------------------------ Curves ------------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_loss\"],   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"CE loss\"); plt.title(\"TabM (Low-RAM, Fixed) Loss\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"loss_curves.png\", dpi=150); plt.close()\n",
    "\n",
    "# ------------------------ Save artifacts ------------------------\n",
    "joblib.dump(pp, OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "enc.save_meta(OUTPUT_DIR / \"encoder_meta.json\")\n",
    "with open(OUTPUT_DIR / \"training_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"target_name\": TARGET_NAME,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"label_order_zero_indexed\": list(range(NUM_CLASSES)),\n",
    "        \"seed\": SEED,\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"splits\": {\"train\": \"splits/train_indices.csv\", \"val\": \"splits/val_indices.csv\", \"test\": \"splits/test_indices.csv\"},\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"tabm_cfg\": ckpt[\"tabm_config\"],\n",
    "        \"train_time_min\": round(elapsed/60, 2),\n",
    "        \"feature_names\": pp.feature_names_,\n",
    "        \"cat_features\": pp.cat_cols_,\n",
    "        \"num_features\": pp.num_cols_,\n",
    "        \"column_names_canonicalized\": True\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ All artifacts saved to Google Drive at: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ Inference helper ------------------------\n",
    "def predict_target_risk_class(df_new: pd.DataFrame,\n",
    "                              model_path=OUTPUT_DIR / \"tabm_model.pt\",\n",
    "                              preproc_path=OUTPUT_DIR / \"preprocessor.pkl\",\n",
    "                              encoder_meta_path=OUTPUT_DIR / \"encoder_meta.json\",\n",
    "                              batch_size=4096) -> pd.Series:\n",
    "    device = torch.device(\"cpu\")\n",
    "    pp = joblib.load(preproc_path)\n",
    "    enc = TorchTabEncoder.load_meta(encoder_meta_path)\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    cfg = ckpt[\"tabm_config\"]\n",
    "\n",
    "    # Ensure incoming columns match training convention\n",
    "    df_new = df_new.copy()\n",
    "    canon_cols_inplace(df_new)\n",
    "\n",
    "    n_num = len(enc.num_cols)\n",
    "    cat_cards = [enc.cat_cardinalities_[c] for c in enc.cat_cols] if enc.cat_cols else None\n",
    "    num_emb = LinearReLUEmbeddings(n_num) if (cfg.get(\"use_num_embeddings\") and n_num > 0) else None\n",
    "    model = TabM.make(\n",
    "        n_num_features=n_num, cat_cardinalities=cat_cards, num_embeddings=num_emb,\n",
    "        d_out=cfg[\"d_out\"], k=cfg[\"k\"], n_blocks=cfg[\"n_blocks\"], d_block=cfg[\"d_block\"], dropout=cfg[\"dropout\"]\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n",
    "\n",
    "    dproc = pp.transform(df_new)\n",
    "    Xn, Xc = enc.transform(dproc)\n",
    "    ds = TabDataset(Xn, Xc, y=None)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xnum_np, xcat_np in dl:\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            y_pred = model(xnum, xcat) if (xcat.shape[1] > 0 or n_num > 0) else model(xnum)\n",
    "            p = F.softmax(y_pred, dim=-1).mean(dim=1)\n",
    "            preds.append(torch.argmax(p, dim=-1).cpu().numpy())\n",
    "    yhat0 = np.concatenate(preds, axis=0).astype(\"int16\")\n",
    "    return pd.Series(yhat0 + 1, index=dproc.index, name=\"pred_target_risk_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0hXUxmtEIHc"
   },
   "source": [
    "##### GPU ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17683,
     "status": "ok",
     "timestamp": 1761810655996,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "5AqqCA_fF5k9",
    "outputId": "bf72f833-942e-4441-f8fa-8df5e080ef3b"
   },
   "outputs": [],
   "source": [
    "%pip install -q -U pip setuptools wheel\n",
    "%pip install -q --no-cache-dir tabm rtdl-num-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3331,
     "status": "ok",
     "timestamp": 1761810659923,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "VbWnvwlLGMej",
    "outputId": "4c6a73f6-e866-46c6-dec0-0f6d154889ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabm: 0.0.3\n",
      "rtdl-num-embeddings: 0.0.12\n"
     ]
    }
   ],
   "source": [
    "import tabm, rtdl_num_embeddings\n",
    "print(\"tabm:\", tabm.__version__)\n",
    "print(\"rtdl-num-embeddings:\", rtdl_num_embeddings.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "executionInfo": {
     "elapsed": 279692,
     "status": "error",
     "timestamp": 1761811575047,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "ofV5mosREKOp",
    "outputId": "1c5ca2cb-6290-423d-f759-56119869b810"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# TabM (PyTorch • CUDA) Multiclass Training Pipeline — Colab GPU\n",
    "# - Colab-ready; saves under RESULTS/TabM_GPU/<timestamp>\n",
    "# - Robust preprocessing: datetime expansion, float32 downcast, stable ordinal encoding (unseen->'Missing')\n",
    "# - Stratified 70/15/15 split + saved indices\n",
    "# - Model: TabM (parameter-efficient ensemble MLP) with numeric embeddings\n",
    "# - GPU: CUDA mixed precision (fp16) + GradScaler; gradient accumulation\n",
    "# - Metrics: accuracy, macro/weighted P/R/F1, log loss + reports + confusions\n",
    "# - Artifacts: tabm_model.pt, preprocessor.pkl, config.json, metrics.csv\n",
    "# - Inference helper returns labels in 1..10 (averaging ensemble probabilities)\n",
    "# ===============================================================\n",
    "\n",
    "# ------------------------ IMPORTS ------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, log_loss, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "import os, subprocess, sys, json, warnings, re, shutil\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# TabM\n",
    "from tabm import TabM\n",
    "from rtdl_num_embeddings import PiecewiseLinearEmbeddings, LinearReLUEmbeddings\n",
    "\n",
    "# ------------------------ DRIVE SETUP ------------------------\n",
    "drive_folder = 'path'\n",
    "try:\n",
    "    from google.colab import drive as _colab_drive\n",
    "    if not Path(\"/content/drive\").exists() or not any(Path(\"/content/drive\").iterdir()):\n",
    "        _colab_drive.mount(\"/content/drive\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "RUN_NAME = f\"TabM_GPU_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUTPUT_DIR = Path(drive_folder) / \"TabM_GPU\" / RUN_NAME\n",
    "(OUTPUT_DIR / \"splits\").mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving all outputs to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# >>>>>>>>>>>> SET THIS TO YOUR FILE <<<<<<<<<<<<\n",
    "DATA_CSV = \"path\"\n",
    "\n",
    "TARGET_ASCII   = \"target_risk_class\"\n",
    "TARGET_PERSIAN = \"risk score\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "\n",
    "# Training controls (tuned for ~15GB VRAM, e.g., T4)\n",
    "MAX_EPOCHS      = 80\n",
    "PATIENCE        = 8\n",
    "BATCH_SIZE      = 1024          # reduce if you still see OOM; try 512\n",
    "ACCUM_STEPS     = 2             # effective batch = BATCH_SIZE * ACCUM_STEPS\n",
    "LEARNING_RATE   = 2e-3\n",
    "WEIGHT_DECAY    = 1e-4\n",
    "NUM_WORKERS     = 2\n",
    "MIXED_PRECISION = True          # CUDA autocast(fp16) + GradScaler\n",
    "K_ENSEMBLE      = 16            # TabM's k (8..16 fits better on T4)\n",
    "EMBEDDING_TYPE  = \"piecewise\"   # numeric embedding type\n",
    "\n",
    "MISSING_TOKEN = \"Missing\"\n",
    "\n",
    "# ------------------------ GPU / DEVICE ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ------------------------ UTILITIES ------------------------\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"int64\",\"int32\",\"float32\"]).columns:\n",
    "        if pdt.is_float_dtype(df[c]):\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:\n",
    "            df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].astype(object)\n",
    "            if s.astype(str).str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try:\n",
    "                    df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pdt.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]   = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"]  = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]    = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]    = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]   = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"] = s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]   = s.dt.is_month_end.astype(\"Int8\")\n",
    "    if dt_cols:\n",
    "        df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def resolve_target_name(df):\n",
    "    for k in [TARGET_ASCII, TARGET_PERSIAN, \"risk_class\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "        if k in df.columns: return k\n",
    "    norm = {re.sub(r\"[_\\-\\s]+\",\" \", str(c)).strip().lower(): c for c in df.columns}\n",
    "    for k in [\"target_risk_class\", \"risk score\", \"risk class\", \"label\", \"target\", \"class\", \"y\"]:\n",
    "        kk = re.sub(r\"[_\\-\\s]+\",\" \", k).strip().lower()\n",
    "        if kk in norm: return norm[kk]\n",
    "    return None\n",
    "\n",
    "# ------------------------ PREPROCESSOR ------------------------\n",
    "class TabularPreprocessor:\n",
    "    \"\"\"\n",
    "    - Expands datetimes, normalizes whitespace\n",
    "    - numerics as float32, median impute\n",
    "    - categoricals -> stable ordinal codes per column; unseen -> 'Missing'\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.num_cols_, self.cat_cols_ = [], []\n",
    "        self.num_median_, self.cat_maps_, self.cat_cardinalities_ = {}, {}, {}\n",
    "        self.feature_names_, self.fitted_ = [], False\n",
    "\n",
    "    def _prep_base(self, df):\n",
    "        d = df.copy()\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == object:\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        for c in d.columns:\n",
    "            if pdt.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X):\n",
    "        d = self._prep_base(X)\n",
    "        self.num_cols_ = [c for c in d.columns if pdt.is_numeric_dtype(d[c])]\n",
    "        self.cat_cols_ = [c for c in d.columns if not pdt.is_numeric_dtype(d[c])]\n",
    "        # stats\n",
    "        for c in self.num_cols_:\n",
    "            self.num_median_[c] = float(pd.to_numeric(d[c], errors=\"coerce\").median())\n",
    "        # build categorical maps (ensure MISSING_TOKEN present)\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(\"string\").fillna(MISSING_TOKEN)\n",
    "            cats = pd.Index(pd.unique(pd.concat([pd.Series([MISSING_TOKEN]), s])))\n",
    "            cmap = {v: i for i, v in enumerate(cats)}\n",
    "            self.cat_maps_[c] = cmap\n",
    "            self.cat_cardinalities_[c] = len(cmap)\n",
    "        self.feature_names_ = self.num_cols_ + self.cat_cols_\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert self.fitted_\n",
    "        d = self._prep_base(X)\n",
    "        for c in self.feature_names_:\n",
    "            if c not in d.columns:\n",
    "                d[c] = np.nan\n",
    "        d = d[self.feature_names_].copy()\n",
    "        # numerics\n",
    "        for c in self.num_cols_:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").astype(\"float32\")\n",
    "            if d[c].isna().any():\n",
    "                d[c] = d[c].fillna(self.num_median_[c])\n",
    "        # categoricals\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(\"string\").fillna(MISSING_TOKEN)\n",
    "            cmap = self.cat_maps_[c]\n",
    "            d[c] = s.map(cmap).fillna(cmap[MISSING_TOKEN]).astype(\"int32\")\n",
    "        return d\n",
    "\n",
    "# ------------------------ LOAD & TARGET ------------------------\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "tgt = resolve_target_name(df)\n",
    "if tgt is None:\n",
    "    raise KeyError(\"Could not find target column. Expected 'target_risk_class' or 'risk score'.\")\n",
    "\n",
    "y_1_10 = pd.to_numeric(df[tgt], errors=\"coerce\").astype(\"Int64\")\n",
    "y_1_10 = y_1_10.where((y_1_10>=1) & (y_1_10<=10))\n",
    "mask = y_1_10.notna()\n",
    "df = df.loc[mask].copy()\n",
    "y_1_10 = y_1_10.loc[mask].astype(\"int16\")\n",
    "y = (y_1_10 - 1).astype(\"int64\")  # 0..9\n",
    "X = df.drop(columns=[tgt])\n",
    "\n",
    "# ------------------------ SPLIT (70/15/15) ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "y_trainval, y_test  = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(OUTPUT_DIR/\"splits/train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(OUTPUT_DIR/\"splits/val_indices.csv\",   index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(OUTPUT_DIR/\"splits/test_indices.csv\",  index=False)\n",
    "\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ PREPROCESS ------------------------\n",
    "pp = TabularPreprocessor().fit(X_train)\n",
    "Xtr = pp.transform(X_train)\n",
    "Xva = pp.transform(X_val)\n",
    "Xte = pp.transform(X_test)\n",
    "\n",
    "num_cols = pp.num_cols_\n",
    "cat_cols = pp.cat_cols_\n",
    "cat_cardinalities = [pp.cat_cardinalities_[c] for c in cat_cols]\n",
    "print(f\"Numerics: {len(num_cols)} | Categoricals: {len(cat_cols)}\")\n",
    "\n",
    "# ------------------------ DATASETS ------------------------\n",
    "class TabDataset(data.Dataset):\n",
    "    def __init__(self, X_enc: pd.DataFrame, y_arr: np.ndarray | None, num_cols, cat_cols):\n",
    "        self.X = X_enc\n",
    "        self.y = None if y_arr is None else np.asarray(y_arr, dtype=np.int64)\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.X.iloc[idx]\n",
    "        x_num = torch.tensor(row[self.num_cols].values, dtype=torch.float32) if self.num_cols else torch.empty( (0,), dtype=torch.float32 )\n",
    "        x_cat = torch.tensor(row[self.cat_cols].values, dtype=torch.long)     if self.cat_cols else torch.empty( (0,), dtype=torch.long )\n",
    "        if self.y is None:\n",
    "            return x_num, x_cat\n",
    "        return x_num, x_cat, torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "ds_tr = TabDataset(Xtr, y_train.values, num_cols, cat_cols)\n",
    "ds_va = TabDataset(Xva, y_val.values,   num_cols, cat_cols)\n",
    "ds_te = TabDataset(Xte, y_test.values,  num_cols, cat_cols)\n",
    "\n",
    "dl_kwargs = dict(\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    "    persistent_workers=(NUM_WORKERS > 0),\n",
    "    drop_last=False\n",
    ")\n",
    "dl_tr = data.DataLoader(ds_tr, shuffle=True,  **dl_kwargs)\n",
    "dl_va = data.DataLoader(ds_va, shuffle=False, **dl_kwargs)\n",
    "dl_te = data.DataLoader(ds_te, shuffle=False, **dl_kwargs)\n",
    "\n",
    "# ------------------------ MODEL ------------------------\n",
    "d_out = NUM_CLASSES\n",
    "\n",
    "# Numeric embeddings (VRAM-friendly defaults)\n",
    "num_embeddings = None\n",
    "if len(num_cols) > 0:\n",
    "    d_emb = 16\n",
    "    num_emb_inf = None\n",
    "    if len(num_cols) > 0:\n",
    "        try:\n",
    "            num_emb_inf = PiecewiseLinearEmbeddings(len(num_cols), d_emb, version=\"B\")\n",
    "        except TypeError:\n",
    "            try:\n",
    "                num_emb_inf = PiecewiseLinearEmbeddings(len(num_cols), d_emb)\n",
    "            except Exception:\n",
    "                num_emb_inf = LinearReLUEmbeddings(len(num_cols), d_emb)\n",
    "\n",
    "model = TabM.make(\n",
    "    n_num_features=len(num_cols),\n",
    "    num_embeddings=num_embeddings,\n",
    "    cat_cardinalities=cat_cardinalities if len(cat_cols)>0 else None,\n",
    "    d_out=NUM_CLASSES,\n",
    "    k=K_ENSEMBLE,                 # ensemble size (reduce if OOM)\n",
    "    arch_type=\"batch-ensemble\",\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer & schedule\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=MAX_EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(MIXED_PRECISION and device.type==\"cuda\"))\n",
    "\n",
    "# ------------------------ TRAINING UTILS ------------------------\n",
    "def _forward_logits(x_num, x_cat):\n",
    "    if x_num is not None and x_cat is not None and x_cat.numel()>0:\n",
    "        return model(x_num, x_cat)        # (B, k, C)\n",
    "    elif x_num is not None:\n",
    "        return model(x_num)               # (B, k, C)\n",
    "    else:\n",
    "        return model(None, x_cat)         # (B, k, C)\n",
    "\n",
    "def _step_batch(batch, train: bool):\n",
    "    if len(num_cols) > 0 and len(cat_cols) > 0:\n",
    "        x_num, x_cat, yb = batch\n",
    "    elif len(num_cols) > 0:\n",
    "        x_num, yb = batch[0], batch[-1]\n",
    "        x_cat = None\n",
    "    elif len(cat_cols) > 0:\n",
    "        x_cat, yb = batch[0], batch[-1]\n",
    "        x_num = None\n",
    "    else:\n",
    "        raise ValueError(\"No features!\")\n",
    "\n",
    "    if x_num is not None: x_num = x_num.to(device, non_blocking=True)\n",
    "    if x_cat is not None and x_cat.numel()>0: x_cat = x_cat.to(device, non_blocking=True)\n",
    "    yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "    autocast_ctx = torch.cuda.amp.autocast(enabled=(MIXED_PRECISION and device.type==\"cuda\"), dtype=torch.float16)\n",
    "    with autocast_ctx:\n",
    "        logits_k = _forward_logits(x_num, x_cat)   # (B, k, C)\n",
    "        B, K, C = logits_k.shape\n",
    "        loss = F.cross_entropy(\n",
    "            logits_k.reshape(B*K, C),\n",
    "            yb.unsqueeze(1).repeat(1, K).reshape(B*K),\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "    if train:\n",
    "        if scaler.is_enabled():\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "    return loss.item(), logits_k.detach(), yb.detach()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    all_probs, all_y = [], []\n",
    "    total_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        # forward (no grad)\n",
    "        if len(num_cols) > 0 and len(cat_cols) > 0:\n",
    "            x_num, x_cat, yb = batch\n",
    "        elif len(num_cols) > 0:\n",
    "            x_num, yb = batch[0], batch[-1]\n",
    "            x_cat = None\n",
    "        else:\n",
    "            x_cat, yb = batch[0], batch[-1]\n",
    "            x_num = None\n",
    "\n",
    "        if x_num is not None: x_num = x_num.to(device, non_blocking=True)\n",
    "        if x_cat is not None and x_cat.numel()>0: x_cat = x_cat.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=(MIXED_PRECISION and device.type==\"cuda\"), dtype=torch.float16):\n",
    "            logits_k = _forward_logits(x_num, x_cat)\n",
    "            B,K,C = logits_k.shape\n",
    "            l = F.cross_entropy(logits_k.reshape(B*K, C),\n",
    "                                yb.unsqueeze(1).repeat(1, K).reshape(B*K),\n",
    "                                reduction=\"mean\")\n",
    "\n",
    "        total_loss += l.item() * yb.size(0)\n",
    "        probs_k = F.softmax(logits_k.float(), dim=-1)   # (B, k, C)\n",
    "        probs   = probs_k.mean(dim=1)                   # (B, C)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_y.append(yb.cpu().numpy())\n",
    "\n",
    "    probs = np.concatenate(all_probs, axis=0)\n",
    "    ys    = np.concatenate(all_y,    axis=0)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    metrics = {\n",
    "        \"n_samples\": int(len(ys)),\n",
    "        \"accuracy\": float(accuracy_score(ys, preds)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(ys, preds, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(ys, probs, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = float(\"nan\")\n",
    "    return metrics, probs, preds, ys, total_loss / max(1, len(ys))\n",
    "\n",
    "# ------------------------ TRAIN LOOP ------------------------\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_seen = 0\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    for it, batch in enumerate(dl_tr, start=1):\n",
    "        l, _, yb = _step_batch(batch, train=True)\n",
    "        bs = yb.size(0)\n",
    "        n_seen += bs\n",
    "        epoch_loss += l * bs\n",
    "\n",
    "        # gradient accumulation\n",
    "        if scaler.is_enabled():\n",
    "            scaler.step(opt) if (it % ACCUM_STEPS == 0) else None\n",
    "            scaler.update()  if (it % ACCUM_STEPS == 0) else None\n",
    "            if (it % ACCUM_STEPS == 0):\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            if (it % ACCUM_STEPS == 0):\n",
    "                opt.step()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Handle last partial accumulation step\n",
    "    if (it % ACCUM_STEPS) != 0:\n",
    "        if scaler.is_enabled():\n",
    "            scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
    "        else:\n",
    "            opt.step(); opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    train_loss = epoch_loss / max(1, n_seen)\n",
    "    val_metrics, _, _, _, val_loss = evaluate(dl_va)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "    # Early stopping on val_loss\n",
    "    if val_loss < best_val - 1e-4:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ------------------------ EVALUATION & SAVE ------------------------\n",
    "def eval_split(name, dl, X_raw, y0):\n",
    "    metrics, probs, preds, ys, _ = evaluate(dl)\n",
    "    ys_one   = ys + 1\n",
    "    pred_one = preds + 1\n",
    "    report = classification_report(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0)\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics\n",
    "\n",
    "metrics_train = eval_split(\"train\", dl_tr, X_train, y_train)\n",
    "metrics_val   = eval_split(\"val\",   dl_va, X_val,   y_val)\n",
    "metrics_test  = eval_split(\"test\",  dl_te, X_test,  y_test)\n",
    "\n",
    "metrics_df = pd.DataFrame([ {**{\"split\":\"train\"}, **metrics_train},\n",
    "                             {**{\"split\":\"val\"},   **metrics_val},\n",
    "                             {**{\"split\":\"test\"},  **metrics_test} ])\n",
    "metrics_df.to_csv(OUTPUT_DIR / \"metrics.csv\", index=False)\n",
    "print(metrics_df)\n",
    "\n",
    "# Save artifacts\n",
    "torch.save(model.state_dict(), OUTPUT_DIR / \"tabm_model.pt\")\n",
    "joblib.dump(pp, OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "with open(OUTPUT_DIR / \"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"timestamp\": RUN_NAME,\n",
    "        \"seed\": SEED,\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"k_ensemble\": K_ENSEMBLE,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"accum_steps\": ACCUM_STEPS,\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"patience\": PATIENCE,\n",
    "        \"mixed_precision\": MIXED_PRECISION,\n",
    "        \"numeric_cols\": num_cols,\n",
    "        \"categorical_cols\": cat_cols,\n",
    "        \"cat_cardinalities\": cat_cardinalities\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ All artifacts saved to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ INFERENCE HELPER ------------------------\n",
    "def predict_target_risk_class_tabm(df_new: pd.DataFrame,\n",
    "                                   model_path=OUTPUT_DIR / \"tabm_model.pt\",\n",
    "                                   preproc_path=OUTPUT_DIR / \"preprocessor.pkl\",\n",
    "                                   batch_size=BATCH_SIZE):\n",
    "    \"\"\"Predict on raw frame BEFORE preprocessing; returns labels 1..10.\n",
    "       Averages ensemble probabilities across k (recommended).\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    mdl = TabM.make(\n",
    "        n_num_features=len(num_cols),\n",
    "        num_embeddings=PiecewiseLinearEmbeddings(n_features=len(num_cols)) if len(num_cols)>0 else None,\n",
    "        cat_cardinalities=cat_cardinalities if len(cat_cols)>0 else None,\n",
    "        d_out=NUM_CLASSES,\n",
    "        k=K_ENSEMBLE,\n",
    "        arch_type=\"batch-ensemble\",\n",
    "    )\n",
    "    mdl.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "    mdl.to(device)\n",
    "    mdl.eval()\n",
    "    preproc = joblib.load(preproc_path)\n",
    "    d = preproc.transform(df_new)\n",
    "    ds = TabDataset(d, None, num_cols, cat_cols)\n",
    "    dl = data.DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=(device.type==\"cuda\"))\n",
    "    preds_all = []\n",
    "    with torch.no_grad():\n",
    "        for (xb_num, xb_cat) in dl:\n",
    "            x_num = xb_num.to(device, non_blocking=True) if len(num_cols)>0 else None\n",
    "            x_cat = xb_cat.to(device, non_blocking=True) if len(cat_cols)>0 else None\n",
    "            with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\"), dtype=torch.float16):\n",
    "                logits_k = mdl(x_num, x_cat) if (x_num is not None and x_cat is not None and x_cat.numel()>0) \\\n",
    "                          else (mdl(x_num) if x_num is not None else mdl(None, x_cat))\n",
    "                probs = F.softmax(logits_k.float(), dim=-1).mean(dim=1)  # average across k\n",
    "            preds_all.append(probs.argmax(dim=1).cpu())\n",
    "    pred0 = torch.cat(preds_all, dim=0).numpy()\n",
    "    pred1 = pred0 + 1\n",
    "    return pd.Series(pred1, index=df_new.index, name=\"pred_target_risk_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RpnnY1YYWzfH"
   },
   "source": [
    "#### Tab-Transformer (CPU/VSCODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WtEJajpVW4Nu"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# Tab-Transformer (CPU, PyTorch) – Multiclass Pipeline (FAST v2)\n",
    "# - Local paths; timestamped outputs under ./RESULTS/tabtransformer_outputs\n",
    "# - Canonicalized column names; label column = target_risk_class (values 1..10)\n",
    "# - Preprocess: datetime expansion, numeric downcast+median, capped & unique categoricals\n",
    "# - CPU-optimized: tiny encoder, mean pooling, capped batches/epoch, no dataloader workers\n",
    "# - Training: AdamW, cosine LR, early stopping (val CE)\n",
    "# - Metrics: accuracy, macro/weighted P/R/F1, log loss, ROC-AUC OvR macro\n",
    "# - Artifacts: tabtransformer_model.pt, preprocessor.pkl, encoder_meta.json, metrics.csv,\n",
    "#   loss_curves.png, reports + confusions, training_meta.json\n",
    "# - Inference helper returns labels in 1..10\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, warnings, datetime, math, time, random, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    log_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ PATHS ------------------------\n",
    "DATA_CSV = \"path\"\n",
    "OUTPUT_DIR = Path(\"path\"); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_DIR  = OUTPUT_DIR / \"splits\"; SPLIT_DIR.mkdir(exist_ok=True)\n",
    "RUN_NAME = f\"TabTransformer_CPU_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"Saving all outputs to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "# On CPU, fewer threads sometimes yields better throughput\n",
    "torch.set_num_threads(max(1, (os.cpu_count() or 4)//2))\n",
    "\n",
    "TARGET_NAME = \"target_risk_class\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "\n",
    "# ---- Low-RAM & training profile (FAST) ----\n",
    "BATCH_SIZE       = 4096        # larger batch since model is small\n",
    "GRAD_ACC_STEPS   = 1\n",
    "MAX_EPOCHS       = 10\n",
    "BASE_LR          = 2e-3\n",
    "WEIGHT_DECAY     = 3e-4\n",
    "PATIENCE         = 2\n",
    "MIN_DELTA        = 1e-4\n",
    "WARMUP_EPOCHS    = 2\n",
    "NUM_WORKERS      = 0           # Windows/CPU: avoid multiprocessing overhead\n",
    "MAX_BATCHES_PER_EPOCH = 100    # caps compute per epoch (~100*4096 ≈ 410k samples)\n",
    "\n",
    "# ---- Evaluation toggles ----\n",
    "EVAL_TRAIN = False             # set True if you really want train metrics\n",
    "\n",
    "# ---- Preprocessing ----\n",
    "MISSING_TOKEN      = \"Missing\"\n",
    "MAX_CAT_CARD       = 200       # cap cardinality to shrink embeddings\n",
    "STORE_NUM_AS_FP16  = True\n",
    "STORE_CAT_AS_INT32 = True\n",
    "\n",
    "# ---- Tab-Transformer hyperparams (tiny-but-strong) ----\n",
    "D_TOKEN       = 32             # must be divisible by N_HEADS\n",
    "N_HEADS       = 4\n",
    "N_LAYERS      = 1\n",
    "D_FF          = 128\n",
    "ATTN_DROPOUT  = 0.10\n",
    "FFN_DROPOUT   = 0.10\n",
    "RESID_DROPOUT = 0.10\n",
    "POOLING       = \"mean\"        # \"concat\" | \"mean\" (mean keeps 32 dims)\n",
    "\n",
    "# ---- Numeric path ----\n",
    "USE_NUMERIC  = True\n",
    "D_NUM_PROJ   = 64\n",
    "HEAD_HIDDEN  = 128\n",
    "HEAD_DEPTH   = 1\n",
    "HEAD_DROPOUT = 0.10\n",
    "ACTIVATION   = \"silu\"         # \"relu\" | \"gelu\" | \"silu\"\n",
    "\n",
    "# ---- sanity ----\n",
    "assert D_TOKEN % N_HEADS == 0, \"D_TOKEN must be divisible by N_HEADS\"\n",
    "\n",
    "# ------------------------ helpers ------------------------\n",
    "def canon_col(name: str) -> str:\n",
    "    s = re.sub(r\"[^0-9A-Za-z_]+\", \"_\", str(name))\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def canon_cols_inplace(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [canon_col(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique_in_order(seq):\n",
    "    seen = set(); out = []\n",
    "    for x in seq:\n",
    "        x = str(x)\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"int64\",\"int32\",\"float32\"]).columns:\n",
    "        if pdt.is_float_dtype(df[c]): df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:                         df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].astype(object)\n",
    "            if s.astype(str).str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try: df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception: pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pdt.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]   = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"]  = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]    = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]    = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]   = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"] = s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]   = s.dt.is_month_end.astype(\"Int8\")\n",
    "    if dt_cols: df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def pd_cat_fix(series, allowed):\n",
    "    allowed = _unique_in_order(list(allowed) + [MISSING_TOKEN])\n",
    "    s = series.astype(\"string\").fillna(MISSING_TOKEN)\n",
    "    s = s.where(s.isin(allowed), MISSING_TOKEN)\n",
    "    return pd.Categorical(s, categories=pd.Index(allowed), ordered=False)\n",
    "\n",
    "def _act(name):\n",
    "    return {\"relu\": nn.ReLU, \"gelu\": nn.GELU, \"silu\": nn.SiLU}[name]()\n",
    "\n",
    "# ------------------------ Preprocessor ------------------------\n",
    "class TabularPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.num_cols_ = []; self.cat_cols_ = []\n",
    "        self.num_median_ = {}; self.cat_categories_ = {}\n",
    "        self.feature_names_ = []; self.fitted_ = False\n",
    "\n",
    "    def _prep_base(self, X):\n",
    "        d = X.copy()\n",
    "        canon_cols_inplace(d)\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == object:\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        for c in d.columns:\n",
    "            if pdt.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X):\n",
    "        d = self._prep_base(X)\n",
    "        self.num_cols_ = [c for c in d.columns if pdt.is_numeric_dtype(d[c])]\n",
    "        self.cat_cols_ = [c for c in d.columns if not pdt.is_numeric_dtype(d[c])]\n",
    "        for c in self.num_cols_:\n",
    "            self.num_median_[c] = float(pd.to_numeric(d[c], errors=\"coerce\").median())\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(\"string\").fillna(MISSING_TOKEN)\n",
    "            vc = s.value_counts(dropna=False)\n",
    "            if MAX_CAT_CARD and len(vc) > (MAX_CAT_CARD - 1):\n",
    "                top = vc.index.astype(\"string\").tolist()[:MAX_CAT_CARD - 1]\n",
    "                cats = _unique_in_order(top + [MISSING_TOKEN])\n",
    "            else:\n",
    "                cats = _unique_in_order(pd.unique(s).astype(\"string\").tolist() + [MISSING_TOKEN])\n",
    "            if len(cats) != len(set(cats)):\n",
    "                cats = _unique_in_order(cats)\n",
    "            self.cat_categories_[c] = cats\n",
    "        self.feature_names_ = self.num_cols_ + self.cat_cols_\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert self.fitted_\n",
    "        d = self._prep_base(X)\n",
    "        for c in self.feature_names_:\n",
    "            if c not in d.columns:\n",
    "                d[c] = np.nan\n",
    "        d = d[self.feature_names_].copy()\n",
    "        for c in self.num_cols_:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").astype(\"float32\")\n",
    "            if d[c].isna().any():\n",
    "                d[c] = d[c].fillna(self.num_median_[c])\n",
    "        for c in self.cat_cols_:\n",
    "            d[c] = pd_cat_fix(d[c], self.cat_categories_[c])\n",
    "        return d\n",
    "\n",
    "# ------------------------ Torch encoder ------------------------\n",
    "class TorchTabEncoder:\n",
    "    def __init__(self, num_cols, cat_cols, cat_categories):\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.cat_categories = {c: list(cats) for c, cats in cat_categories.items()}\n",
    "        self.num_mean_ = None; self.num_std_  = None\n",
    "        self.cat_cardinalities_ = {c: len(self.cat_categories[c]) for c in self.cat_cols}\n",
    "\n",
    "    def fit(self, df_proc):\n",
    "        if self.num_cols:\n",
    "            arr = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            self.num_mean_ = arr.mean(axis=0).astype(\"float32\")\n",
    "            std = arr.std(axis=0).astype(\"float32\")\n",
    "            self.num_std_  = np.where(std < 1e-6, 1.0, std).astype(\"float32\")\n",
    "        else:\n",
    "            self.num_mean_ = np.array([], dtype=\"float32\")\n",
    "            self.num_std_  = np.array([], dtype=\"float32\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_proc):\n",
    "        if self.num_cols:\n",
    "            Xn = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            Xn = (Xn - self.num_mean_) / self.num_std_\n",
    "            if STORE_NUM_AS_FP16: Xn = Xn.astype(\"float16\")\n",
    "        else:\n",
    "            Xn = np.zeros((len(df_proc), 0), dtype=\"float16\" if STORE_NUM_AS_FP16 else \"float32\")\n",
    "\n",
    "        Xc_list = []\n",
    "        for c in self.cat_cols:\n",
    "            codes = df_proc[c].cat.codes.to_numpy(copy=False)\n",
    "            fix = self.cat_categories[c].index(MISSING_TOKEN)\n",
    "            codes = np.where(codes < 0, fix, codes)\n",
    "            codes = codes.astype(\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "            Xc_list.append(codes)\n",
    "        Xc = np.stack(Xc_list, axis=1) if Xc_list else np.zeros((len(df_proc),0), dtype=\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "        return Xn, Xc\n",
    "\n",
    "    def save_meta(self, path_json):\n",
    "        meta = {\n",
    "            \"num_cols\": self.num_cols,\n",
    "            \"cat_cols\": self.cat_cols,\n",
    "            \"cat_categories\": self.cat_categories,\n",
    "            \"num_mean\": self.num_mean_.tolist(),\n",
    "            \"num_std\": self.num_std_.tolist(),\n",
    "            \"cat_cardinalities\": self.cat_cardinalities_,\n",
    "        }\n",
    "        with open(path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_meta(path_json):\n",
    "        with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        enc = TorchTabEncoder(meta[\"num_cols\"], meta[\"cat_cols\"], meta[\"cat_categories\"])\n",
    "        enc.num_mean_ = np.array(meta[\"num_mean\"], dtype=\"float32\")\n",
    "        enc.num_std_  = np.array(meta[\"num_std\"], dtype=\"float32\")\n",
    "        enc.cat_cardinalities_ = {k:int(v) for k,v in meta[\"cat_cardinalities\"].items()}\n",
    "        return enc\n",
    "\n",
    "# ------------------------ Dataset ------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, Xn, Xc, y=None):\n",
    "        self.Xn = Xn; self.Xc = Xc\n",
    "        self.y  = None if y is None else y.astype(\"int64\")\n",
    "    def __len__(self): return len(self.Xn)\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is None: return self.Xn[i], self.Xc[i]\n",
    "        return self.Xn[i], self.Xc[i], self.y[i]\n",
    "\n",
    "# ------------------------ Tab-Transformer model ------------------------\n",
    "class CatTokenEmbeddings(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, d_token):\n",
    "        super().__init__()\n",
    "        self.n_cat = len(cat_cardinalities)\n",
    "        self.d_token = d_token\n",
    "        self.value_embs = nn.ModuleList([nn.Embedding(c, d_token) for c in cat_cardinalities])\n",
    "        self.col_emb = nn.Parameter(torch.zeros(self.n_cat, d_token))\n",
    "        nn.init.trunc_normal_(self.col_emb, std=0.02)\n",
    "    def forward(self, x_cat):\n",
    "        if x_cat.ndim == 1: x_cat = x_cat.unsqueeze(1)\n",
    "        B, n = x_cat.shape\n",
    "        if n == 0:\n",
    "            return x_cat.new_zeros((B, 0, self.d_token)).float()  # safe dtype\n",
    "        tokens = [emb(x_cat[:, j]) for j, emb in enumerate(self.value_embs)]\n",
    "        tok = torch.stack(tokens, dim=1)          # (B, n_cat, d_token)\n",
    "        tok = tok + self.col_emb.unsqueeze(0)     # add column embeddings\n",
    "        return tok\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, d_token, n_heads, d_ff, n_layers, attn_dropout, ffn_dropout, resid_dropout):\n",
    "        super().__init__()\n",
    "        if n_layers == 0:\n",
    "            self.encoder = None\n",
    "        else:\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_token, nhead=n_heads, dim_feedforward=d_ff,\n",
    "                dropout=resid_dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "            )\n",
    "            self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "    def forward(self, x_tokens):\n",
    "        if x_tokens.size(1) == 0 or self.encoder is None:\n",
    "            return x_tokens\n",
    "        return self.encoder(x_tokens)\n",
    "\n",
    "class NumericProjector(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, hidden=256, p=0.10, act=\"silu\"):\n",
    "        super().__init__()\n",
    "        if in_dim == 0:\n",
    "            self.net = None\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.LayerNorm(hidden), _act(act), nn.Dropout(p),\n",
    "                nn.Linear(hidden, out_dim), nn.LayerNorm(out_dim), _act(act),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        if self.net is None:\n",
    "            return x.new_zeros((x.size(0), 0))\n",
    "        return self.net(x)\n",
    "\n",
    "class HeadMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, depth, out_dim, p=0.10, act=\"silu\"):\n",
    "        super().__init__()\n",
    "        if in_dim == 0:\n",
    "            raise ValueError(\"No input features: fused_in == 0 (both numeric and categorical are empty).\")\n",
    "        layers = []\n",
    "        d = in_dim\n",
    "        for _ in range(max(0, depth-1)):\n",
    "            layers += [nn.Linear(d, hidden), nn.LayerNorm(hidden), _act(act), nn.Dropout(p)]\n",
    "            d = hidden\n",
    "        layers += [nn.Linear(d, out_dim)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, n_num, cat_cardinalities, d_token, n_heads, n_layers,\n",
    "                 d_ff, attn_dropout, ffn_dropout, resid_dropout,\n",
    "                 pooling, d_num_proj, head_hidden, head_depth, d_out, act=\"silu\"):\n",
    "        super().__init__()\n",
    "        self.n_num = n_num\n",
    "        self.pooling = pooling\n",
    "        self.d_token = d_token\n",
    "\n",
    "        self.cat_emb = CatTokenEmbeddings(cat_cardinalities, d_token) if cat_cardinalities else None\n",
    "        self.encoder  = TransformerEncoder(d_token, n_heads, d_ff, n_layers, attn_dropout, ffn_dropout, resid_dropout)\n",
    "        self.num_proj = NumericProjector(n_num, d_num_proj, hidden=max(128, d_num_proj*2), p=HEAD_DROPOUT, act=act) if (USE_NUMERIC and n_num>0) else NumericProjector(0,0)\n",
    "\n",
    "        cat_out_dim = 0\n",
    "        if cat_cardinalities:\n",
    "            cat_out_dim = (len(cat_cardinalities) * d_token) if pooling == \"concat\" else d_token\n",
    "        fused_in = cat_out_dim + (d_num_proj if (USE_NUMERIC and n_num>0) else 0)\n",
    "        print(f\"[TabTransformer] cat_out_dim={cat_out_dim}  num_proj={(d_num_proj if (USE_NUMERIC and n_num>0) else 0)}  fused_in={fused_in}\")\n",
    "        if fused_in == 0:\n",
    "            raise ValueError(\"No input features: both numeric and categorical are empty after preprocessing.\")\n",
    "        self.head = HeadMLP(fused_in, head_hidden, head_depth, d_out, p=HEAD_DROPOUT, act=act)\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        if self.cat_emb is not None and x_cat is not None and x_cat.size(1) > 0:\n",
    "            tok = self.cat_emb(x_cat)\n",
    "            enc = self.encoder(tok)\n",
    "            cat_repr = enc.mean(dim=1) if self.pooling == \"mean\" else enc.flatten(1)\n",
    "        else:\n",
    "            cat_repr = x_num.new_zeros((x_num.size(0), 0))\n",
    "        if self.n_num > 0 and x_num is not None and x_num.size(1) > 0:\n",
    "            num_repr = self.num_proj(x_num)\n",
    "        else:\n",
    "            num_repr = x_num.new_zeros((x_num.size(0), 0))\n",
    "        z = torch.cat([cat_repr, num_repr], dim=1)\n",
    "        return self.head(z)\n",
    "\n",
    "# ------------------------ LOAD & TARGET ------------------------\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "canon_cols_inplace(df)\n",
    "if TARGET_NAME not in df.columns:\n",
    "    raise KeyError(f\"Expected label column '{TARGET_NAME}' after canonicalization; got first columns {list(df.columns)[:20]}\")\n",
    "y1 = pd.to_numeric(df[TARGET_NAME], errors=\"coerce\").astype(\"Int64\")\n",
    "y1 = y1.where((y1>=1) & (y1<=10))\n",
    "mask = y1.notna()\n",
    "df = df.loc[mask].copy()\n",
    "y1 = y1.loc[mask].astype(\"int16\")\n",
    "y  = (y1 - 1).astype(\"int16\")\n",
    "X  = df.drop(columns=[TARGET_NAME])\n",
    "del df; gc.collect()\n",
    "\n",
    "# ------------------------ SPLIT (70/15/15) ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(SPLIT_DIR/\"train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(SPLIT_DIR/\"val_indices.csv\",   index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(SPLIT_DIR/\"test_indices.csv\",  index=False)\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ PREPROCESS & ENCODE ------------------------\n",
    "pp = TabularPreprocessor().fit(X_train)\n",
    "Xtr_df = pp.transform(X_train); Xva_df = pp.transform(X_val); Xte_df = pp.transform(X_test)\n",
    "\n",
    "enc = TorchTabEncoder(pp.num_cols_, pp.cat_cols_, pp.cat_categories_).fit(Xtr_df)\n",
    "Xtr_num, Xtr_cat = enc.transform(Xtr_df)\n",
    "Xva_num, Xva_cat = enc.transform(Xva_df)\n",
    "Xte_num, Xte_cat = enc.transform(Xte_df)\n",
    "\n",
    "# free raw frames\n",
    "del X_train, X_val, X_test, X_trainval, Xtr_df, Xva_df, Xte_df, X, y, y1; gc.collect()\n",
    "\n",
    "y_tr = y_train.values.astype(\"int64\")\n",
    "y_va = y_val.values.astype(\"int64\")\n",
    "y_te = y_test.values.astype(\"int64\")\n",
    "\n",
    "ds_tr = TabDataset(Xtr_num, Xtr_cat, y_tr)\n",
    "ds_va = TabDataset(Xva_num, Xva_cat, y_va)\n",
    "ds_te = TabDataset(Xte_num, Xte_cat, y_te)\n",
    "\n",
    "# Windows/CPU: set num_workers=0\n",
    "_dl_workers = NUM_WORKERS\n",
    "\n",
    "dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,  num_workers=_dl_workers)\n",
    "dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=_dl_workers)\n",
    "dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, num_workers=_dl_workers)\n",
    "\n",
    "# ------------------------ CLASS WEIGHTS ------------------------\n",
    "classes_present = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_tr)\n",
    "cw_map = {int(c): float(w) for c, w in zip(classes_present, cw)}\n",
    "class_weights = np.ones(NUM_CLASSES, dtype=\"float32\")\n",
    "for c, w in cw_map.items(): class_weights[c] = w\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# ------------------------ MODEL / OPTIM ------------------------\n",
    "device = torch.device(\"cpu\")\n",
    "n_num = Xtr_num.shape[1]\n",
    "cat_cards = [enc.cat_cardinalities_[c] for c in enc.cat_cols] if enc.cat_cols else []\n",
    "\n",
    "model = TabTransformer(\n",
    "    n_num=n_num, cat_cardinalities=cat_cards,\n",
    "    d_token=D_TOKEN, n_heads=N_HEADS, n_layers=N_LAYERS,\n",
    "    d_ff=D_FF, attn_dropout=ATTN_DROPOUT, ffn_dropout=FFN_DROPOUT, resid_dropout=RESID_DROPOUT,\n",
    "    pooling=POOLING, d_num_proj=D_NUM_PROJ if (USE_NUMERIC and n_num>0) else 0,\n",
    "    head_hidden=HEAD_HIDDEN, head_depth=HEAD_DEPTH, d_out=NUM_CLASSES, act=ACTIVATION\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Tab-Transformer params: {total_params/1e6:.2f}M | cats: {len(cat_cards)} | nums: {n_num}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def cosine_factor(epoch, max_epochs=MAX_EPOCHS, warmup=WARMUP_EPOCHS):\n",
    "    if epoch < warmup: return (epoch + 1) / max(1, warmup)\n",
    "    t = (epoch - warmup) / max(1, max_epochs - warmup)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda e: cosine_factor(e))\n",
    "\n",
    "# ------------------------ TRAIN (early stopping + capped batches) ------------------------\n",
    "best_val = float(\"inf\"); best_epoch = -1; pat = 0\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"lr\": []}\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (xnum_np, xcat_np, yb) in enumerate(dl_tr, 1):\n",
    "        xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "        xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "        yb   = yb.to(device)\n",
    "\n",
    "        logits = model(xnum, xcat)\n",
    "        loss = F.cross_entropy(logits, yb, weight=class_weights_t.to(device))\n",
    "\n",
    "        (loss / GRAD_ACC_STEPS).backward()\n",
    "        if step % GRAD_ACC_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total += loss.item() * yb.size(0); n += yb.size(0)\n",
    "        del xnum, xcat\n",
    "\n",
    "        if step >= MAX_BATCHES_PER_EPOCH:\n",
    "            break\n",
    "\n",
    "    train_loss = total / max(1, n)\n",
    "\n",
    "    # ---- validation\n",
    "    model.eval()\n",
    "    vtotal, vn = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xnum_np, xcat_np, yb in dl_va:\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            yb   = yb.to(device)\n",
    "            logits = model(xnum, xcat)\n",
    "            vloss = F.cross_entropy(logits, yb, weight=class_weights_t.to(device))\n",
    "            vtotal += vloss.item() * yb.size(0); vn += yb.size(0)\n",
    "            del xnum, xcat\n",
    "    val_loss = vtotal / max(1, vn)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_loss + MIN_DELTA < best_val:\n",
    "        best_val = val_loss; best_epoch = epoch; pat = 0\n",
    "        torch.save({\"state_dict\": model.state_dict(),\n",
    "                    \"tt_config\": {\n",
    "                        \"n_num\": n_num, \"cat_cardinalities\": cat_cards,\n",
    "                        \"d_token\": D_TOKEN, \"n_heads\": N_HEADS, \"n_layers\": N_LAYERS,\n",
    "                        \"d_ff\": D_FF, \"attn_dropout\": ATTN_DROPOUT, \"ffn_dropout\": FFN_DROPOUT, \"resid_dropout\": RESID_DROPOUT,\n",
    "                        \"pooling\": POOLING, \"d_num_proj\": (D_NUM_PROJ if (USE_NUMERIC and n_num>0) else 0),\n",
    "                        \"head_hidden\": HEAD_HIDDEN, \"head_depth\": HEAD_DEPTH,\n",
    "                        \"d_out\": NUM_CLASSES, \"activation\": ACTIVATION\n",
    "                    }},\n",
    "                   OUTPUT_DIR / \"tabtransformer_model.pt\")\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (best @ {best_epoch} | val {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Training time: {elapsed/60:.1f} min; best epoch: {best_epoch}\")\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "\n",
    "def predict_proba_dl(model, dl):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            if len(batch) == 3: xnum_np, xcat_np, _ = batch\n",
    "            else:               xnum_np, xcat_np = batch\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            logits = model(xnum, xcat)\n",
    "            p = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "            probs.append(p); del xnum, xcat\n",
    "    return np.vstack(probs)\n",
    "\n",
    "ckpt = torch.load(OUTPUT_DIR / \"tabtransformer_model.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "# leaner eval settings\n",
    "_eval_bs = 4096\n",
    "\n",
    "dl_va_eval = DataLoader(ds_va, batch_size=_eval_bs, shuffle=False, num_workers=0)\n",
    "dl_te_eval = DataLoader(ds_te, batch_size=_eval_bs, shuffle=False, num_workers=0)\n",
    "if EVAL_TRAIN:\n",
    "    dl_tr_eval = DataLoader(ds_tr, batch_size=_eval_bs, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "def eval_split(name, dl, y_zero):\n",
    "    proba = predict_proba_dl(model, dl)\n",
    "    pred0 = np.argmax(proba, axis=1)\n",
    "    metrics = {\n",
    "        \"split\": name,\n",
    "        \"n_samples\": int(len(y_zero)),\n",
    "        \"accuracy\": float(accuracy_score(y_zero, pred0)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_zero, pred0, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(y_zero, proba, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = float(\"nan\")\n",
    "    try:\n",
    "        y_bin = pd.get_dummies(pd.Categorical(y_zero, categories=list(range(NUM_CLASSES))))\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(roc_auc_score(y_bin.values, proba, average=\"macro\", multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(\"nan\")\n",
    "\n",
    "    ys_one  = y_zero + 1\n",
    "    pred_one = pred0 + 1\n",
    "    report = classification_report(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0)\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics, pred_one, proba\n",
    "\n",
    "metrics_rows = []\n",
    "if EVAL_TRAIN:\n",
    "    m_train, _, _ = eval_split(\"train\", dl_tr_eval, y_tr); metrics_rows.append(m_train)\n",
    "m_val,   _, _ = eval_split(\"val\",   dl_va_eval, y_va);    metrics_rows.append(m_val)\n",
    "m_test,  _, _ = eval_split(\"test\",  dl_te_eval, y_te);   metrics_rows.append(m_test)\n",
    "\n",
    "pd.DataFrame(metrics_rows).to_csv(OUTPUT_DIR / \"metrics_tabtransformer.csv\", index=False)\n",
    "print(pd.DataFrame(metrics_rows))\n",
    "\n",
    "# ------------------------ Curves ------------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_loss\"],   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"CE loss\"); plt.title(\"Tab-Transformer Loss\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"loss_curves.png\", dpi=150); plt.close()\n",
    "\n",
    "# ------------------------ Save artifacts ------------------------\n",
    "joblib.dump(pp, OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "enc.save_meta(OUTPUT_DIR / \"encoder_meta.json\")\n",
    "with open(OUTPUT_DIR / \"training_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"target_name\": TARGET_NAME,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"label_order_zero_indexed\": list(range(NUM_CLASSES)),\n",
    "        \"seed\": SEED,\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"splits\": {\"train\": \"splits/train_indices.csv\", \"val\": \"splits/val_indices.csv\", \"test\": \"splits/test_indices.csv\"},\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"tt_config\": ckpt[\"tt_config\"],\n",
    "        \"train_time_min\": round(elapsed/60, 2),\n",
    "        \"feature_names\": pp.feature_names_,\n",
    "        \"cat_features\": pp.cat_cols_,\n",
    "        \"num_features\": pp.num_cols_,\n",
    "        \"column_names_canonicalized\": True\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ All artifacts saved to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ Inference helper ------------------------\n",
    "\n",
    "def predict_target_risk_class(df_new: pd.DataFrame,\n",
    "                              model_path=OUTPUT_DIR / \"tabtransformer_model.pt\",\n",
    "                              preproc_path=OUTPUT_DIR / \"preprocessor.pkl\",\n",
    "                              encoder_meta_path=OUTPUT_DIR / \"encoder_meta.json\",\n",
    "                              batch_size=4096) -> pd.Series:\n",
    "    \"\"\"Predict on new raw rows (returns labels in 1..10).\"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    pp = joblib.load(preproc_path)\n",
    "    enc = TorchTabEncoder.load_meta(encoder_meta_path)\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    cfg = ckpt[\"tt_config\"]\n",
    "\n",
    "    df_new = df_new.copy(); canon_cols_inplace(df_new)\n",
    "\n",
    "    n_num = len(enc.num_cols)\n",
    "    cat_cards = [enc.cat_cardinalities_[c] for c in enc.cat_cols] if enc.cat_cols else []\n",
    "    model = TabTransformer(\n",
    "        n_num=n_num, cat_cardinalities=cat_cards,\n",
    "        d_token=cfg[\"d_token\"], n_heads=cfg[\"n_heads\"], n_layers=cfg[\"n_layers\"],\n",
    "        d_ff=cfg[\"d_ff\"], attn_dropout=cfg[\"attn_dropout\"], ffn_dropout=cfg[\"ffn_dropout\"],\n",
    "        resid_dropout=cfg[\"resid_dropout\"], pooling=cfg[\"pooling\"],\n",
    "        d_num_proj=cfg[\"d_num_proj\"], head_hidden=cfg[\"head_hidden\"], head_depth=cfg[\"head_depth\"],\n",
    "        d_out=cfg[\"d_out\"], act=cfg.get(\"activation\",\"silu\")\n",
    "    ).to(device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n",
    "\n",
    "    dproc = pp.transform(df_new)\n",
    "    Xn, Xc = enc.transform(dproc)\n",
    "    ds = TabDataset(Xn, Xc, y=None)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xnum_np, xcat_np in dl:\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            logits = model(xnum, xcat)\n",
    "            preds.append(torch.argmax(F.softmax(logits, dim=-1), dim=-1).cpu().numpy())\n",
    "    yhat0 = np.concatenate(preds, axis=0).astype(\"int16\")\n",
    "    return pd.Series(yhat0 + 1, index=dproc.index, name=\"pred_target_risk_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRNw-cQpW9nS"
   },
   "source": [
    "#### FT-Transformer (CPU/VSCODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1762938823410,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "2su5SjP5W_9W"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# FT-Transformer (CPU, PyTorch, no external rtdl) – Multiclass Pipeline (FAST v2)\n",
    "# - Outputs under ./RESULTS/fttransformer_outputs/<timestamp>\n",
    "# - Canonicalized column names; label column = target_risk_class (values 1..10)\n",
    "# - Preprocess: datetime expansion, numeric downcast+median; capped & unique categoricals\n",
    "# - CPU-optimized: tiny encoder, capped batches/epoch, no dataloader workers\n",
    "# - Training: AdamW, cosine LR, early stopping (val CE)\n",
    "# - Metrics: accuracy, macro/weighted P/R/F1, log loss, ROC-AUC OvR macro\n",
    "# - Artifacts: fttransformer_model.pt, preprocessor.pkl, encoder_meta.json, metrics.csv,\n",
    "#   loss_curves.png, reports + confusions, training_meta.json\n",
    "# - Inference helper returns labels in 1..10\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, warnings, datetime, math, time, random, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    log_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ PATHS ------------------------\n",
    "DATA_CSV = \"path\"\n",
    "RUN_NAME = f\"FTTransformer_CPU_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUTPUT_DIR = Path(\"path\") / RUN_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_DIR  = OUTPUT_DIR / \"splits\"; SPLIT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Saving all outputs to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "# On CPU, fewer threads can be faster\n",
    "torch.set_num_threads(max(1, (os.cpu_count() or 4)//2))\n",
    "\n",
    "TARGET_NAME = \"target_risk_class\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "\n",
    "# ---- Training profile (FAST) ----\n",
    "BATCH_SIZE       = 4096        # larger batch for tiny model\n",
    "GRAD_ACC_STEPS   = 1\n",
    "MAX_EPOCHS       = 10\n",
    "BASE_LR          = 2e-3\n",
    "WEIGHT_DECAY     = 3e-4\n",
    "PATIENCE         = 2\n",
    "MIN_DELTA        = 1e-4\n",
    "WARMUP_EPOCHS    = 2\n",
    "NUM_WORKERS      = 0           # Windows/CPU: avoid multiprocessing overhead\n",
    "MAX_BATCHES_PER_EPOCH = 100    # caps work per epoch (~100*4096 ≈ 410k samples)\n",
    "\n",
    "# ---- Evaluation toggles ----\n",
    "EVAL_TRAIN = False             # set True if you need train metrics too\n",
    "\n",
    "# ---- Preprocessing ----\n",
    "MISSING_TOKEN      = \"Missing\"\n",
    "MAX_CAT_CARD       = 200       # cap cardinality to shrink embeddings\n",
    "STORE_NUM_AS_FP16  = True\n",
    "STORE_CAT_AS_INT32 = True\n",
    "\n",
    "# ---- FT-style model hyperparams (tiny-but-strong) ----\n",
    "D_TOKEN            = 32\n",
    "N_BLOCKS           = 1\n",
    "ATTN_N_HEADS       = 4\n",
    "FFN_D_HIDDEN       = 128\n",
    "ATTN_DROPOUT       = 0.10\n",
    "FFN_DROPOUT        = 0.10\n",
    "RESID_DROPOUT      = 0.10\n",
    "USE_CLS_TOKEN      = False     # mean pooling keeps sequence length small\n",
    "\n",
    "assert D_TOKEN % ATTN_N_HEADS == 0, \"D_TOKEN must be divisible by ATTN_N_HEADS\"\n",
    "\n",
    "# ------------------------ helpers ------------------------\n",
    "def canon_col(name: str) -> str:\n",
    "    s = re.sub(r\"[^0-9A-Za-z_]+\", \"_\", str(name))\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def canon_cols_inplace(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [canon_col(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique_in_order(seq):\n",
    "    seen = set(); out = []\n",
    "    for x in seq:\n",
    "        xs = str(x)\n",
    "        if xs not in seen:\n",
    "            seen.add(xs); out.append(xs)\n",
    "    return out\n",
    "\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"int64\",\"int32\",\"float32\"]).columns:\n",
    "        if pdt.is_float_dtype(df[c]): df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:                         df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].astype(object)\n",
    "            if s.astype(str).str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try: df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception: pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pdt.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]   = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"]  = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]    = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]    = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]   = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"] = s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]   = s.dt.is_month_end.astype(\"Int8\")\n",
    "    if dt_cols: df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def pd_cat_fix(series, allowed):\n",
    "    allowed = _unique_in_order(list(allowed) + [MISSING_TOKEN])\n",
    "    s = series.astype(\"string\").fillna(MISSING_TOKEN)\n",
    "    s = s.where(s.isin(allowed), MISSING_TOKEN)\n",
    "    return pd.Categorical(s, categories=pd.Index(allowed), ordered=False)\n",
    "\n",
    "# ------------------------ Preprocessor ------------------------\n",
    "class TabularPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.num_cols_ = []; self.cat_cols_ = []\n",
    "        self.num_median_ = {}; self.cat_categories_ = {}\n",
    "        self.feature_names_ = []; self.fitted_ = False\n",
    "\n",
    "    def _prep_base(self, X):\n",
    "        d = X.copy()\n",
    "        canon_cols_inplace(d)\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == object:\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        for c in d.columns:\n",
    "            if pdt.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X):\n",
    "        d = self._prep_base(X)\n",
    "        self.num_cols_ = [c for c in d.columns if pdt.is_numeric_dtype(d[c])]\n",
    "        self.cat_cols_ = [c for c in d.columns if not pdt.is_numeric_dtype(d[c])]\n",
    "        for c in self.num_cols_:\n",
    "            self.num_median_[c] = float(pd.to_numeric(d[c], errors=\"coerce\").median())\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(\"string\").fillna(MISSING_TOKEN)\n",
    "            vc = s.value_counts(dropna=False)\n",
    "            if MAX_CAT_CARD and len(vc) > (MAX_CAT_CARD - 1):\n",
    "                top = vc.index.astype(\"string\").tolist()[:MAX_CAT_CARD - 1]\n",
    "                cats = _unique_in_order(top + [MISSING_TOKEN])\n",
    "            else:\n",
    "                cats = _unique_in_order(pd.unique(s).astype(\"string\").tolist() + [MISSING_TOKEN])\n",
    "            if len(cats) != len(set(cats)):\n",
    "                cats = _unique_in_order(cats)\n",
    "            self.cat_categories_[c] = cats\n",
    "        self.feature_names_ = self.num_cols_ + self.cat_cols_\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert self.fitted_\n",
    "        d = self._prep_base(X)\n",
    "        for c in self.feature_names_:\n",
    "            if c not in d.columns:\n",
    "                d[c] = np.nan\n",
    "        d = d[self.feature_names_].copy()\n",
    "        for c in self.num_cols_:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").astype(\"float32\")\n",
    "            if d[c].isna().any():\n",
    "                d[c] = d[c].fillna(self.num_median_[c])\n",
    "        for c in self.cat_cols_:\n",
    "            d[c] = pd_cat_fix(d[c], self.cat_categories_[c])\n",
    "        return d\n",
    "\n",
    "# ------------------------ Encoder (standardize nums + codes for cats) ------------------------\n",
    "class TorchTabEncoder:\n",
    "    def __init__(self, num_cols, cat_cols, cat_categories):\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.cat_categories = {c: list(cats) for c, cats in cat_categories.items()}\n",
    "        self.num_mean_ = None; self.num_std_ = None\n",
    "        self.cat_cardinalities_ = {c: len(self.cat_categories[c]) for c in self.cat_cols}\n",
    "\n",
    "    def fit(self, df_proc):\n",
    "        if self.num_cols:\n",
    "            arr = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            self.num_mean_ = arr.mean(axis=0).astype(\"float32\")\n",
    "            std = arr.std(axis=0).astype(\"float32\")\n",
    "            self.num_std_  = np.where(std < 1e-6, 1.0, std).astype(\"float32\")\n",
    "        else:\n",
    "            self.num_mean_ = np.array([], dtype=\"float32\")\n",
    "            self.num_std_  = np.array([], dtype=\"float32\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_proc):\n",
    "        if self.num_cols:\n",
    "            Xn = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            Xn = (Xn - self.num_mean_) / self.num_std_\n",
    "            if STORE_NUM_AS_FP16: Xn = Xn.astype(\"float16\")\n",
    "        else:\n",
    "            Xn = np.zeros((len(df_proc), 0), dtype=\"float16\" if STORE_NUM_AS_FP16 else \"float32\")\n",
    "\n",
    "        Xc_list = []\n",
    "        for c in self.cat_cols:\n",
    "            codes = df_proc[c].cat.codes.to_numpy(copy=False)\n",
    "            fix = self.cat_categories[c].index(MISSING_TOKEN)\n",
    "            codes = np.where(codes < 0, fix, codes)\n",
    "            codes = codes.astype(\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "            Xc_list.append(codes)\n",
    "        Xc = np.stack(Xc_list, axis=1) if Xc_list else np.zeros((len(df_proc),0), dtype=\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "        return Xn, Xc\n",
    "\n",
    "    def save_meta(self, path_json):\n",
    "        meta = {\n",
    "            \"num_cols\": self.num_cols,\n",
    "            \"cat_cols\": self.cat_cols,\n",
    "            \"cat_categories\": self.cat_categories,\n",
    "            \"num_mean\": self.num_mean_.tolist(),\n",
    "            \"num_std\": self.num_std_.tolist(),\n",
    "            \"cat_cardinalities\": self.cat_cardinalities_,\n",
    "        }\n",
    "        with open(path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_meta(path_json):\n",
    "        with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        enc = TorchTabEncoder(meta[\"num_cols\"], meta[\"cat_cols\"], meta[\"cat_categories\"])\n",
    "        enc.num_mean_ = np.array(meta[\"num_mean\"], dtype=\"float32\")\n",
    "        enc.num_std_  = np.array(meta[\"num_std\"], dtype=\"float32\")\n",
    "        enc.cat_cardinalities_ = {k:int(v) for k,v in meta[\"cat_cardinalities\"].items()}\n",
    "        return enc\n",
    "\n",
    "# ------------------------ Dataset ------------------------\n",
    "class TabDataset(Dataset):\n",
    "    def __init__(self, Xn, Xc, y=None):\n",
    "        self.Xn = Xn; self.Xc = Xc\n",
    "        self.y  = None if y is None else y.astype(\"int64\")\n",
    "    def __len__(self): return len(self.Xn)\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is None: return self.Xn[i], self.Xc[i]\n",
    "        return self.Xn[i], self.Xc[i], self.y[i]\n",
    "\n",
    "# ------------------------ FT-style Model (no rtdl) ------------------------\n",
    "class NumericFeatureTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Tokenizes each numeric feature as: token_j = x_j * W_j + b_j, where W_j, b_j in R^{d_token}.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_num: int, d_token: int):\n",
    "        super().__init__()\n",
    "        self.n_num = n_num\n",
    "        self.d_token = d_token\n",
    "        if n_num > 0:\n",
    "            self.weight = nn.Parameter(torch.empty(n_num, d_token))\n",
    "            self.bias   = nn.Parameter(torch.zeros(n_num, d_token))\n",
    "            nn.init.trunc_normal_(self.weight, std=0.02)\n",
    "            nn.init.zeros_(self.bias)\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\",   None)\n",
    "\n",
    "    def forward(self, x_num: torch.Tensor):  # (B, n_num)\n",
    "        if self.n_num == 0:\n",
    "            return x_num.new_zeros((x_num.size(0), 0, self.d_token))\n",
    "        return x_num.unsqueeze(-1) * self.weight.unsqueeze(0) + self.bias.unsqueeze(0)\n",
    "\n",
    "class CategoricalFeatureTokenizer(nn.Module):\n",
    "    def __init__(self, cat_cardinalities, d_token):\n",
    "        super().__init__()\n",
    "        self.n_cat = len(cat_cardinalities)\n",
    "        self.d_token = d_token\n",
    "        self.embs = nn.ModuleList([nn.Embedding(card, d_token) for card in cat_cardinalities])\n",
    "        for emb in self.embs:\n",
    "            nn.init.trunc_normal_(emb.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x_cat: torch.Tensor):  # (B, n_cat)\n",
    "        if self.n_cat == 0:\n",
    "            return x_cat.new_zeros((x_cat.size(0), 0, self.d_token), dtype=torch.float32)\n",
    "        toks = [emb(x_cat[:, j]) for j, emb in enumerate(self.embs)]\n",
    "        return torch.stack(toks, dim=1)   # (B, n_cat, d)\n",
    "\n",
    "class FTTransformerNoRTDL(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_num: int,\n",
    "                 cat_cardinalities: list,\n",
    "                 d_token: int,\n",
    "                 n_blocks: int,\n",
    "                 n_heads: int,\n",
    "                 ffn_d_hidden: int,\n",
    "                 attn_dropout: float,\n",
    "                 ffn_dropout: float,\n",
    "                 resid_dropout: float,\n",
    "                 use_cls_token: bool,\n",
    "                 d_out: int):\n",
    "        super().__init__()\n",
    "        self.n_num = n_num\n",
    "        self.use_cls = use_cls_token\n",
    "        self.d_token = d_token\n",
    "\n",
    "        self.num_tok = NumericFeatureTokenizer(n_num, d_token)\n",
    "        self.cat_tok = CategoricalFeatureTokenizer(cat_cardinalities, d_token) if cat_cardinalities else None\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_token)) if use_cls_token else None\n",
    "        if self.cls is not None:\n",
    "            nn.init.trunc_normal_(self.cls, std=0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_token,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=ffn_d_hidden,\n",
    "            dropout=resid_dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_blocks)\n",
    "        self.dropout = nn.Dropout(resid_dropout)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_token),\n",
    "            nn.Linear(d_token, d_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num: torch.Tensor, x_cat: torch.Tensor):\n",
    "        num_tokens = self.num_tok(x_num) if x_num is not None and x_num.size(1) > 0 else None\n",
    "        cat_tokens = self.cat_tok(x_cat) if (self.cat_tok is not None and x_cat is not None and x_cat.size(1) > 0) else None\n",
    "        if num_tokens is None and cat_tokens is None:\n",
    "            raise ValueError(\"Both numeric and categorical tokens are empty.\")\n",
    "\n",
    "        tokens = cat_tokens if num_tokens is None else num_tokens if cat_tokens is None else torch.cat([num_tokens, cat_tokens], dim=1)\n",
    "\n",
    "        if self.use_cls:\n",
    "            B = tokens.size(0)\n",
    "            cls_tok = self.cls.expand(B, -1, -1)\n",
    "            tokens = torch.cat([cls_tok, tokens], dim=1)\n",
    "\n",
    "        z = self.encoder(tokens)\n",
    "        pooled = z[:, 0, :] if self.use_cls else z.mean(dim=1)\n",
    "        logits = self.head(self.dropout(pooled))\n",
    "        return logits\n",
    "\n",
    "# ------------------------ LOAD & TARGET ------------------------\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "canon_cols_inplace(df)\n",
    "if TARGET_NAME not in df.columns:\n",
    "    raise KeyError(f\"Expected label column '{TARGET_NAME}' after canonicalization; got first columns {list(df.columns)[:20]}\")\n",
    "y1 = pd.to_numeric(df[TARGET_NAME], errors=\"coerce\").astype(\"Int64\")\n",
    "y1 = y1.where((y1>=1) & (y1<=10))\n",
    "mask = y1.notna()\n",
    "df = df.loc[mask].copy()\n",
    "y1 = y1.loc[mask].astype(\"int16\")\n",
    "y  = (y1 - 1).astype(\"int16\")\n",
    "X  = df.drop(columns=[TARGET_NAME])\n",
    "del df; gc.collect()\n",
    "\n",
    "# ------------------------ SPLIT (70/15/15) ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(SPLIT_DIR/\"train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(SPLIT_DIR/\"val_indices.csv\",   index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(SPLIT_DIR/\"test_indices.csv\",  index=False)\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ PREPROCESS & ENCODE ------------------------\n",
    "pp = TabularPreprocessor().fit(X_train)\n",
    "Xtr_df = pp.transform(X_train); Xva_df = pp.transform(X_val); Xte_df = pp.transform(X_test)\n",
    "\n",
    "enc = TorchTabEncoder(pp.num_cols_, pp.cat_cols_, pp.cat_categories_).fit(Xtr_df)\n",
    "Xtr_num, Xtr_cat = enc.transform(Xtr_df)\n",
    "Xva_num, Xva_cat = enc.transform(Xva_df)\n",
    "Xte_num, Xte_cat = enc.transform(Xte_df)\n",
    "\n",
    "# free raw frames\n",
    "del X_train, X_val, X_test, X_trainval, Xtr_df, Xva_df, Xte_df, X, y, y1; gc.collect()\n",
    "\n",
    "y_tr = y_train.values.astype(\"int64\")\n",
    "y_va = y_val.values.astype(\"int64\")\n",
    "y_te = y_test.values.astype(\"int64\")\n",
    "\n",
    "ds_tr = TabDataset(Xtr_num, Xtr_cat, y_tr)\n",
    "ds_va = TabDataset(Xva_num, Xva_cat, y_va)\n",
    "ds_te = TabDataset(Xte_num, Xte_cat, y_te)\n",
    "\n",
    "# Windows/CPU: num_workers=0\n",
    "_dl_workers = NUM_WORKERS\n",
    "\n",
    "dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True,  num_workers=_dl_workers)\n",
    "dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, num_workers=_dl_workers)\n",
    "dl_te = DataLoader(ds_te, batch_size=BATCH_SIZE, shuffle=False, num_workers=_dl_workers)\n",
    "\n",
    "# ------------------------ CLASS WEIGHTS ------------------------\n",
    "classes_present = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_tr)\n",
    "cw_map = {int(c): float(w) for c, w in zip(classes_present, cw)}\n",
    "class_weights = np.ones(NUM_CLASSES, dtype=\"float32\")\n",
    "for c, w in cw_map.items(): class_weights[c] = w\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# ------------------------ MODEL / OPTIM ------------------------\n",
    "device = torch.device(\"cpu\")\n",
    "n_num = Xtr_num.shape[1]\n",
    "cat_cards = [enc.cat_cardinalities_[c] for c in enc.cat_cols] if enc.cat_cols else []\n",
    "\n",
    "if (n_num == 0) and (len(cat_cards) == 0):\n",
    "    raise ValueError(\"No input features: both numeric and categorical are empty after preprocessing.\")\n",
    "\n",
    "model = FTTransformerNoRTDL(\n",
    "    n_num=n_num,\n",
    "    cat_cardinalities=cat_cards,\n",
    "    d_token=D_TOKEN,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_heads=ATTN_N_HEADS,\n",
    "    ffn_d_hidden=FFN_D_HIDDEN,\n",
    "    attn_dropout=ATTN_DROPOUT,\n",
    "    ffn_dropout=FFN_DROPOUT,\n",
    "    resid_dropout=RESID_DROPOUT,\n",
    "    use_cls_token=USE_CLS_TOKEN,\n",
    "    d_out=NUM_CLASSES\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"FT-style params: {total_params/1e6:.2f}M | cats: {len(cat_cards)} | nums: {n_num}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "def cosine_factor(epoch, max_epochs=MAX_EPOCHS, warmup=WARMUP_EPOCHS):\n",
    "    if epoch < warmup: return (epoch + 1) / max(1, warmup)\n",
    "    t = (epoch - warmup) / max(1, max_epochs - warmup)\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * t))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda e: cosine_factor(e))\n",
    "\n",
    "# ------------------------ TRAIN (early stopping + capped batches) ------------------------\n",
    "best_val = float(\"inf\"); best_epoch = -1; pat = 0\n",
    "history = {\"epoch\": [], \"train_loss\": [], \"val_loss\": [], \"lr\": []}\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, (xnum_np, xcat_np, yb) in enumerate(dl_tr, 1):\n",
    "        xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "        xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "        yb   = yb.to(device)\n",
    "\n",
    "        logits = model(xnum if n_num>0 else None, xcat if len(cat_cards)>0 else None)\n",
    "        loss = F.cross_entropy(logits, yb, weight=class_weights_t.to(device))\n",
    "\n",
    "        (loss / GRAD_ACC_STEPS).backward()\n",
    "        if step % GRAD_ACC_STEPS == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total += loss.item() * yb.size(0); n += yb.size(0)\n",
    "        del xnum, xcat\n",
    "\n",
    "        if step >= MAX_BATCHES_PER_EPOCH:\n",
    "            break\n",
    "\n",
    "    train_loss = total / max(1, n)\n",
    "\n",
    "    # ---- validation\n",
    "    model.eval()\n",
    "    vtotal, vn = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for xnum_np, xcat_np, yb in dl_va:\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            yb   = yb.to(device)\n",
    "            logits = model(xnum if n_num>0 else None, xcat if len(cat_cards)>0 else None)\n",
    "            vloss = F.cross_entropy(logits, yb, weight=class_weights_t.to(device))\n",
    "            vtotal += vloss.item() * yb.size(0); vn += yb.size(0)\n",
    "            del xnum, xcat\n",
    "    val_loss = vtotal / max(1, vn)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"epoch\"].append(epoch)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
    "    print(f\"Epoch {epoch:03d} | train {train_loss:.4f} | val {val_loss:.4f} | lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "    if val_loss + MIN_DELTA < best_val:\n",
    "        best_val = val_loss; best_epoch = epoch; pat = 0\n",
    "        torch.save({\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"ftt_config\": {\n",
    "                \"n_num\": n_num, \"cat_cardinalities\": cat_cards,\n",
    "                \"d_token\": D_TOKEN, \"n_blocks\": N_BLOCKS, \"attention_n_heads\": ATTN_N_HEADS,\n",
    "                \"ffn_d_hidden\": FFN_D_HIDDEN, \"attention_dropout\": ATTN_DROPOUT,\n",
    "                \"ffn_dropout\": FFN_DROPOUT, \"residual_dropout\": RESID_DROPOUT,\n",
    "                \"use_cls_token\": USE_CLS_TOKEN,\n",
    "                \"d_out\": NUM_CLASSES\n",
    "            }\n",
    "        }, OUTPUT_DIR / \"fttransformer_model.pt\")\n",
    "    else:\n",
    "        pat += 1\n",
    "        if pat >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (best @ {best_epoch} | val {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "elapsed = time.time() - t0\n",
    "print(f\"Training time: {elapsed/60:.1f} min; best epoch: {best_epoch}\")\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "def predict_proba_dl(model, dl):\n",
    "    model.eval()\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            if len(batch) == 3: xnum_np, xcat_np, _ = batch\n",
    "            else:               xnum_np, xcat_np = batch\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            logits = model(xnum if n_num>0 else None, xcat if len(cat_cards)>0 else None)\n",
    "            p = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "            probs.append(p); del xnum, xcat\n",
    "    return np.vstack(probs)\n",
    "\n",
    "ckpt = torch.load(OUTPUT_DIR / \"fttransformer_model.pt\", map_location=device)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "_eval_bs = 4096\n",
    "dl_va_eval = DataLoader(ds_va, batch_size=_eval_bs, shuffle=False, num_workers=0)\n",
    "dl_te_eval = DataLoader(ds_te, batch_size=_eval_bs, shuffle=False, num_workers=0)\n",
    "if EVAL_TRAIN:\n",
    "    dl_tr_eval = DataLoader(ds_tr, batch_size=_eval_bs, shuffle=False, num_workers=0)\n",
    "\n",
    "def eval_split(name, dl, y_zero):\n",
    "    proba = predict_proba_dl(model, dl)\n",
    "    pred0 = np.argmax(proba, axis=1)\n",
    "    metrics = {\n",
    "        \"split\": name,\n",
    "        \"n_samples\": int(len(y_zero)),\n",
    "        \"accuracy\": float(accuracy_score(y_zero, pred0)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_zero, pred0, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(y_zero, proba, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = float(\"nan\")\n",
    "    try:\n",
    "        y_bin = pd.get_dummies(pd.Categorical(y_zero, categories=list(range(NUM_CLASSES))))\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(roc_auc_score(y_bin.values, proba, average=\"macro\", multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(\"nan\")\n",
    "\n",
    "    ys_one  = y_zero + 1\n",
    "    pred_one = pred0 + 1\n",
    "    report = classification_report(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0)\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics, pred_one, proba\n",
    "\n",
    "metrics_rows = []\n",
    "if EVAL_TRAIN:\n",
    "    m_train, _, _ = eval_split(\"train\", dl_tr_eval, y_tr); metrics_rows.append(m_train)\n",
    "m_val,   _, _ = eval_split(\"val\",   dl_va_eval, y_va);    metrics_rows.append(m_val)\n",
    "m_test,  _, _ = eval_split(\"test\",  dl_te_eval, y_te);   metrics_rows.append(m_test)\n",
    "\n",
    "pd.DataFrame(metrics_rows).to_csv(OUTPUT_DIR / \"metrics_fttransformer.csv\", index=False)\n",
    "print(pd.DataFrame(metrics_rows))\n",
    "\n",
    "# ------------------------ Curves ------------------------\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"train\")\n",
    "plt.plot(history[\"epoch\"], history[\"val_loss\"],   label=\"val\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"CE loss\"); plt.title(\"FT-Transformer (no rtdl) Loss\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"loss_curves.png\", dpi=150); plt.close()\n",
    "\n",
    "# ------------------------ Save artifacts ------------------------\n",
    "joblib.dump(pp, OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "enc.save_meta(OUTPUT_DIR / \"encoder_meta.json\")\n",
    "with open(OUTPUT_DIR / \"training_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"target_name\": TARGET_NAME,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"label_order_zero_indexed\": list(range(NUM_CLASSES)),\n",
    "        \"seed\": SEED,\n",
    "        \"best_epoch\": int(best_epoch),\n",
    "        \"splits\": {\"train\": \"splits/train_indices.csv\", \"val\": \"splits/val_indices.csv\", \"test\": \"splits/test_indices.csv\"},\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"ftt_config\": {\n",
    "            \"n_num\": n_num, \"cat_cardinalities\": cat_cards,\n",
    "            \"d_token\": D_TOKEN, \"n_blocks\": N_BLOCKS, \"attention_n_heads\": ATTN_N_HEADS,\n",
    "            \"ffn_d_hidden\": FFN_D_HIDDEN, \"attention_dropout\": ATTN_DROPOUT,\n",
    "            \"ffn_dropout\": FFN_DROPOUT, \"residual_dropout\": RESID_DROPOUT,\n",
    "            \"use_cls_token\": USE_CLS_TOKEN, \"d_out\": NUM_CLASSES\n",
    "        },\n",
    "        \"train_time_min\": round(elapsed/60, 2),\n",
    "        \"feature_names\": pp.feature_names_,\n",
    "        \"cat_features\": pp.cat_cols_,\n",
    "        \"num_features\": pp.num_cols_,\n",
    "        \"column_names_canonicalized\": True\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ All artifacts saved to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ Inference helper ------------------------\n",
    "def predict_target_risk_class(\n",
    "    df_new: pd.DataFrame,\n",
    "    model_path=OUTPUT_DIR / \"fttransformer_model.pt\",\n",
    "    preproc_path=OUTPUT_DIR / \"preprocessor.pkl\",\n",
    "    encoder_meta_path=OUTPUT_DIR / \"encoder_meta.json\",\n",
    "    batch_size=4096\n",
    ") -> pd.Series:\n",
    "    \"\"\"Predict on new raw rows (returns labels in 1..10).\"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    pp_inf = joblib.load(preproc_path)\n",
    "    enc_inf = TorchTabEncoder.load_meta(encoder_meta_path)\n",
    "\n",
    "    cfg = json.load(open(OUTPUT_DIR / \"training_meta.json\", \"r\", encoding=\"utf-8\"))[\"ftt_config\"]\n",
    "    n_num = len(enc_inf.num_cols)\n",
    "    cat_cards = [enc_inf.cat_cardinalities_[c] for c in enc_inf.cat_cols] if enc_inf.cat_cols else []\n",
    "    model = FTTransformerNoRTDL(\n",
    "        n_num=n_num,\n",
    "        cat_cardinalities=cat_cards,\n",
    "        d_token=cfg[\"d_token\"],\n",
    "        n_blocks=cfg[\"n_blocks\"],\n",
    "        n_heads=cfg[\"attention_n_heads\"],\n",
    "        ffn_d_hidden=cfg[\"ffn_d_hidden\"],\n",
    "        attn_dropout=cfg[\"attention_dropout\"],\n",
    "        ffn_dropout=cfg[\"ffn_dropout\"],\n",
    "        resid_dropout=cfg[\"residual_dropout\"],\n",
    "        use_cls_token=cfg.get(\"use_cls_token\", False),\n",
    "        d_out=cfg[\"d_out\"]\n",
    "    ).to(device)\n",
    "    ckpt = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"state_dict\"]); model.eval()\n",
    "\n",
    "    # Preprocess -> encode\n",
    "    df_new = df_new.copy(); canon_cols_inplace(df_new)\n",
    "    dproc = pp_inf.transform(df_new)\n",
    "    Xn, Xc = enc_inf.transform(dproc)\n",
    "    ds = TabDataset(Xn, Xc, y=None)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xnum_np, xcat_np in dl:\n",
    "            xnum = torch.as_tensor(xnum_np, device=device).float()\n",
    "            xcat = torch.as_tensor(xcat_np, device=device).long()\n",
    "            logits = model(xnum if n_num>0 else None, xcat if len(cat_cards)>0 else None)\n",
    "            preds.append(torch.argmax(F.softmax(logits, dim=-1), dim=-1).cpu().numpy())\n",
    "    yhat0 = np.concatenate(preds, axis=0).astype(\"int16\")\n",
    "    return pd.Series(yhat0 + 1, index=dproc.index, name=\"pred_target_risk_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kCm1Y4ruGPU"
   },
   "source": [
    "### Deep Learning (MLP) Based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qx50xUM1uJzY"
   },
   "source": [
    "#### Tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1762938873469,
     "user": {
      "displayName": "mehdi Shahsavari",
      "userId": "06278019994274872134"
     },
     "user_tz": -210
    },
    "id": "7X7yLmyPuN7G"
   },
   "outputs": [],
   "source": [
    "# ===============================================================\n",
    "# TabNet (CPU) – Multiclass Pipeline (FAST, robust)\n",
    "# - Mirrors your other pipelines: same preprocessing + artifacts\n",
    "# - Preprocess: datetime expansion; numeric downcast+median; capped & unique categoricals\n",
    "# - Encoding: standardized numerics + integer codes for categoricals\n",
    "# - Model: TabNet with categorical embeddings, early stopping on val\n",
    "# - Metrics: accuracy, macro/weighted P/R/F1, log loss, ROC-AUC OvR macro\n",
    "# - Artifacts: tabnet_model.zip, preprocessor.pkl, encoder_meta.json, metrics.csv,\n",
    "#              loss_curves.png, classification reports + confusion matrices, training_meta.json\n",
    "# - Inference helper returns labels in 1..10\n",
    "# ===============================================================\n",
    "\n",
    "import os, re, json, warnings, datetime, time, random, gc, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types as pdt\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
    "    log_loss, classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------------ TabNet ------------------------\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ------------------------ PATHS ------------------------\n",
    "DATA_CSV = \"path\"\n",
    "RUN_NAME = f\"TabNet_CPU_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "OUTPUT_DIR = Path(\"path\") / RUN_NAME\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SPLIT_DIR  = OUTPUT_DIR / \"splits\"; SPLIT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Saving all outputs to: {OUTPUT_DIR.resolve()}\")\n",
    "\n",
    "# ------------------------ CONFIG ------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.set_num_threads(max(1, (os.cpu_count() or 4)//2))\n",
    "\n",
    "TARGET_NAME = \"target_risk_class\"\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE  = 0.15\n",
    "\n",
    "# ---- Preprocessing ----\n",
    "MISSING_TOKEN      = \"Missing\"\n",
    "MAX_CAT_CARD       = 500     # cap very high-card categorical columns\n",
    "STORE_CAT_AS_INT32 = True    # for memory\n",
    "\n",
    "# ---- TabNet hyperparams (CPU-friendly) ----\n",
    "TN_N_D            = 32\n",
    "TN_N_A            = 32\n",
    "TN_N_STEPS        = 3\n",
    "TN_GAMMA          = 1.5\n",
    "TN_N_INDEPENDENT  = 1\n",
    "TN_N_SHARED       = 1\n",
    "TN_MOMENTUM       = 0.02\n",
    "TN_VBS            = 4096               # virtual batch size\n",
    "TN_BATCH_SIZE     = 65536              # real batch size (CPU-friendly large batch)\n",
    "TN_MAX_EPOCHS     = 60\n",
    "TN_PATIENCE       = 10\n",
    "TN_LR             = 3e-2\n",
    "TN_WEIGHT_DECAY   = 1e-5\n",
    "TN_SPARSEMAP      = True               # sparsemax mask (stable on tabular)\n",
    "\n",
    "# ------------------------ helpers ------------------------\n",
    "def canon_col(name: str) -> str:\n",
    "    s = re.sub(r\"[^0-9A-Za-z_]+\", \"_\", str(name))\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def canon_cols_inplace(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.columns = [canon_col(c) for c in df.columns]\n",
    "    return df\n",
    "\n",
    "def _unique_in_order(seq):\n",
    "    seen = set(); out = []\n",
    "    for x in seq:\n",
    "        xs = str(x)\n",
    "        if xs not in seen:\n",
    "            seen.add(xs); out.append(xs)\n",
    "    return out\n",
    "\n",
    "def normalize_ws(x):\n",
    "    if isinstance(x, str):\n",
    "        x = re.sub(r\"[\\u200c\\u200f\\u200e]\", \"\", x)\n",
    "        x = x.replace(\"\\u00a0\", \" \")\n",
    "        x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    return x\n",
    "\n",
    "def downcast_numeric(df):\n",
    "    df = df.copy()\n",
    "    for c in df.select_dtypes(include=[\"float64\",\"int64\",\"int32\",\"float32\"]).columns:\n",
    "        if pdt.is_float_dtype(df[c]): df[c] = pd.to_numeric(df[c], downcast=\"float\")\n",
    "        else:                         df[c] = pd.to_numeric(df[c], downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def parse_possible_datetimes(df):\n",
    "    df = df.copy()\n",
    "    for c in df.columns:\n",
    "        if df[c].dtype == object:\n",
    "            s = df[c].astype(object)\n",
    "            if s.astype(str).str.contains(r\"\\d{4}[-/]\\d{1,2}[-/]\\d{1,2}\", regex=True, na=False).mean() > 0.2:\n",
    "                try: df[c] = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "                except Exception: pass\n",
    "    return df\n",
    "\n",
    "def expand_datetimes(df):\n",
    "    df = df.copy()\n",
    "    dt_cols = [c for c in df.columns if pdt.is_datetime64_any_dtype(df[c])]\n",
    "    for c in dt_cols:\n",
    "        s = df[c]\n",
    "        df[f\"{c}__year\"]   = s.dt.year.astype(\"Int16\")\n",
    "        df[f\"{c}__month\"]  = s.dt.month.astype(\"Int8\")\n",
    "        df[f\"{c}__day\"]    = s.dt.day.astype(\"Int8\")\n",
    "        df[f\"{c}__dow\"]    = s.dt.dayofweek.astype(\"Int8\")\n",
    "        df[f\"{c}__hour\"]   = s.dt.hour.fillna(0).astype(\"Int8\")\n",
    "        df[f\"{c}__mstart\"] = s.dt.is_month_start.astype(\"Int8\")\n",
    "        df[f\"{c}__mend\"]   = s.dt.is_month_end.astype(\"Int8\")\n",
    "    if dt_cols: df.drop(columns=dt_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "def pd_cat_fix(series, allowed):\n",
    "    allowed = _unique_in_order(list(allowed) + [MISSING_TOKEN])\n",
    "    s = series.astype(\"string\").fillna(MISSING_TOKEN)\n",
    "    s = s.where(s.isin(allowed), MISSING_TOKEN)\n",
    "    return pd.Categorical(s, categories=pd.Index(allowed), ordered=False)\n",
    "\n",
    "# ------------------------ Preprocessor ------------------------\n",
    "class TabularPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.num_cols_ = []; self.cat_cols_ = []\n",
    "        self.num_median_ = {}; self.cat_categories_ = {}\n",
    "        self.feature_names_ = []; self.fitted_ = False\n",
    "\n",
    "    def _prep_base(self, X):\n",
    "        d = X.copy()\n",
    "        canon_cols_inplace(d)\n",
    "        for c in d.columns:\n",
    "            if d[c].dtype == object:\n",
    "                d[c] = d[c].map(normalize_ws)\n",
    "        d = parse_possible_datetimes(d)\n",
    "        d = expand_datetimes(d)\n",
    "        for c in d.columns:\n",
    "            if pdt.is_bool_dtype(d[c]):\n",
    "                d[c] = d[c].astype(\"int8\")\n",
    "        d = downcast_numeric(d)\n",
    "        return d\n",
    "\n",
    "    def fit(self, X):\n",
    "        d = self._prep_base(X)\n",
    "        self.num_cols_ = [c for c in d.columns if pdt.is_numeric_dtype(d[c])]\n",
    "        self.cat_cols_ = [c for c in d.columns if not pdt.is_numeric_dtype(d[c])]\n",
    "        for c in self.num_cols_:\n",
    "            self.num_median_[c] = float(pd.to_numeric(d[c], errors=\"coerce\").median())\n",
    "        for c in self.cat_cols_:\n",
    "            s = d[c].astype(\"string\").fillna(MISSING_TOKEN)\n",
    "            vc = s.value_counts(dropna=False)\n",
    "            if MAX_CAT_CARD and len(vc) > (MAX_CAT_CARD - 1):\n",
    "                top = vc.index.astype(\"string\").tolist()[:MAX_CAT_CARD - 1]\n",
    "                cats = _unique_in_order(top + [MISSING_TOKEN])\n",
    "            else:\n",
    "                cats = _unique_in_order(pd.unique(s).astype(\"string\").tolist() + [MISSING_TOKEN])\n",
    "            if len(cats) != len(set(cats)):\n",
    "                cats = _unique_in_order(cats)\n",
    "            self.cat_categories_[c] = cats\n",
    "        self.feature_names_ = self.num_cols_ + self.cat_cols_\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert self.fitted_\n",
    "        d = self._prep_base(X)\n",
    "        for c in self.feature_names_:\n",
    "            if c not in d.columns:\n",
    "                d[c] = np.nan\n",
    "        d = d[self.feature_names_].copy()\n",
    "        for c in self.num_cols_:\n",
    "            d[c] = pd.to_numeric(d[c], errors=\"coerce\").astype(\"float32\")\n",
    "            if d[c].isna().any():\n",
    "                d[c] = d[c].fillna(self.num_median_[c])\n",
    "        for c in self.cat_cols_:\n",
    "            d[c] = pd_cat_fix(d[c], self.cat_categories_[c])\n",
    "        return d\n",
    "\n",
    "# ------------------------ Encoder for TabNet ------------------------\n",
    "class TabNetEncoder:\n",
    "    \"\"\"\n",
    "    - Z-score numerics\n",
    "    - Integer codes for categoricals (kept as ints)\n",
    "    - Returns:\n",
    "        X_all (float32)  : numerics then categorical codes (cast to float32 for TabNet)\n",
    "        cat_idxs         : positions of categorical columns in X_all\n",
    "        cat_dims         : cardinalities\n",
    "    \"\"\"\n",
    "    def __init__(self, num_cols, cat_cols, cat_categories):\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.cat_cols = list(cat_cols)\n",
    "        self.cat_categories = {c: list(cats) for c, cats in cat_categories.items()}\n",
    "        self.num_mean_ = None; self.num_std_ = None\n",
    "        self.cat_cardinalities_ = {c: len(self.cat_categories[c]) for c in self.cat_cols}\n",
    "\n",
    "    def fit(self, df_proc):\n",
    "        if self.num_cols:\n",
    "            arr = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            self.num_mean_ = arr.mean(axis=0).astype(\"float32\")\n",
    "            std = arr.std(axis=0).astype(\"float32\")\n",
    "            self.num_std_  = np.where(std < 1e-6, 1.0, std).astype(\"float32\")\n",
    "        else:\n",
    "            self.num_mean_ = np.array([], dtype=\"float32\")\n",
    "            self.num_std_  = np.array([], dtype=\"float32\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, df_proc):\n",
    "        N = len(df_proc)\n",
    "        # numerics -> z-score\n",
    "        if self.num_cols:\n",
    "            Xn = df_proc[self.num_cols].astype(\"float32\").values\n",
    "            Xn = (Xn - self.num_mean_) / self.num_std_\n",
    "        else:\n",
    "            Xn = np.zeros((N, 0), dtype=\"float32\")\n",
    "\n",
    "        # categorical -> integer codes (kept as int, but cast to float32 for TabNet input)\n",
    "        Xc_list = []\n",
    "        for c in self.cat_cols:\n",
    "            codes = df_proc[c].cat.codes.to_numpy(copy=False)\n",
    "            fix = self.cat_categories[c].index(MISSING_TOKEN)\n",
    "            codes = np.where(codes < 0, fix, codes).astype(\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "            Xc_list.append(codes)\n",
    "        Xc = np.stack(Xc_list, axis=1) if Xc_list else np.zeros((N, 0), dtype=\"int32\" if STORE_CAT_AS_INT32 else \"int64\")\n",
    "\n",
    "        # concatenate (TabNet expects float32; it will embed categorical columns by indices)\n",
    "        X_all = Xn if Xc.shape[1] == 0 else np.hstack([Xn, Xc.astype(\"float32\")]).astype(\"float32\")\n",
    "\n",
    "        # categorical positions are after numerics\n",
    "        cat_idxs = list(range(Xn.shape[1], Xn.shape[1] + Xc.shape[1]))\n",
    "        cat_dims = [self.cat_cardinalities_[c] for c in self.cat_cols]\n",
    "        return X_all, cat_idxs, cat_dims\n",
    "\n",
    "    def save_meta(self, path_json):\n",
    "        meta = {\n",
    "            \"num_cols\": self.num_cols,\n",
    "            \"cat_cols\": self.cat_cols,\n",
    "            \"cat_categories\": self.cat_categories,\n",
    "            \"num_mean\": self.num_mean_.tolist(),\n",
    "            \"num_std\": self.num_std_.tolist(),\n",
    "            \"cat_cardinalities\": self.cat_cardinalities_,\n",
    "        }\n",
    "        with open(path_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_meta(path_json):\n",
    "        with open(path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            meta = json.load(f)\n",
    "        enc = TabNetEncoder(meta[\"num_cols\"], meta[\"cat_cols\"], meta[\"cat_categories\"])\n",
    "        enc.num_mean_ = np.array(meta[\"num_mean\"], dtype=\"float32\")\n",
    "        enc.num_std_  = np.array(meta[\"num_std\"], dtype=\"float32\")\n",
    "        enc.cat_cardinalities_ = {k:int(v) for k,v in meta[\"cat_cardinalities\"].items()}\n",
    "        return enc\n",
    "\n",
    "# ------------------------ LOAD & TARGET ------------------------\n",
    "df = pd.read_csv(DATA_CSV, low_memory=False)\n",
    "canon_cols_inplace(df)\n",
    "if TARGET_NAME not in df.columns:\n",
    "    raise KeyError(f\"Expected label column '{TARGET_NAME}' after canonicalization; got first columns {list(df.columns)[:20]}\")\n",
    "y1 = pd.to_numeric(df[TARGET_NAME], errors=\"coerce\").astype(\"Int64\")\n",
    "y1 = y1.where((y1>=1) & (y1<=10))\n",
    "mask = y1.notna()\n",
    "df = df.loc[mask].copy()\n",
    "y1 = y1.loc[mask].astype(\"int16\")\n",
    "y  = (y1 - 1).astype(\"int16\")  # 0..K-1\n",
    "X  = df.drop(columns=[TARGET_NAME])\n",
    "del df; gc.collect()\n",
    "\n",
    "# ------------------------ SPLIT (70/15/15) ------------------------\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=SEED)\n",
    "trainval_idx, test_idx = next(sss1.split(X, y))\n",
    "X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "\n",
    "val_rel = VAL_SIZE / (1.0 - TEST_SIZE)\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_rel, random_state=SEED)\n",
    "train_idx, val_idx = next(sss2.split(X_trainval, y_trainval))\n",
    "X_train, X_val = X_trainval.iloc[train_idx], X_trainval.iloc[val_idx]\n",
    "y_train, y_val = y_trainval.iloc[train_idx], y_trainval.iloc[val_idx]\n",
    "\n",
    "pd.Series(X_train.index, name=\"index\").to_csv(SPLIT_DIR/\"train_indices.csv\", index=False)\n",
    "pd.Series(X_val.index,   name=\"index\").to_csv(SPLIT_DIR/\"val_indices.csv\", index=False)\n",
    "pd.Series(X_test.index,  name=\"index\").to_csv(SPLIT_DIR/\"test_indices.csv\", index=False)\n",
    "print(f\"Shapes -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "# ------------------------ PREPROCESS & ENCODE ------------------------\n",
    "pp = TabularPreprocessor().fit(X_train)\n",
    "Xtr_df = pp.transform(X_train); Xva_df = pp.transform(X_val); Xte_df = pp.transform(X_test)\n",
    "\n",
    "enc = TabNetEncoder(pp.num_cols_, pp.cat_cols_, pp.cat_categories_).fit(Xtr_df)\n",
    "X_tr, cat_idxs, cat_dims = enc.transform(Xtr_df)\n",
    "X_va, _, _               = enc.transform(Xva_df)\n",
    "X_te, _, _               = enc.transform(Xte_df)\n",
    "\n",
    "n_num = len(pp.num_cols_)\n",
    "n_cat = len(pp.cat_cols_)\n",
    "print(f\"[TabNet] nums={n_num} cats={n_cat}  total_in={X_tr.shape[1]}  cat_embs={len(cat_dims)}\")\n",
    "\n",
    "# free raw frames\n",
    "del X_train, X_val, X_test, X_trainval, Xtr_df, Xva_df, Xte_df, X, y, y1; gc.collect()\n",
    "\n",
    "y_tr = y_train.values.astype(\"int64\")\n",
    "y_va = y_val.values.astype(\"int64\")\n",
    "y_te = y_test.values.astype(\"int64\")\n",
    "\n",
    "# ------------------------ CLASS WEIGHTS / SAMPLE WEIGHTS ------------------------\n",
    "classes_present = np.unique(y_tr)\n",
    "cw = compute_class_weight(class_weight=\"balanced\", classes=classes_present, y=y_tr)\n",
    "cw_map = {int(c): float(w) for c, w in zip(classes_present, cw)}\n",
    "class_weights = np.ones(NUM_CLASSES, dtype=\"float32\")\n",
    "for c, w in cw_map.items(): class_weights[c] = w\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "\n",
    "sw_tr = class_weights[y_tr]\n",
    "sw_va = class_weights[y_va]\n",
    "\n",
    "# loss with class weights\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float32))\n",
    "\n",
    "# ---- cat embedding sizes (TabNet best practice: min(50, (dim+1)//2))\n",
    "def _emb_size(card): return int(min(32, (card + 1) // 2))\n",
    "cat_emb_dim = [_emb_size(c) for c in cat_dims]\n",
    "\n",
    "# ------------------------ MODEL / TRAIN ------------------------\n",
    "mask_type = \"sparsemax\" if TN_SPARSEMAP else \"entmax\"\n",
    "model = TabNetClassifier(\n",
    "    n_d=TN_N_D, n_a=TN_N_A, n_steps=TN_N_STEPS, gamma=TN_GAMMA,\n",
    "    n_independent=TN_N_INDEPENDENT, n_shared=TN_N_SHARED,\n",
    "    cat_idxs=cat_idxs, cat_dims=cat_dims, cat_emb_dim=cat_emb_dim,\n",
    "    optimizer_fn=torch.optim.AdamW,\n",
    "    optimizer_params=dict(lr=TN_LR, weight_decay=TN_WEIGHT_DECAY),\n",
    "    mask_type=mask_type,\n",
    "    momentum=TN_MOMENTUM,\n",
    "    verbose=10,  # TabNet internal verbosity\n",
    "    scheduler_params={\"step_size\": 50, \"gamma\": 0.9},  # mild LR decay\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "model.fit(\n",
    "    X_train=X_tr, y_train=y_tr,\n",
    "    eval_set=[(X_va, y_va)],\n",
    "    eval_name=[\"val\"],\n",
    "    eval_metric=[\"logloss\"],         # monitor neg log-likelihood\n",
    "    max_epochs=TN_MAX_EPOCHS,\n",
    "    patience=TN_PATIENCE,\n",
    "    batch_size=TN_BATCH_SIZE,\n",
    "    virtual_batch_size=TN_VBS,\n",
    "    num_workers=max(1, (os.cpu_count() or 4)//2),\n",
    "    loss_fn=loss_fn,\n",
    "    weights=sw_tr,                   # class-balanced training\n",
    ")\n",
    "elapsed = time.time() - t0\n",
    "best_epoch = getattr(model, \"best_epoch\", None)\n",
    "print(f\"Training time: {elapsed/60:.1f} min; best epoch: {best_epoch}\")\n",
    "\n",
    "# ------------------------ Evaluation ------------------------\n",
    "def predict_proba_tabnet(m, X):\n",
    "    return m.predict_proba(X)\n",
    "\n",
    "def eval_split(name, Xmat, y_zero):\n",
    "    proba = predict_proba_tabnet(model, Xmat)\n",
    "    pred0 = np.argmax(proba, axis=1)\n",
    "\n",
    "    metrics = {\n",
    "        \"split\": name,\n",
    "        \"n_samples\": int(len(y_zero)),\n",
    "        \"accuracy\": float(accuracy_score(y_zero, pred0)),\n",
    "    }\n",
    "    for avg in [\"macro\", \"weighted\"]:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(y_zero, pred0, average=avg, zero_division=0)\n",
    "        metrics[f\"precision_{avg}\"] = float(p)\n",
    "        metrics[f\"recall_{avg}\"]    = float(r)\n",
    "        metrics[f\"f1_{avg}\"]        = float(f1)\n",
    "    try:\n",
    "        metrics[\"log_loss\"] = float(log_loss(y_zero, proba, labels=list(range(NUM_CLASSES))))\n",
    "    except Exception:\n",
    "        metrics[\"log_loss\"] = float(\"nan\")\n",
    "    try:\n",
    "        y_bin = pd.get_dummies(pd.Categorical(y_zero, categories=list(range(NUM_CLASSES))))\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(roc_auc_score(y_bin.values, proba, average=\"macro\", multi_class=\"ovr\"))\n",
    "    except Exception:\n",
    "        metrics[\"roc_auc_ovr_macro\"] = float(\"nan\")\n",
    "\n",
    "    ys_one  = y_zero + 1\n",
    "    pred_one = pred0 + 1\n",
    "    report = classification_report(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)), zero_division=0)\n",
    "    with open(OUTPUT_DIR / f\"classification_report_{name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "    cm = confusion_matrix(ys_one, pred_one, labels=list(range(1, NUM_CLASSES+1)))\n",
    "    pd.DataFrame(cm, index=range(1, NUM_CLASSES+1), columns=range(1, NUM_CLASSES+1))\\\n",
    "      .to_csv(OUTPUT_DIR / f\"confusion_matrix_{name}.csv\")\n",
    "    return metrics, pred_one, proba\n",
    "\n",
    "m_val,   _, _ = eval_split(\"val\",  X_va, y_va)\n",
    "m_test,  _, _ = eval_split(\"test\", X_te, y_te)\n",
    "pd.DataFrame([m_val, m_test]).to_csv(OUTPUT_DIR / \"metrics_tabnet.csv\", index=False)\n",
    "print(pd.DataFrame([m_val, m_test]))\n",
    "\n",
    "# ------------------------ Curves ------------------------\n",
    "try:\n",
    "    hist = model.history\n",
    "    train_loss = hist[\"loss\"]\n",
    "    val_loss   = hist[\"val_logloss\"]\n",
    "    iters = np.arange(1, len(train_loss)+1)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(iters, train_loss, label=\"train\")\n",
    "    plt.plot(iters, val_loss[:len(iters)], label=\"val\")\n",
    "    plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.title(\"TabNet Loss\"); plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / \"loss_curves.png\", dpi=150); plt.close()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ------------------------ Save artifacts ------------------------\n",
    "model.save_model(str(OUTPUT_DIR / \"tabnet_model\"))\n",
    "joblib.dump(pp,  OUTPUT_DIR / \"preprocessor.pkl\")\n",
    "enc.save_meta(OUTPUT_DIR / \"encoder_meta.json\")\n",
    "\n",
    "with open(OUTPUT_DIR / \"training_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"target_name\": TARGET_NAME,\n",
    "        \"num_classes\": NUM_CLASSES,\n",
    "        \"label_order_zero_indexed\": list(range(NUM_CLASSES)),\n",
    "        \"seed\": SEED,\n",
    "        \"best_epoch\": int(best_epoch) if best_epoch is not None else None,\n",
    "        \"splits\": {\"train\": \"splits/train_indices.csv\", \"val\": \"splits/val_indices.csv\", \"test\": \"splits/test_indices.csv\"},\n",
    "        \"data_csv\": DATA_CSV,\n",
    "        \"tabnet_config\": {\n",
    "            \"n_d\": TN_N_D, \"n_a\": TN_N_A, \"n_steps\": TN_N_STEPS, \"gamma\": TN_GAMMA,\n",
    "            \"n_independent\": TN_N_INDEPENDENT, \"n_shared\": TN_N_SHARED,\n",
    "            \"batch_size\": TN_BATCH_SIZE, \"virtual_batch_size\": TN_VBS,\n",
    "            \"lr\": TN_LR, \"weight_decay\": TN_WEIGHT_DECAY,\n",
    "            \"mask_type\": (\"sparsemax\" if TN_SPARSEMAP else \"entmax\"),\n",
    "            \"cat_idxs\": cat_idxs, \"cat_dims\": cat_dims, \"cat_emb_dim\": cat_emb_dim\n",
    "        },\n",
    "        \"train_time_min\": round(elapsed/60, 2),\n",
    "        \"feature_names\": (pp.num_cols_ + pp.cat_cols_),\n",
    "        \"cat_features\": pp.cat_cols_,\n",
    "        \"num_features\": pp.num_cols_,\n",
    "        \"column_names_canonicalized\": True\n",
    "    }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✅ All artifacts saved to: {OUTPUT_DIR.resolve()} (model: tabnet_model.zip)\")\n",
    "\n",
    "# ------------------------ Inference helper ------------------------\n",
    "def predict_target_risk_class_tabnet(\n",
    "    df_new: pd.DataFrame,\n",
    "    model_prefix=OUTPUT_DIR / \"tabnet_model\",          # TabNet uses prefix without .zip\n",
    "    preproc_path=OUTPUT_DIR / \"preprocessor.pkl\",\n",
    "    encoder_meta_path=OUTPUT_DIR / \"encoder_meta.json\",\n",
    "    batch_size=200000\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Predict on new raw rows (returns labels in 1..10).\n",
    "    Uses the same preprocessing and encoding as training.\n",
    "    \"\"\"\n",
    "    # reload model\n",
    "    tabnet = TabNetClassifier()\n",
    "    tabnet.load_model(str(model_prefix))  # loads from prefix (will look for .zip)\n",
    "\n",
    "    pp_inf  = joblib.load(preproc_path)\n",
    "    enc_inf = TabNetEncoder.load_meta(encoder_meta_path)\n",
    "\n",
    "    df_new = df_new.copy(); canon_cols_inplace(df_new)\n",
    "    dproc = pp_inf.transform(df_new)\n",
    "    X_all, _, _ = enc_inf.transform(dproc)\n",
    "\n",
    "    # batch predict for huge inputs\n",
    "    N = X_all.shape[0]\n",
    "    preds = np.empty(N, dtype=\"int16\")\n",
    "    for s in range(0, N, batch_size):\n",
    "        e = min(N, s+batch_size)\n",
    "        proba = tabnet.predict_proba(X_all[s:e])\n",
    "        preds[s:e] = np.argmax(proba, axis=1).astype(\"int16\")\n",
    "    return pd.Series(preds + 1, index=dproc.index, name=\"pred_target_risk_class\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "FC2Hbr5K7Frs",
    "7tz2O2qak4ZA",
    "qYgHQIGxMxhJ",
    "Ob4iRedyKClF",
    "Vv6k9GYESOYT",
    "hlupx7C0Mq3T",
    "T3BjoXBRND4y",
    "zIOPqlZAEMzo",
    "y0hXUxmtEIHc",
    "RpnnY1YYWzfH",
    "KRNw-cQpW9nS",
    "-kCm1Y4ruGPU",
    "Qx50xUM1uJzY"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
